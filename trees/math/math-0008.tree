\title{Matrix Computation}
\taxon{Linear Algebra}
\date{2024-06-10}
\import{macros}
\p{
    This post shows operations and applications over matrix, refers to Wikipedia.
}
\transclude{def-0043}
\p{
    If the following condition satisfies:
    ##{
        a_{ij} = a_{ji} \quad \forall i,j
    }
    Then the matrix is called symmetric.
}
\transclude{def-001N}
\transclude{def-0044}
\p{
    To compute the inverse of a matrix, we need \strong{Adjugate matrix}.
}
\transclude{def-0045}
\transclude{def-0046}
\p{
    Then the inverse of #{A} is the transpose of the cofactor matrix times the reciprocal of the determinant of #{A}:
    ##{
        A^{-1} = \frac{1}{\det A} \cdot \text{adj} A = \frac{1}{\det A} \cdot C^T
    }
}
\transclude{def-0047}
\subtree{
    \title{An important property of the inverse of a matrix}
    \p{
        ##{
            A \cdot \text{adj} A = \text{adj} A \cdot A = \det A \cdot I
        }
    }
    \proof{
        Let #{A \cdot \text{adj} A = (b_{ij})} and we have
        ##{
            b_{ij} = a_{i1}A_{j1} + a_{i2}A_{j2} + \cdots + a_{in}A_{jn} = \delta_{ij} \cdot \det A
        }
        Hence we have #{A \cdot \text{adj} A = \det A \cdot I} 
    }
}
\subtree{
    \title{Matrix Polynomial and Computation}
    \transclude{def-0048}
    \p{
        If #{A} is a diagonal matrix, then the polynomial of #{A} is the diagonal matrix of the polynomial of the diagonal elements of #{A}.
        ##{
            p(A) = \begin{bmatrix}
                p(a_{11}) & 0 & \cdots & 0 \\
                0 & p(a_{22}) & \cdots & 0 \\
                \vdots & \vdots & \ddots & \vdots \\
                0 & 0 & \cdots & p(a_{nn})
            \end{bmatrix}
        }
    }
    \p{
        If #{A = P\Lambda P^{-1}}, then #{A^k = P \Lambda ^k P^{-1}} and hence
        ##{
            p(A) = a_0 I + a_1 A + a_2 A^2 + \cdots + a_n A^n = P \Lambda P^{-1}
        }
    }
}
\subtree{
    \title{Solving a Linear System}
    \transclude{thm-0011}
    \p{
        Matrix partitioning is the process of dividing a matrix into smaller submatrices. 
        This is often done to simplify the computation of matrix operations, such as matrix multiplication.
    }
    \transclude{def-0049}
    \p{
        If the partitioned matrix is formed as diagonal blocks, then we can compute the determinant of the matrix by the following formula:
        ##{
            \det A = \det A_1 \cdot \det A_2 \cdots \det A_n
        }
        And the inverse of the matrix is
        ##{
            A^{-1} = \begin{bmatrix}
                A_1^{-1} & O & \cdots & O \\
                O & A_2^{-1} & \cdots & O \\
                \vdots & \vdots & \ddots & \vdots \\
                O & O & \cdots & A_n^{-1}
            \end{bmatrix}
        }
    }
    \p{
        The column partitioning of matrix is useful. 
        If we have #{m\times s} matrix #{A = (a_{ij})} and #{s\times n} matrix #{B=(b_{ij})},
        their product can be written:
        ##{
            AB = \begin{bmatrix} A_1 \\ A_2 \\ \vdots A_m \end{bmatrix}
            \begin{bmatrix}
                B_1 & B_2 & \cdots & B_n
            \end{bmatrix} = 
            \begin{bmatrix}
                A_1B_1 & A_1B_2 & \cdots & A_1B_n \\
                A_2B_1 & A_2B_2 & \cdots & A_2B_n \\
                \vdots & \vdots & \ddots & \vdots \\
                A_mB_1 & A_mB_2 & \cdots & A_mB_n
            \end{bmatrix}
        }
        We can show that #{A=O\iff A^TA=O}.
    }
}
\subtree{
    \title{Matrix Transformation}
    \taxon{Section}
    \transclude{def-004A}
    \transclude{def-004B}
    \p{
        We can show that a matrix #{A} is invertible iff there are finite elementary matrices
        #{E_1, E_2, \cdots, E_n} such that
        ##{
            A = E_1E_2\cdots E_n
        }
    }
    \p{
        From above we can deduce that a square matrix #{A} is invertible iff #{A\sim E}.
        This trick can be used for solving a linear system and computing the inverse of a matrix.
        For instance, given #{AX=B} we can solve #{X} by the following steps:
        Let #{P} be a matrix such that #{PA=I} where #{I} is the identity matrix.
        Hence #{P = A^{-1}} and we have #{X = PB}, we can do elementary operations over matrix #{(A, B)}
        to get the solution of #{X}.
    }
}
\subtree{
    \title{Rank}
    \p{
        Now let's talk about the concept of \strong{rank}.
        In linear algebra, the (column)\strong{rank} of a matrix #{A} is the dimension of the vector space 
        generated (or spanned) by its columns.
    }
    \transclude{def-004J}
    \p{
        Similarly we can define the row rank of a matrix. A fundamental result in linear algebra is 
        that the column rank and the row rank are always equal. Hence we can simply call it the rank of a matrix.
    }
    \p{
        A common approach to finding the rank of a matrix is to reduce it to a simpler form, 
        generally \strong{row echelon form}, by elementary row operations.
        Row operations do not change the row space (hence do not change the row rank), and, being 
        invertible, map the column space to an isomorphic space (hence do not change the column rank).
        ##{
            A \sim B \implies \rank(A) = \rank(B)
            \\ 
            \rank(A) = \rank(A^T)
        }
        Once in row echelon form, the rank is clearly the same for both row rank and column rank, 
        and equals to the number of \strong{pivots} (or basic columns) and also the number of non-zero rows.
    }
    \transclude{def-004I}
    \p{
        For instance, matrix #{A = \begin{bmatrix}1&2&1\\-2&-3&1\\3&5&0\end{bmatrix}} can be transformed into reduced row-echelon form:
        ##{\begin{aligned}{\begin{bmatrix}1&2&1\\-2&-3&1\\3&5&0\end{bmatrix}}&\xrightarrow {2R_{1}+R_{2}\to R_{2}} {\begin{bmatrix}1&2&1\\0&1&3\\3&5&0\end{bmatrix}}\xrightarrow {-3R_{1}+R_{3}\to R_{3}} {\begin{bmatrix}1&2&1\\0&1&3\\0&-1&-3\end{bmatrix}}\\&\xrightarrow {R_{2}+R_{3}\to R_{3}} \,\,{\begin{bmatrix}1&2&1\\0&1&3\\0&0&0\end{bmatrix}}\xrightarrow {-2R_{2}+R_{1}\to R_{1}} {\begin{bmatrix}1&0&-5\\0&1&3\\0&0&0\end{bmatrix}}~.\end{aligned}}
        The final matrix has two non-zero rows and thus the rank of matrix #{A} is #{2}.
    }
    \p{
        The determinantal rank of a matrix is the order of the largest non-zero minor of the matrix.
        It is also the number of non-zero eigenvalues of the matrix. 
        This does not give an efficient way of computing the rank, but it is useful theoretically: 
        a single non-zero minor witnesses a lower bound for the rank of the matrix,
        which can be useful to prove that certain operations do not lower the rank of a matrix.
    }
}