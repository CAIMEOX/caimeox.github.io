\title{Matrix}
\taxon{Linear Algebra}
\import{macros}
\p{
    This post shows operations and applications over matrix.
}
\transclude{def-0043}
\p{
    If the following condition satisfies:
    ##{
        a_{ij} = a_{ji} \quad \forall i,j
    }
    Then the matrix is called symmetric.
}
\transclude{def-001N}
\transclude{def-0044}
\p{
    To compute the inverse of a matrix, we need \strong{Adjugate matrix}.
}
\transclude{def-0045}
\transclude{def-0046}
\p{
    Then the inverse of #{A} is the transpose of the cofactor matrix times the reciprocal of the determinant of #{A}:
    ##{
        A^{-1} = \frac{1}{\det A} \cdot \text{adj} A = \frac{1}{\det A} \cdot C^T
    }
}
\transclude{def-0047}
\subtree{
    \title{An important property of the inverse of a matrix}
    \p{
        ##{
            A \cdot \text{adj} A = \text{adj} A \cdot A = \det A \cdot I
        }
    }
    \proof{
        Let #{A \cdot \text{adj} A = (b_{ij})} and we have
        ##{
            b_{ij} = a_{i1}A_{j1} + a_{i2}A_{j2} + \cdots + a_{in}A_{jn} = \delta_{ij} \cdot \det A
        }
        Hence we have #{A \cdot \text{adj} A = \det A \cdot I} 
    }
}
\subtree{
    \title{Matrix Polynomial and Computation}
    \transclude{def-0048}
    \p{
        If #{A} is a diagonal matrix, then the polynomial of #{A} is the diagonal matrix of the polynomial of the diagonal elements of #{A}.
        ##{
            p(A) = \begin{bmatrix}
                p(a_{11}) & 0 & \cdots & 0 \\
                0 & p(a_{22}) & \cdots & 0 \\
                \vdots & \vdots & \ddots & \vdots \\
                0 & 0 & \cdots & p(a_{nn})
            \end{bmatrix}
        }
    }
    \p{
        If #{A = P\Lambda P^{-1}}, then #{A^k = P \Lambda ^k P^{-1}} and hence
        ##{
            p(A) = a_0 I + a_1 A + a_2 A^2 + \cdots + a_n A^n = P \Lambda P^{-1}
        }
    }
}
\subtree{
    \title{Solving a Linear System}
    \transclude{thm-0011}
    \p{
        Matrix partitioning is the process of dividing a matrix into smaller submatrices. 
        This is often done to simplify the computation of matrix operations, such as matrix multiplication.
    }
    \transclude{def-0049}
    \p{
        If the partitioned matrix is formed as diagonal blocks, then we can compute the determinant of the matrix by the following formula:
        ##{
            \det A = \det A_1 \cdot \det A_2 \cdots \det A_n
        }
        And the inverse of the matrix is
        ##{
            A^{-1} = \begin{bmatrix}
                A_1^{-1} & O & \cdots & O \\
                O & A_2^{-1} & \cdots & O \\
                \vdots & \vdots & \ddots & \vdots \\
                O & O & \cdots & A_n^{-1}
            \end{bmatrix}
        }
    }
    \p{
        The column partitioning of matrix is useful. 
        If we have #{m\times s} matrix #{A = (a_{ij})} and #{s\times n} matrix #{B=(b_{ij})},
        their product can be written:
        ##{
            AB = \begin{bmatrix} A_1 \\ A_2 \\ \vdots A_m \end{bmatrix}
            \begin{bmatrix}
                B_1 & B_2 & \cdots & B_n
            \end{bmatrix} = 
            \begin{bmatrix}
                A_1B_1 & A_1B_2 & \cdots & A_1B_n \\
                A_2B_1 & A_2B_2 & \cdots & A_2B_n \\
                \vdots & \vdots & \ddots & \vdots \\
                A_mB_1 & A_mB_2 & \cdots & A_mB_n
            \end{bmatrix}
        }
        We can show that #{A=O\iff A^TA=O}.
    }
}