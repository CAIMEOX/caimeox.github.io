\title{Vector Calculus and Geometry of Space}
\date{2024-04-05}
\taxon{Differential Geometry}
\import{macros}
\p{
    Notes about multi-variable calculus, geometry of space and linear algebra.
    Refer to [df-cm-2018](A Visual Introduction to Differential Forms and Calculus on Manifolds).
}
\subtree{
    \title{Review of Vector Spaces}
    \p{
        We now start with introducing the vector space over the field of real numbers #{\R}.
    }
    \transclude{def-000H}
    \p{
        Use #{\R^2} as an example we can see (Note that we always treat elements of vector spaces as 
        column vectors and never as row vectors):
        ##{
            c \cdot \begin{bmatrix}
                a \\ b
            \end{bmatrix} = \begin{bmatrix}
                c \cdot a \\ c \cdot b
            \end{bmatrix}
        }
    }
    \p{
        Now we will consider a certain type of transformation between vector spaces called a [\strong{linear transformation}](def-0025).
        Suppose #{T} is a mapping between #{\R^n} and #{\R^m}, that is #{T:\R^n\to\R^m}, then #{T} is a linear transformation if:
        ##{
            T(c \cdot \vec{v}) = c \cdot T(\vec{v})
            \\ 
            T(\vec{v} + \vec{w}) = T(\vec{v}) + T(\vec{w})
        }
        If #{T} is a linear transformation from #{\R^m} to #{\R} we simply call it a \strong{linear function} or a \strong{linear functional}.
    }
    \p{
        We now turn our attention to the relationship between linear transformation and matrices. 
        We just stick to vector spaces #{\R^n} and the standard basis made up of the \strong{Euclidian unit vectors}.
        In order to write linear transformation #{T:\R^n\to\R^m} as a matrix we need ordered bases for both #{\R^n} and #{\R^m}.
        We can use the intuitively obvious order #{e_1 < e_2 < \cdots < e_n}.
        Now we can give formal definition of the matrix representation of a linear transformation.
    }
    \transclude{def-003W}
    \p{
        The last major topic in this section is the definition of the [dual space](def-003X).
        In our discussion, we only concern the dual space of #{\R^n} which is denoted as #{(\R^n)^*}.
        Now let's consider the \strong{dual basis} of #{(\R^n)^*} which is denoted as #{\{T_1, \cdots, T_n\}}, 
        which is defined by:
        ##{
            T_i(e_j) = e^i(e_j) = \langle e^i, e_j \rangle = \delta_{j}^i
        }
        where #{\delta_{ij}} is the [Kronecker delta](def-001P). We say that #{T_i} is dual to the vector #{e_i}.
        Note that we also denote #{T_i} as #{e^i} using superscript notation. And the notation #{\langle e^i, e_j \rangle} d
        indicates the products of row vector #{e^i} and column vector #{e_j} (Usually used in quantum computing).
        ##{
            \alpha(v) = \langle \alpha, v \rangle = [a,b] \times \begin{bmatrix}
                x \\ y
            \end{bmatrix} = ax + by
        }
        This explains wht we always denote elements of the vector space as column vectors, because elements of the dual space 
        are written as row vectors and its very important to distinguish between them.
    }
}
\subtree{
    \title{Dot Products}
    \p{
        In linear algebra, \strong{dot product} or \strong{scalar product} is an operation that takes two vectors and returns a scalar.
        Geometrically, it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them.
        Dot product is also used to define lengths and angles.
    }
    \transclude{def-0041}
    \transclude{def-0042}
    \p{
        The geometric definition of dot product helps us express the projection of one vector onto another as well as the component of 
        one vector in the direction of another. By simple geometry we can derive the formula for the \strong{projection}
        ##{
            \text{proj}_{\vec{a}}\vec{b} = \frac{\vec{a}\cdot\vec{b}}{\norm{\vec{a}}}\frac{\vec{a}}{\norm{\vec{a}}}
        }
        and the \strong{component} of #{\vec{b}} in the direction #{\vec{a}} is given by
        ##{
            \text{comp}_{\vec{a}}\vec{b} = \norm{\text{proj}_{\vec{a}}\vec{b}} = \frac{\vec{a}\cdot\vec{b}}{\norm{\vec{a}}}
        }
    }
    \p{
        Two points determine a line, and so does a point and a vector. Define the base point vector #{\vec{b}=(x,y,z)} and
        the direction vector #{\vec{v}=(a,b,c)} then the line is given by #{\vec{r}(t)}
        ##{
            \vec{r}(t) = t\vec{v} + \vec{b} = (at+x, bt+y, ct+z)
        }
        Solving for #{t} in the equation we get
        ##{
            t = \frac{x-at}{a} = \frac{y-bt}{b} = \frac{z-ct}{c}
        }
        which is the \strong{equation of line}.
    }
}
\subtree{
    \title{Volume and Determinants}
    \p{
        The \strong{determinant} has various properties and applications in linear algebra and geometry.
        For us the most useful thing about it will be how it relates to volume:
        the determinant of a matrix gives the \strong{signed volume} of the parallelepiped that 
        is generated by the vectors given by the matrix columns.
    }
    \p{
        Determinants can be introduced in a variety of different ways but many of them are not at all clear.
        It usually relates to volume hence we will actually use our intuitive understanding of volumes and 
        three properties that we expected volume to have to derive the determinant (It is \strong{uniquely} determined!).
    }
    \p{
        So how do we expect volume to behave?
        First we expect a unit cube to have a volume of one.
        Second we expect the \strong{degenerate} parallelepiped to have a volume of zero. Basically in #{n} dimensions any 
        #{n-1} dimensions object has zero #{n}-D volume.
        Third we expect that volumes to be \strong{linear}.
        Now with these three properties we move to the actual mathematics.
    }
    \p{
        Suppose we have a parallelepiped #{\mathscr{P}\in\R^n} whose edges are given by #{v_1, v_2, \cdots, v_n\in\R^n}.
        We sat that the parallelepiped #{\mathscr{P}} is the \strong{span} of the vectors #{v_1, v_2, \cdots, v_n} and 
        write #{\mathscr{P}=\spn\{v_1, v_2, \cdots, v_n\}} (Note that this span is different from linear span).
        We want to find function #{D:\R^{n\times n}\to\R} which takes #{v_1, v_2, \cdots, v_n} or a matrix with #{v_1, v_2, \cdots, v_n} as columns
        to a real number which is the volume of #{\mathscr{P}}. Now we present the three properties in mathematical form.
    }
    \subtree{
        \title{Properties of Volume}
        \ul{
            \li{
                #{ D(I) = I } where #{I = [e_1, e_2, \cdots, e_n]} is the identity matrix.
            }
            \li{
                #{ D(v_1, v_2, \cdots, v_n) = 0 } if #{v_i = v_j} for any #{i\neq j}.
            }
            \li{
                #{ D(v_1, \cdots, v_{j-1}, v+cw, v_{j+1}, \cdots, v_n) \\ 
                = D(v_1, \cdots, v_{j-1}, v, v_{j+1}, \cdots, v_n) + cD(v_1, \cdots, v_{j-1}, w, v_{j+1}, \cdots, v_n) }
                for any #{1 \leq j \leq n}, that is, #{D} is linear.
            }
        }
    }
    \p{
        Now we use these properties of volume to derive several other useful properties.
        The first property is that the volumes are signed.
    }
    \subtree{
        \title{Derived Properties of Volume Function}
        \ul{
            \li{ 
                #{D} is alternating, if we switch any two vectors the sign changes.
                ##{
                    D(v_1, \cdots, v_i, \cdots, v_j, \cdots, v_n) = -D(v_1, \cdots, v_j, \cdots, v_i, \cdots, v_n)
                }
            } 
            \li{
                If #{v_1, v_2, \cdots, v_n} are [linear dependent](def-000Q) then
                ##{
                    D(v_1, v_2, \cdots, v_n) = 0
                }
            }
            \li{
                Adding a multiple of one vector to another does not change the determinant.
            }
        }
    }
    \p{
        We almost ready to derive the formula for determinant. The final ingredient we need to do is \strong{permutations}.
    }
    \transclude{def-003Y}
    \transclude{def-003Z}
    \p{
        Notice that the composition of two permutations is also a permutation. 
        And for any permutation #{\sigma} we can perform a series of transpositions to get the identity permutation.
        It turns out that the count of the number of transpositions needed to get the identity permutation is always the same,
        which is called the \strong{parity} of the permutation.
    }
    \transclude{def-0040}
    \p{
        Now we define the permutation of unit vectors #{E_\sigma = [e_{\sigma(1)}, e_{\sigma(2)}, \cdots, e_{\sigma(n)}]}.
        We got the property that
        ##{
            D(E_\sigma) = \sgn(\sigma)D(I) = \sgn(\sigma)
        }
        Now we have all the pieces necessary to find a formula that will give the volume of the parallelepiped spanned
        by #{n} vectors.
        ##{
            \begin{align*}
                D\left(\begin{bmatrix}
                    a_{11} & a_{12} & \cdots & a_{1n} \\
                    a_{21} & a_{22} & \cdots & a_{2n} \\
                    \vdots & \vdots & \ddots & \vdots \\
                    a_{n1} & a_{n2} & \cdots & a_{nn}
                \end{bmatrix}\right) &= \sum_{i_1=1}^n a_{i_11}D\left(\begin{bmatrix}
                    | & a_{12} & \cdots & a_{1n} \\
                    e_{i_1} & a_{22} & \cdots & a_{2n} \\
                    | & \vdots & \ddots & \vdots \\
                    | & a_{n2} & \cdots & a_{nn}
                \end{bmatrix}\right) \\ 
                &= \sum_{i_1=1}^n a_{i_11} \sum_{i_2=1}^n a_{i_22}D\left(\begin{bmatrix}
                    | & | & \cdots & a_{1n} \\
                    e_{i_1} & e_{i_2} & \cdots & a_{2n} \\
                    | & | & \vdots & \vdots \\
                    | & | & \cdots & a_{nn}
                \end{bmatrix}\right) \\
                &= \vdots \\ 
                &= \sum_{i_1, i_2, \cdots, i_n = 1}^{n} a_{i_11}a_{i_22}\cdots a_{i_nn}D\left(
                    \begin{bmatrix}
                        | & | &  & | \\
                        e_{i_1} & e_{i_2} & \cdots & e_{i_n} \\
                        | & | &  & | \\
                    \end{bmatrix}
                \right) \\ 
                &= \sum_{\sigma\in S_n} a_{\sigma(1)1}\cdots a_{\sigma(n)n}
                D\left(
                    \begin{bmatrix}
                        | & | &  & | \\
                        e_{\sigma(1)} & e_{\sigma(2)} & \cdots & e_{\sigma(n)} \\
                        | & | &  & | \\
                    \end{bmatrix}
                \right) \\ 
                &= \sum_{\sigma\in S_n} a_{\sigma(1)1}a_{\sigma(2)2}\cdots a_{\sigma(n)n} \sgn(\sigma) \\ 
                &= \sum_{\sigma\in S_n} \sgn(\sigma) \prod_{i=1}^n a_{\sigma(i)i}
            \end{align*}
        }
        In the forth step we transform the terms because the value of #{D} is zero for any #{{i_j} = {i_k}},
        non-zero terms should be permutation of #{S_n}.
    }
    \p{
        It's easy to validate that the following properties of the determinant holds:
        \ul{
            \li{ #{D(AB) = D(A)D(B)} }
            \li{ #{D(A) = D(A^T)}}
        }
        The second statement for transpose of #{A} indicates that all the properties above also holds for row as well. 
    }
}
\subtree{
    \title{Derivatives of Multivariable Functions}
    \p{
        In this section we will introduce the idea of the derivative of a multivariable function. 
        Recall that a function #{f:\R\to\R} the derivative of #{f} at #{x_0\in\R} is given by
        ##{
            f'(x_0) = \lim_{h\to 0} \frac{f(x_0+h) - f(x_0)}{h}
        }
        if the limit exists. Now let's do some transformations:
        ##{
            \begin{align*}
                & f'(x_0) = \lim_{h\to 0} \frac{f(x_0+h) - f(x_0)}{h} \\
                \implies & \lim_{h\to 0} \frac{f(x_0+h) - f(x_0) -f'(x_0)h }{h} = 0 \\
                \implies & \lim_{x\to x_0} \frac{f(x) - f(x_0) - f'(x_0)(x-x_0)}{x-x_0} = 0 \\ 
                \implies & \lim_{x\to x_0} \frac{|f(x) - f(x_0) - f'(x_0)(x-x_0)|}{|x-x_0|} = 0
            \end{align*}
        }
    }
    \p{
        Since #{f'(x_0)} represents the slope of the line tangent to the graph of #{f} at #{(x_0, f(x_0))},
        differentiability of #{f} at #{x_0} means that there exists a number #{m} st
        ##{
            \lim_{x\to x_0} \frac{|f(x) - f(x_0) - m(x-x_0)|}{|x-x_0|} = 0
        }
        Now consider the function #{T:\R\to\R} where #{T(s) = ms}
        ##{
            T(s+t) = m(s+t) = ms + mt = T(s) + T(t)
            \\ 
            T(cs) = mcs = c(ms) = cT(s)
        }
        then #{T} is a linear transformation. In fact #{T} is the linear function that most closely approximates the 
        function #{f} at the point #{(x_0, f(x_0))}. So for #{x} values that are very close to #{x_0} we have
        ##{
            f(x) \approx m(x-x_0) + f(x_0)
        }
    }
    \p{
        Now let's generalize the concept of derivatives to functions of the form #{f:\R^n\to\R^m}.
        We assume the function #{f} has the form
        ##{
            \begin{align*}
                &f(x_1, x_2, \cdots, x_n) = 
                \\ 
                &(f_1(x_1, x_2, \cdots, x_n), f_2(x_1, x_2, \cdots, x_n), \cdots, f_m(x_1, x_2, \cdots, x_n))
            \end{align*}
        }
        We want to search for this linear transformation which we will denoted by #{Df},
        that most closely approximates this function #{f:\R^n\to\R^m} at some specific point #{x_0=(x_{1_0}, x_{2_0}, \cdots, x_{n_0}) \in \R^n}.
        If #{f} is differentiable at #{x_0} then there exists a linear transformation #{Df(x_0):\R^n\to\R^m} such that
        ##{
            \lim_{x\to x_0} \frac{
                \norm{
                    f(x) - f(x_0) - Df(x_0)(x-x_0)
                }
            }{\norm{x-x_0}} = 0
        }
        The #{\norm{\cdot}} represents the \strong{Euclidean norm} of the vector (Multi-dimensional version of the absolute value)
        ##{
            \norm{\vec{x}} = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2} = \sqrt{\sum_{i=1}^n x_i^2}
        }
        which is just the length of the vector. This allows us to perform dividing.
    }
    \p{
        As before we have
        ##{
            f(x) \approx Df(x_0)(x-x_0) + f(x_0)
        }
        Now we want to write #{Df(x)} as a matrix. Denote the basis of #{\R^n} as #{e_j} and the basis of #{\R^m} as #{f_i}.
        Then we want to find #{a_{ij}} st
        ##{
            Df(x)(e_j) = \sum_{i=1}^{m} a_{ij} f_j = \begin{bmatrix}
                a_{1j} \\ a_{2j} \\ \vdots \\ a_{mj}
            \end{bmatrix}
        }
        In other words, the #{i}-th component of the #{j}-th column of #{Df(x)} is just the #{i}-th component of the #{Df(x)(e_j)}
    }
    \p{
        Recall from vector calculus that given a function #{f:\R^n\to\R} we defined the \strong{partial derivative} of #{f} with respect to the #{x_j} as 
        ##{
            \frac{\partial f}{\partial x_j} = \lim_{h\to0}
            \frac{f(x_1,\cdots,x_j+h,\cdots,x_n) - f(x_1,\cdots,x_n)}{h}
        }
        Hence we can define the partial derivatives for each #{f_i (1\leq i\leq m)} with respect to each #{x_j (1\leq j\leq n)}.
        ##{
            \frac{\partial f_i}{\partial x_j} = \lim_{h\to0}
            \frac{f_i(x_1,\cdots,x_j+h,\cdots,x_n) - f_i(x_1,\cdots,x_n)}{h}
        }
        Thus we have
        ##{
            \frac{\partial f_i}{\partial x_j} = a_{ij}
        }
        To find #{a_{ij}} of #{Df(x_0)} we need to find the #{i}-th element of #{Df(x_0)(e_j)}. Let
        ##{
            x = \begin{bmatrix}
                x_{1_0} \\ x_{2_0} \\ \vdots \\ x_{n_0}
            \end{bmatrix} + \begin{bmatrix}
                0 \\ \vdots \\ 1 \\ \vdots \\ 0
            \end{bmatrix} = x_0 + he_j
        }
        We have 
        ##{
            \lim_{x\to x_0}\frac{\norm{f(x)-f(x_0)-Df(x_0)(he_j)}}{\norm{he_j}}
            \\ \implies 
            \lim_{h\to0}\frac{\norm{
                f(x_0+he_j) - f(x_0) -hDf(x_0)(e_j)
            }}{\norm{h}} = 0
        }
        The component is given by 
        ##{
            \lim_{h\to0}\frac{\norm{f_i(x_0+he_j)-f_i(x_0)-ha_{ij}}}{\norm{h}} = 0
            \\ \implies 
            a_{ij} = \lim_{h\to0}\frac{f_i(x_0+he_j) - f_i(x_0)}{h}
        }
        which is exactly #{\frac{\partial f_i}{\partial x_j}}. Thus the matrix representation of #{Df(x)} is given by 
        a matrix called the \strong{Jacobin matrix} of #{f}.
        ##{
            Df(x) = \begin{bmatrix}
                \frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \cdots & \frac{\partial f_1}{\partial x_n} \\
                \frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \cdots & \frac{\partial f_2}{\partial x_n} \\
                \vdots & \vdots & \ddots & \vdots \\
                \frac{\partial f_m}{\partial x_1} & \frac{\partial f_m}{\partial x_2} & \cdots & \frac{\partial f_m}{\partial x_n}
            \end{bmatrix} = \left[
                \frac{\partial f_i}{\partial x_j}
            \right]
        }
        where #{i} ranges row and #{j} ranges column.
    }
}