\title{Introduction to Programming Language Semantic}
\date{2024-03-11}
\taxon{Computer Science}
\import{macros}
\def\val{\text{Val}}
\def\add{\text{Add}}
\def\valuation[body]{\llbracket\body\rrbracket}
\p{
    I decided to read one paper or article every week.
    This week's topic is programming language semantics, refer to \strong{Graham Huttons}'s 
    paper [Programming language semantics: It's easy as 1,2,3](pl-123).
}
\p{
    \strong{Semantics} is the general term for the study of meaning.
    \strong{Programming language semantics} gives precise mathematical meaning to programs.
    We use a simple \strong{arithmetic expression language} 
    (including integers and addition only) to illustrate the basic concepts.
    This is an example of \strong{Occam's razor}, a philosophical principle that favours the 
    simplest explanation for a phenomenon.
}
\subtree{
    \title{Arithmetic Expressions}
    \p{
        Now let's define our language of arithmetic expressions
        built up from the set of integers and the operation of addition.
        Use a \strong{context-free} grammar.
        ##{
            E:\equiv\mathbb{Z} | E+E
        }
    }
    \p{
        An expression is either an integer value or the addition of two sub-expressions.
        We assume that parentheses can be \strong{freely} used as required to disambiguate expressions 
        written in normal textual form. This grammar can be easily translated into 
        a \strong{Haskell} data type.
    }
    \codeblock{language-haskell}{data Expr = Val Integer | Add Expr Expr}
}
\subtree{
    \title{Denotational Semantics}
    \p{
        Now we consider \strong{denotational semantics}, 
        where the terms in a language is defined using a 
        \strong{valuation function} that maps terms into values in an appropriate \strong{semantic domain}.
    }
    \p{
        Formally, for a language #{T} of syntactic terms comprises
        two components: a set #{V} of \strong{semantic values} and a 
        \strong{valuation function} of type #{T\to V} that maps terms to 
        their meanings as values.
        This function is written by enclosing a term in a \strong{semantic brackets} 
        (Also known as Oxford or Strachey brackets),
        writing #{\llbracket t\rrbracket} for the value of term #{t}.
        In addition, the valuation function is required to be \strong{compositional},
        the meaning  of a \strong{compound term} is defined purely in terms of the meaning
        of its sub-terms.
    }
    \p{
        Compositionality aids understanding by ensuring that the semantics is modular
        and supports the use of simple \strong{equational reasoning} techniques for proving properties of
        the semantics. When the set of semantic values is clear, a denotational semantics is often
        identified with the underlying valuation function.
    }
    \p{
        Taking #{V} the Haskell type \code{Integer} of integers and define a valuation function
        of type \code{Expr -> Integer} (by following equations) we can define the denotational semantics of our expression language.
        ##{
            \begin{align*}
                \llbracket\val \space n\rrbracket &= n \\
                \llbracket\add \space e_1\space e_2\rrbracket &= \llbracket e_1\rrbracket + \llbracket e_2\rrbracket
            \end{align*}
        }
        This definition satisfies the compositionality requirement obviously.
        Note that the symbol #{+} has two different purposes.
        On the left side, it is a \strong{syntactic} constructor for building terms,
        while on the right side, it is a \strong{semantic} operator for adding integers. 
    }
    \p{
        Compositionality simplifies reasoning because it allows us to 
        replace \strong{equals by equals}. For example,
        ##{
            \frac{
                \llbracket e_1\rrbracket = n_1 \quad \llbracket e_2\rrbracket = n_2
            }{
                \llbracket\add \space e_1\space e_2\rrbracket =
                \llbracket\add \space n_1\space n_2\rrbracket
            }
        }
        we can freely replace the two argument expressions of an addition by other expressions with the same meanings, 
        and the meaning of the whole addition will remain unchanged.
        Using the definition of the valuation function, we can prove this property.
        ##{
            \begin{align*}
                \llbracket\add \space e_1\space e_2\rrbracket &= 
                \llbracket e_1\rrbracket + \llbracket e_2\rrbracket (\text{By definition of } \llbracket-\rrbracket) \\
                &= \llbracket n_1\rrbracket + \llbracket n_2\rrbracket (\text{Assumptions}) \\ 
                &= \llbracket\add \space n_1\space n_2\rrbracket (\text{By definition of } \llbracket-\rrbracket)
            \end{align*}
        }
    }
    \p{
        Given that terms and their semantics are built up \strong{inductively},
        proofs about denotational semantics typically  proceed using \strong{structural induction}.
        Let us show that our expression semantics is \strong{total},
        that is, for every expression #{e} there is an integer #{n} such that #{\llbracket e\rrbracket = n}.
    }
    \proof{
        For the base case #{e = \val \space n}, we have #{\llbracket e\rrbracket = n} trivially.
        For the inductive case #{e = \add \space e_1\space e_2},
        we can assume by induction that #{\llbracket e_1\rrbracket = n_1} and #{\llbracket e_2\rrbracket = n_2}
        for some integers #{n_1} and #{n_2}. Then #{\llbracket e\rrbracket = n_1 + n_2} by definition of the valuation function,
        indicates this case is also true. Therefore, the semantics is total.
    }
    \p{
        The valuation function can be translated into a Haskell function
    }
    \codeblock{language-haskell}{\startverb
eval :: Expr -> Integer
eval (Val n) = n
eval (Add x y) = eval x + eval y
\stopverb}
    \p{
        More generally, a denotational semantics can be viewed as an evaluator (or \strong{interpreter}).
        Even \strong{eval} is defined recursively, the semantics is compositional its behavior
        can be understood  as simply replacing the \strong{constructors} for expressions by other functions.
        In this manner, a denotational semantics can also be viewed as an evaluation function that
        is defined by \strong{folding} over the syntax of the source language.
    }
    \codeblock{language-haskell}{\startverb
eval :: Expr -> Integer
eval = fold id (+)
\stopverb}
    \p{
        The fold operator captures the ideas of replacing constructors
        of the language by other functions
    }
    \codeblock{language-haskell}{\startverb
fold :: (Integer -> a) -> (a -> a -> a) -> Expr -> a 
fold f g (Val n) = f n
fold f g (Add x y) = g (fold f g x) (fold f g y)
\stopverb}
    \p{
        Note that the above semantics for expressions does not specify the order
        of evaluation. If we do wish to make evaluation order explicit 
        this requires the introduction of additional structure into the semantics,
        named \strong{abstract machines} (Discuss later).
    }
}
\subtree{
    \title{Small-Step Operational Semantics}
    \p{
        Another popular approach to semantics is the \strong{operational approach},
        where the meaning of terms is defined using an \strong{execution relation}
        that specifies how terms can be executed in an appropriate machine model.
        There are two basic forms of operational semantics:
        \ul{
            \li{
                \strong{small-step}: describes the individual steps of execution
            }
            \li{
                \strong{big-step}: describes the overall results of execution
            }
        }
        In this section we consider the small-step approach, 
        which is also known as \strong{structural operational semantics}.
    }
    \p{
        Formally, a small-step operational semantics for a language #{T} of syntactic terms
        comprises two components:
        a set #{S} of \strong{execution states} and 
        a \strong{transition relation} of type #{S\to S} that specifies how terms can be executed.
        If there is a transition from state #{s} to state #{s'} in a single execution step, we write #{s\to s'}.
    }
    \p{
        Arithmetic expressions have a simple small-step operational semantics,
        given by taking #{S} as the Haskell type. And we define transition relation 
        on \code{Expr} by the following inference rules.
        ##{
            \begin{align*}
                \frac{}{
                    \add\space(\val\space n_1)\space(\val\space n_2)\to\val\space(n_1+n_2)
                } \\
                \frac{x\to x'}{\add \space x\space y\to\add \space x'\space y} 
                \quad 
                \frac{y\to y'}{\add \space x\space y\to\add \space x\space y'}
            \end{align*}
        }
    }
    \p{
        The first rule states that two values can be added to give a single value and is called a
        \strong{reduction} (or \strong{contraction}) rule.
        An expression that matches such a rule is termed a reducible expression or \strong{redex}.
        The last two rules are called \strong{structural} (or \strong{congruence}) rules as 
        they specify how larger terms can be reduced.
    }
    \p{
        The semantics is \strong{non-deterministic} because an expression
        may have more than one possible transition.
        This is obviously from the structural rules, which allow either sub-expression to be reduced first.
    }
    \p{
        We can now capture a the relation between the denotational and operational semantics,
        namely that making a transition does not change the denotation of an expression.
        ##{
            \frac{
                e\to e'
            }{
                \valuation{e} = \valuation{e'}
            }
        }
        This property can be proved by induction on the structure of the expression #{e}.
        Note that by using the "equals by equals" and the assumption #{x\to x'} we can easily 
        prove the inductive case. The details are omitted here as it involves quite a bit of 
        case analysis. We will later see the \strong{principle of rule induction}, which gives 
        a simpler and more direct way to prove such properties.
    }
    \p{
        Evaluation of an expression using the small-step semantics proceeds by a series of zero
        or more transition steps. Formally we can write #{e\to^* e'} to indicate that #{e} can be
        reduced to #{e'} in zero or more steps.
        We can generate a transition tree that captures all possible execution
        paths for an expression. Using the list comprehension we can define a 
        function that returns the list of all expressions that can be reduced 
        from a given expression #{e} in a single transition.
    }
    \codeblock{language-haskell}{\startverb
trans :: Expr -> [Expr]
trans (Val n) = []
trans (Add (Val n) (Val m)) = [Val (n + m)]
trans (Add x y) = [Add x' y | x' <- trans x] ++ [Add x y' | y' <- trans y]
\stopverb}
    \p{
        We can define a Haskell datatype for transition trees 
        and an execution function that converts expressions into 
        transition trees.
    }
    \codeblock{language-haskell}{\startverb
data Tree a = Node a [Tree a]
exec :: Expr -> Tree Expr
exec e = Node e [exec e' | e' <- trans e]
\stopverb}
    \p{
        Though \code{exec} is defined recursively, its behavior can be understood as simply applying
        the identity function to give the root of the tree and the transition function to generate a 
        list of residual expressions to be processed to give the subtrees.
        A small-step semantics can be viewed as giving rise to an execution
        function that is defined by \strong{unfolding} to transition trees.
    }
    \codeblock{language-haskell}{\startverb
exec :: Expr -> Tree Expr
exec = unfold id trans 
\stopverb}
    \p{
        The \code{unfold} function captures the idea of generating a tree 
        from a seed value #{x} by applying a function #{f} to give the root 
        and a function #{g} to give a list of residual values to be processed
        for the subtrees.
    }
    \codeblock{language-haskell}{\startverb
unfold :: (t -> a) -> (t -> [t]) -> t -> Tree a
unfold f g x = Node (f x) [unfold f g x' | x' <- g x]
\stopverb}
    \p{
        The operational semantics corresponds to \strong{unfolding to transition trees},
        while denotational semantics corresponds to \strong{folding over syntax trees}.
        Thinking about semantics in terms of recursion operators reveals a duality
    }
    \p{
        The above semantics for expressions does not specify the order of evaluation.
        But we can modify the inference rules to achieve this. Replace the second #{\add} 
        rule by the following rule ensures the first argument of addition is 
        always reduced first.
        ##{
            \frac{
                y\to y'
            }{
                \add (\val\space n)\space y \to \add (\val\space n)\space y'
            }
        }
    }
}
\subtree{
    \title{Rule induction}
    \p{
        For denotational semantics we have structural induction,
        dual to this, for operational semantics we have \strong{rule induction}.
        This allows us to perform proofs by considering the structure of the rules 
        that are used to define the semantics.
    }
    \p{
        We introduce the idea of rule induction using a simple numeric example.
        We begin by inductively defining a set of natural numbers.
        ##{
                \frac{}{0\in\N} \quad 
                \frac{n\in\N}{n+1\in\N}
        }
        This is the standard definition of the natural numbers using peano axioms,
        where the first rule states that zero is a natural number and the second rule states that
        if #{n} is a natural number then so is #{n+1}.
    }
    \p{
        For the inductively defined set #{\N}. The principle of rule induction
        states that in order to prove a property #{P(n)} for all natural numbers #{n},
        it suffices to prove that #{P(0)} holds and that if #{P(n)} holds then #{P(n+1)} holds.
        ##{
            \frac{
                P(0)\quad\forall n\in\N. P(n)\to P(n+1)
            }{
                \forall n\in\N. P(n)
            }
        }
        Notice that this is the well-known \strong{principle of mathematical induction}.
    }
    \p{
        The concept of rule induction can easily be generalised to multiple base and 
        inductive cases, to rule with multiple preconditions and so on.
        For the small-step semantics of expressions, we have one base case and two inductive cases.
        Hence if we want to show that some property #{P(e,e')} on pairs of expression holds for 
        all transition #{e\to e'}, we can use rule induction:
        ##{
            \frac{
                \begin{align*}
                    P(\add\space(\val\space n_1)\space(\val\space n_2),\val\space(n_1+n_2)) \\
                    \forall x\to x'. P(x,x')\to P(\add\space x\space y,\add\space x'\space y) \\
                    \forall y\to y'. P(y,y')\to P(\add\space x\space y,\add\space x\space y')
                \end{align*}
            }{
                \forall e\to e'. P(e,e')
            }
        }
        We write #{\forall x\to y.P(x,y)} as shorthand for 
        ##{\forall x,y.x\to y\Rightarrow P(x,y)}. Now we give the proof 
        of the property #{\valuation{e} = \valuation{e'}} for all transitions #{e\to e'}.
        ##{
            \forall e\to e'. \valuation{e} = \valuation{e'}
        }
    }
    \proof{
        The proof consists of three parts: the base case, the reduction rule and the structural rule.
        ##{
            \begin{align*}
                \valuation{\add\space(\val\space n)\space(\val\space m)}
                &= \valuation{\val\space n} + \valuation{\val\space m} \\
                &= n + m \\
                &= \valuation{\val\space(n+m)}
            \end{align*}
        }
        and
        ##{
            \begin{align*}
                \valuation{\add\space x\space y}
                &= \valuation{x} + \valuation{y}  \\
                &= \valuation{x'} + \valuation{y} (\text{By assumption }\valuation{x}=\valuation{x'}) \\
                &= \valuation{\add\space x'\space y}
            \end{align*}
        }
        and 
        ##{
            \begin{align*}
                \valuation{\add\space x\space y}
                &= \valuation{x} + \valuation{y}  \\
                &= \valuation{x} + \valuation{y'} (\text{By assumption }\valuation{y}=\valuation{y'}) \\ 
                &= \valuation{\add\space x\space y'}
            \end{align*}
        }
    }
}
\subtree{
    \def\hole{[-]}
    \def\addl[a0][a1]{\text{AddL}\space\a0\space\a1}
    \def\addr[a0][a1]{\text{AddR}\space\a0\space\a1}
    \title{Contextual Semantics}
    \p{
        The small-step semantics for expressions above has one basic reduction rule 
        for adding values and two structural rules that allow addition to be performed
        in larger expressions. Separating these two kinds of rules results 
        the notion of \strong{contextual semantics} (or \strong{reduction semantics}).
    }
    \p{
        Informally, a context in this setting is a term with a "\strong{hole}", 
        usually written as #{\hole}, which can be filled with another term later.
        In a contextual semantics, the hole represents the location where a \strong{single}
        basic step of execution may take place within a term.
    }
    \p{
        Consider the following transition in our small-step semantics.
        ##{
            (1+2)+(3+4)\to 3+(3+4)
        }
        The addition is performed on the left side.
        We say that we can perform the basic step #{1+2\to3} in the 
        \strong{context} #{\hole+(3+4)} where #{\hole} implies the location of the
        addition takes place.
    }
    \p{
        The language #{C} of contexts of arithmetic expressions can be formally defined by 
        ##{
            C:\equiv\hole|C+E|E+C
        }
        As previously, we can define a Haskell datatype for contexts.
    }
    \codeblock{language-haskell}{data Cont = Hole | Add Cont Expr | Add Expr Cont}
    \p{
        The kind of context is known as "outside-in" as locating the hole involves
        navigating from the outside of the context inwards. To fill the hole in 
        a context #{c} with an expression #{e} we write #{c\space[e]}:
        ##{
            \begin{align*}
                \text{Hole}\space[e] &= e \\
                (\addl{c}{r})\space[e] &= \add\space (c\space[e])\space r \\
                (\addr{l}{c})\space[e] &= \add\space l\space(c\space[e])
            \end{align*}
        }
    }
    \p{
        Use the idea of hole filling we can redefine the small-step semantics
        for expressions in contextual style.
        ##{
            \frac{}{
                \add\space(\val\space n_1)\space(\val\space n_2)\rightarrowtail \val\space(n_1+n_2)
            } \quad
            \frac{
                e \rightarrowtail e'
            }{
                c\space [e] \to c\space [e']
            }
        }
        This first rule defines a reduction relation #{\rightarrowtail} that captures the basic behavior of addition,
        while the second defines a transition relation #{\to} that allows the first rule to be applied in 
        any context.
    }
    \p{
        We have now refactored the small-step semantics into a single reduction rule and a single structural rule.
        If we subsequently want to extend the language with other features,
        it only requires adding new reduction rules and extending the notion of contexts
        but not need to adding new structural rules.
    }
    \p{
        Now we define the hole filling function in Haskell.
    }
    \codeblock{language-haskell}{\startverb
fill :: Cont -> Expr -> Expr
fill Hole e = e
fill (AddL c r) e = Add (fill c e) r
fill (AddR l c) e = Add l (fill c e)
\stopverb}
    \p{The dual operation which splits an expression into all possible pairs of 
    contexts and expressions can be defined:}
    \codeblock{language-haskell}{\startverb
split :: Expr -> [(Cont,Expr)]
split e = (Hole, e) : case e of
  Val n -> []
  Add l r -> [(AddL c r, x) | (c, x) <- split l] ++ 
             [(AddR l c, x) | (c, x) <- split r]
\stopverb}
    \p{
        A pair #{(c,x)} comprising a context #[c] and an expression #{x} is
        an element of the list returned by \code{split e} precisely when \code{fill c x = e}.
        The contextual semantics can be translated into Haskell function that returns
        the lists of all expressions that can be reached by performing a single reduction step.
    }
    \codeblock{language-haskell}{\startverb
reduce :: Expr -> [Expr]
reduce (Add (Val n) (Val m)) = [Val (n + m)]
reduce _ = []
\stopverb}
    \p{
        or a single transition step.
    }
    \codeblock{language-haskell}{\startverb
trans :: Expr -> [Expr]
trans e = [fill c x' | (c, x) <- split e, x' <- reduce x]
\stopverb}
    \p{
        This function splits the given expression into all possible context 
        and expression pairs, then considering any reduction that can made by each component
        expression, and finally filling the hole in the context with the reduced expression.
    }
    \p{
        Notice that the contextual semantics does not specify an evaluation
        order for addition and is non-deterministic. We can modify the language
        of contexts to achieve so.
        ##{
            C:\equiv\hole|C+E|\Z+C
        }
        This version of the semantics also satisfies a \strong{unique decomposition property},
        that is, any expression #{e} that is not a value can be uniquely decomposed into the 
        form #{e=c\space[x]} for some context #{c} and reducible expression #{x}. This can be 
        proved by induction on the expression #{e}.
    }
    \p{
        Contexts are related to a number of other important concepts in
        programming and semantics, including the use of continuations to make control flow
        explicit (John C. Reynolds 1972).
    }
}
\subtree{
    \title{Big-step Semantics}
    \def\evalto{\Downarrow}
    \p{
        Big-step semantics is also known as \strong{natural semantics},
        which focus on large execution step. For a language T of syntactic terms
        comprises two components: a set #{V} of values and 
        an evaluation relation between #{T} and #{V} that relates each term 
        to all values that can be reached by fully executing the term.
        If a term #{t} and a value #{v} are related, we say that #{t} can evaluate to #{v} 
        and write this as #{t\Downarrow v}
    }
    ##{
        \frac{}{
            \val\space n\evalto n
        }
        \quad
        \frac{
            x\evalto n\quad y\evalto m 
        }{
            \add\space x\space y\evalto n+m
        }
    }
    \p{The Haskell function definition is similar to the small-step semantics,
    by using the list comprehension.}
    \codeblock{language-haskell}{\startverb
eval :: Expr -> [Integer]
eval (Val n) = [n]
eval (Add x y) = [n + m | n <- eval x, m <- eval y]
\stopverb}
    \p{
        This is similar to denotational semantics but using inference rules
        rather than a functional manner.
        However, there is no need for a big-step semantics to be compositional.
        Formally, the denotational and big-step semantics for the expression language
        are equivalent, which can be captured by the following property:
        ##{
            \valuation{e} = n \iff e\evalto n
        }
        This is easily verified by induction.
    }
}
\subtree{
    
    \title{Abstract Machine}
    \p{
        All the exmaples we have meet focused on explaining semantic ideas,
        now we show how language of integers and addition can also be used to help 
        discover semantic ideas.
        We show how it can be used as the basis for discovering how to implement 
        an \strong{abstract machine}.
    }
    \p{
        Remember the evaluation order problem we meet before,
        if we want to make evaluation order explicit, we can introduce additional
        structure into the semantics by constructing an abstract machine.
    }
    \p{
        Formally, an abstract machine is usually deifned by a set of syntactic rewrite ruless 
        that make explicit how each step of evaluation proceeds.
        This section we show how an abstract machine for our example language can 
        be systematically derived from the evaluation function using steps
        based on two important semantic concepts, \strong{continuations} and \strong{defunctionalisation}.
    }
    \subtree{
        \title{Add Continuations}
        \def\eval1{\text{eval}'}
        \def\eval{\text{eval}}
        \p{
            We want to make the order of evaluation explicit in the semantics itself.
            A standard technique for achieving this aim is to rewrite the semantics
            in \strong{continuation-passing style}. 
        }
        \p{
            In our setting, a continuation is a function that will be applied to the result of an
            evaluation. Formally, for our semantics \code{eval :: Expr -> Integer}, a continuation
            is a function of type \code{Integer -> Integer} that will be applied to the resulting integer
            to give a new integer. (This type can be generalized to \code{Integer -> a}, but we do not need now)
            We capture the notion of such a continuation using the following type declaration:
        }
        \codeblock{language-haskell}{type Continuation = Integer -> Integer}
        \p{
            We now define a new semantics \code{eval'}, which takes an expression and returns
            an integer as previously but also takes a continuation as an additional argument,
            which is applied to the result of evaluating the expression.
        }
        \codeblock{language-haskell}{eval' :: Expr -> Continuation -> Integer}
        \p{
            The behavior of \code{eval'} should be:
        }
        ##{
            \text{eval' e c} = \text{c (eval e)}
        }
        \p{
            We want to calculate a definition that satisfies the specification.
            Using structural induction on the expression #{e}, we construct 
            the term \code{eval' e c} by gradually removing the reference to \code{eval}.
            For the base case, #{e = \val\space n} we have
        }
        ##{
            \begin{align*}
                \eval1\space(\val\space n)\space c 
                &= c\space (\eval\space (\val\space n)) \\
                &= c\space n
            \end{align*}
        }
        \p{
            For the inductive case, #{e = \add\space x\space y} we begin in the same way:
        }
        ##{
            \begin{align*}
                \eval1\space(\add\space x\space y)\space c 
                &= c\space (\eval\space (\add\space x\space y)) \\
                &= c\space (\eval\space x + \eval\space y)
            \end{align*}
        }
        \p{
            However, no further definition can be applied now, so we consider the inductive hypothese:
            Forall #{c'} and #{c''} we have #{\eval1\space x\space c' = c'(\eval\space x)} and
            #{\eval1\space y\space c'' = c''(\eval\space y)}. 
        }
    }
}