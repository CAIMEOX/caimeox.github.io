<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="true" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>1355</fr:anchor><fr:addr type="user">def-0025</fr:addr><fr:route>def-0025.xml</fr:route><fr:title text="Linear Map">Linear Map</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>A <fr:strong>linear map</fr:strong> is a function between two vector spaces that preserves the operations of addition and scalar multiplication.
    In other words, a function <fr:tex display="inline">T: V \to  W</fr:tex> where <fr:tex display="inline">V,W</fr:tex> are vector spaces if the following conditions are satisfied:
    <fr:ul><fr:li>Additivity: <fr:tex display="inline">T(u+v) = T(u) + T(v)</fr:tex> for all <fr:tex display="inline">u,v \in  V</fr:tex></fr:li>
        <fr:li>Homogeneity: <fr:tex display="inline">T(\alpha  v) = \alpha  T(v)</fr:tex> for all <fr:tex display="inline">\alpha  \in  \mathbb {F}</fr:tex> and <fr:tex display="inline">v \in  V</fr:tex></fr:li></fr:ul>
    Sometimes we ignore the brackets and write <fr:tex display="inline">T v</fr:tex> instead of <fr:tex display="inline">T(v)</fr:tex>.</fr:p></fr:mainmatter><fr:backmatter><fr:tree toc="false" numbered="false" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:title text="Context">Context</fr:title><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:tree toc="true" numbered="false" show-heading="true" show-metadata="true" expanded="false" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>1356</fr:anchor><fr:addr type="user">math-0005</fr:addr><fr:route>math-0005.xml</fr:route><fr:title text="Linear Maps">Linear Maps</fr:title><fr:taxon>Linear Algebra</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>1</fr:month><fr:day>31</fr:day></fr:date><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>This note introduces the concept of linear maps.
    Refer to <fr:link type="local" href="linear-algebra-2015.xml" addr="linear-algebra-2015" title="Linear Algebra Done Right">Linear Algebra Done Right</fr:link>.</fr:p><fr:p>Now we arrive at the main topic of this chapter: linear maps. 
    In classic mathematics, to understand the properties of the structure or space,
    we often study the maps between them.
    For vector spaces we study the <fr:strong>linear maps</fr:strong>.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>556</fr:anchor><fr:addr type="user">def-0025</fr:addr><fr:route>def-0025.xml</fr:route><fr:title text="Linear Map">Linear Map</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>A <fr:strong>linear map</fr:strong> is a function between two vector spaces that preserves the operations of addition and scalar multiplication.
    In other words, a function <fr:tex display="inline">T: V \to  W</fr:tex> where <fr:tex display="inline">V,W</fr:tex> are vector spaces if the following conditions are satisfied:
    <fr:ul><fr:li>Additivity: <fr:tex display="inline">T(u+v) = T(u) + T(v)</fr:tex> for all <fr:tex display="inline">u,v \in  V</fr:tex></fr:li>
        <fr:li>Homogeneity: <fr:tex display="inline">T(\alpha  v) = \alpha  T(v)</fr:tex> for all <fr:tex display="inline">\alpha  \in  \mathbb {F}</fr:tex> and <fr:tex display="inline">v \in  V</fr:tex></fr:li></fr:ul>
    Sometimes we ignore the brackets and write <fr:tex display="inline">T v</fr:tex> instead of <fr:tex display="inline">T(v)</fr:tex>.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>Now we can talk about the set of all linear maps between two vector spaces.
    <fr:tex display="block">         \mathcal {L}(V,W) = \{  T: V \to  W | T \text { is a linear map} \}     </fr:tex></fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>557</fr:anchor><fr:addr type="user">eg-0002</fr:addr><fr:route>eg-0002.xml</fr:route><fr:title text="Differentiation is linear map">Differentiation is linear map</fr:title><fr:taxon>Example</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Define <fr:tex display="inline">D\in \mathcal {L}(\mathcal {P}(\mathbb {R}),\mathcal {P}(\mathbb {R}))</fr:tex> (recall that <fr:tex display="inline">\mathcal {P}</fr:tex> means <fr:link type="local" href="def-0027.xml" addr="def-0027" title="Polynomial">set of polynomials</fr:link>) by
    <fr:tex display="block">         D(f) = f&apos;     </fr:tex>
    We can see that <fr:tex display="inline">D</fr:tex> a linear map.
    <fr:ul><fr:li>Additivity: <fr:tex display="inline">D(f+g) = (f+g)&apos; = f&apos; + g&apos; = D(f) + D(g)</fr:tex></fr:li>
        <fr:li>Homogeneity: <fr:tex display="inline">D(\alpha  f) = (\alpha  f)&apos; = \alpha  f&apos; = \alpha  D(f)</fr:tex></fr:li></fr:ul></fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>558</fr:anchor><fr:addr type="user">eg-0003</fr:addr><fr:route>eg-0003.xml</fr:route><fr:title text="Integration is linear map">Integration is linear map</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Let <fr:tex display="inline">V</fr:tex> be the vector space of all continuous functions on the interval <fr:tex display="inline">[a,b]</fr:tex>.
    The map <fr:tex display="inline">I: V \to  V</fr:tex> defined by
    <fr:tex display="block">         I(f) = \int _a^x f(t) dt     </fr:tex>
    is a <fr:strong>linear map</fr:strong>.
    In other words, <fr:tex display="inline">I</fr:tex> preserves the operations of addition and scalar multiplication:
    For all <fr:tex display="inline">f,g \in  V</fr:tex> and all <fr:tex display="inline">\alpha  \in  \mathbb {R}</fr:tex>,
    <fr:tex display="block">         I(f+g) = I(f) + I(g) \quad  \text {and} \quad  I(\alpha  f) = \alpha  I(f)     </fr:tex></fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>We can find a linear map that takes on <fr:em>whatever values we wish</fr:em> on the 
    vectors in a basis by the following theorem.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>559</fr:anchor><fr:addr type="user">thm-000F</fr:addr><fr:route>thm-000F.xml</fr:route><fr:title text="Linear maps and basis of domain">Linear maps and basis of domain</fr:title><fr:taxon>Theorem</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Let <fr:tex display="inline">v_1, v_2, \ldots , v_n</fr:tex> be a basis of vector space <fr:tex display="inline">V</fr:tex>.
    Then for any vector space <fr:tex display="inline">W</fr:tex> and any vectors <fr:tex display="inline">w_1, w_2, \ldots , w_n</fr:tex> in <fr:tex display="inline">W</fr:tex>,
    there exists a unique linear map <fr:tex display="inline">T: V \to  W</fr:tex> such that
    <fr:tex display="block">         T(v_i) = w_i \quad  \text {for all} \quad  i = 1,2,\ldots ,n     </fr:tex></fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>Now let&apos;s turn to the algebraic operations over the set of linear maps <fr:tex display="inline">\mathcal {L}(V,W)</fr:tex>.
    We begin by defining the addition and scalar multiplication of linear maps.
    This leads to a surprising result: the set of linear maps is actually a vector space.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>560</fr:anchor><fr:addr type="user">def-0029</fr:addr><fr:route>def-0029.xml</fr:route><fr:title text="Addition and scalar multiplication over {L}(V,W)">Addition and scalar multiplication over <fr:tex display="inline">\mathcal {L}(V,W)</fr:tex></fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Let <fr:tex display="inline">T_1, T_2 \in  \mathcal {L}(V,W)</fr:tex>.
    We define the <fr:strong>addition</fr:strong> of <fr:tex display="inline">T_1</fr:tex> and <fr:tex display="inline">T_2</fr:tex> as the linear map <fr:tex display="inline">T_1 + T_2: V \to  W</fr:tex> such that
    <fr:tex display="block">         (T_1 + T_2)(v) = T_1(v) + T_2(v) \quad  \text {for all} \quad  v \in  V     </fr:tex>
    The scalar multiplication of a linear map <fr:tex display="inline">T \in  \mathcal {L}(V,W)</fr:tex> by a scalar <fr:tex display="inline">c \in  \mathbb {F}</fr:tex> is the linear map <fr:tex display="inline">cT: V \to  W</fr:tex> such that
    <fr:tex display="block">         (cT)(v) = cT(v) \quad  \text {for all} \quad  v \in  V     </fr:tex>
    With these operations, <fr:tex display="inline">\mathcal {L}(V,W)</fr:tex> is a <fr:link type="local" href="def-000H.xml" addr="def-000H" title="Vector Space"><fr:strong>vector space</fr:strong></fr:link> over the field <fr:tex display="inline">\mathbb {F}</fr:tex>.
    Note that the additive identity of <fr:tex display="inline">\mathcal {L}(V,W)</fr:tex> is the <fr:strong>zero map</fr:strong> <fr:tex display="inline">0: V \to  W</fr:tex> such that
    <fr:tex display="block">         0(v) = 0 \quad  \text {for all} \quad  v \in  V     </fr:tex></fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>Usually it makes no sense to multiply two linear maps. But we can define
    an operation called the <fr:strong>product</fr:strong> of linear maps, which is just the composition of the two functions.
    This can form a <fr:strong>monoid</fr:strong> or even a <fr:strong>group</fr:strong> under certain conditions.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>561</fr:anchor><fr:addr type="user">def-002A</fr:addr><fr:route>def-002A.xml</fr:route><fr:title text="Product of Linear Maps">Product of Linear Maps</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Let <fr:tex display="inline">T_1: V \to  W</fr:tex> and <fr:tex display="inline">T_2: W \to  U</fr:tex> be linear maps.
    We define the <fr:strong>product</fr:strong> of <fr:tex display="inline">T_1</fr:tex> and <fr:tex display="inline">T_2</fr:tex> as the linear map <fr:tex display="inline">T_2 \circ  T_1: V \to  U</fr:tex> such that
    <fr:tex display="block">         (T_2 \circ  T_1)(v) = T_2(T_1(v)) \quad  \text {for all} \quad  v \in  V     </fr:tex>
    Note that this is just the composition of the two functions <fr:tex display="inline">T_1</fr:tex> and <fr:tex display="inline">T_2</fr:tex>. 
    And we usually denote <fr:tex display="inline">T_2 \circ  T_1</fr:tex> by <fr:tex display="inline">T_2T_1</fr:tex>.
    The product of linear maps is associative, that is,
    <fr:tex display="block">         (T_3 \circ  T_2) \circ  T_1 = T_3 \circ  (T_2 \circ  T_1)     </fr:tex>
    for any linear maps <fr:tex display="inline">T_1: V \to  W</fr:tex>, <fr:tex display="inline">T_2: W \to  U</fr:tex>, and <fr:tex display="inline">T_3: U \to  X</fr:tex>.
    The identity map <fr:tex display="inline">I_V: V \to  V</fr:tex> is the identity element of the set of linear maps <fr:tex display="inline">\mathcal {L}(V,V)</fr:tex> under the product operation.
    That is, for any linear map <fr:tex display="inline">T: V \to  V</fr:tex>,
    <fr:tex display="block">         I_V \circ  T = T \circ  I_V = T     </fr:tex>
    where <fr:tex display="inline">I_V</fr:tex> is the identity map on <fr:tex display="inline">V</fr:tex>.
    The set of all linear maps from a vector space to itself, <fr:tex display="inline">\mathcal {L}(V,V)</fr:tex>, forms a <fr:link type="local" href="def-0007.xml" addr="def-0007" title="Monoid"><fr:strong>monoid</fr:strong></fr:link> under the product operation.
    The set of all invertible linear maps from a vector space to itself, <fr:tex display="inline">\mathcal {L}(V,V)^*</fr:tex>, forms a group under the product operation.
    The identity map is the identity element of the <fr:link type="local" href="def-0001.xml" addr="def-0001" title="Group"><fr:strong>group</fr:strong></fr:link> <fr:tex display="inline">\mathcal {L}(V,V)^*</fr:tex>.</fr:p><fr:p>With addition we also have the distributive law for the product of linear maps.
    That is, for any linear maps <fr:tex display="inline">S,S_1,S_2: V \to  W</fr:tex> and <fr:tex display="inline">T,T_1,T_2: U\to  V</fr:tex>:
    <fr:tex display="block">         (S_1 + S_2)T = S_1T + S_2T \quad  \text {and} \quad  T(S_1 + S_2) = TS_1 + TS_2     </fr:tex></fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>In algebra, we have a structure named <fr:strong>kernel</fr:strong>, which is the set of all elements that are mapped to the zero element.
    For linear maps, the kernel is the <fr:strong>null space</fr:strong></fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>562</fr:anchor><fr:addr type="user">def-002C</fr:addr><fr:route>def-002C.xml</fr:route><fr:title text="Null Space">Null Space</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>For <fr:tex display="inline">T: V \to  W</fr:tex>, the <fr:strong>null space</fr:strong> of <fr:tex display="inline">T</fr:tex> is the set of all vectors in <fr:tex display="inline">V</fr:tex> that are mapped to <fr:tex display="inline">0</fr:tex> in <fr:tex display="inline">W</fr:tex>.
    <fr:tex display="block">         \text {null } T = \{ v \in  V | T(v) = 0 \}     </fr:tex>
    The null space of <fr:tex display="inline">T</fr:tex> is a subspace of <fr:tex display="inline">V</fr:tex>.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>The injective linear map is defined like normal <fr:link type="local" href="def-002D.xml" addr="def-002D" title="Injective">injective</fr:link> functions.
    To check whether a linear map is injective, we can just check whether the null space is trivial.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>563</fr:anchor><fr:addr type="user">thm-000G</fr:addr><fr:route>thm-000G.xml</fr:route><fr:title text="Injectivity equivalent to Kernel Triviality">Injectivity equivalent to Kernel Triviality</fr:title><fr:taxon>Theorem</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Let <fr:tex display="inline">T: V \to  W</fr:tex> be a linear map. Then <fr:tex display="inline">T</fr:tex> is injective if and only if <fr:tex display="inline">\text {null } T = \{ 0 \}</fr:tex>.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>The image of a linear map is the set of all elements that are mapped to by some element in the domain.
    This is called the <fr:strong>range</fr:strong> of the linear map, just like <fr:link type="local" href="def-002E.xml" addr="def-002E" title="Range">range</fr:link> of normal function.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>564</fr:anchor><fr:addr type="user">thm-000H</fr:addr><fr:route>thm-000H.xml</fr:route><fr:title text="Range is a subspace">Range is a subspace</fr:title><fr:taxon>Theorem</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>If <fr:tex display="inline">T: V \to  W</fr:tex> is a linear map, then the range of <fr:tex display="inline">T</fr:tex> is a subspace of <fr:tex display="inline">W</fr:tex>.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>The next theorem plays a crucial role in the study of linear maps.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>565</fr:anchor><fr:addr type="user">thm-000I</fr:addr><fr:route>thm-000I.xml</fr:route><fr:title text="Fundamental Theorems of Linear Maps">Fundamental Theorems of Linear Maps</fr:title><fr:taxon>Theorem</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Let <fr:tex display="inline">V</fr:tex> be finite-dimensional vector space and <fr:tex display="inline">T : V \to  W</fr:tex> be a linear map. 
    Then <fr:tex display="inline">\text {range } T</fr:tex> is finite-dimensional and 
    <fr:tex display="block">         \dim  V = \dim  \text {range } T + \dim  \text {null } T     </fr:tex></fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>Now we can show that no linear map from a finite-dimensional vector space
    to a <fr:em>smaller</fr:em> (In dimension) vector space can be <fr:link type="local" href="def-002D.xml" addr="def-002D" title="Injective">injective</fr:link>.
    This can be easily proved by the fundamental theorem of linear maps.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>566</fr:anchor><fr:addr type="user">thm-000M</fr:addr><fr:route>thm-000M.xml</fr:route><fr:title text="Map to smaller dimension is not injective">Map to smaller dimension is not injective</fr:title><fr:taxon>Lemma</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Let <fr:tex display="inline">V</fr:tex> and <fr:tex display="inline">W</fr:tex> be finite-dimensional vector spaces, 
    and <fr:tex display="inline">\dim  V &gt; \dim  W</fr:tex>.
    Then no linear map <fr:tex display="inline">T:V\to  W</fr:tex> is injective.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>Similarly, we can show that no linear map from a finite-dimensional vector space
    to a <fr:em>larger</fr:em> (In dimension) vector space can be <fr:link type="local" href="def-002F.xml" addr="def-002F" title="Surjective">surjective</fr:link>.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>567</fr:anchor><fr:addr type="user">thm-000N</fr:addr><fr:route>thm-000N.xml</fr:route><fr:title text="Map to bigger dimension is not surjective">Map to bigger dimension is not surjective</fr:title><fr:taxon>Lemma</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Let <fr:tex display="inline">V</fr:tex> and <fr:tex display="inline">W</fr:tex> be finite-dimensional vector spaces, 
    and <fr:tex display="inline">\dim  V &lt; \dim  W</fr:tex>.
    Then no linear map <fr:tex display="inline">T:V\to  W</fr:tex> is surjective.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>These two lemmas are very important in the study of linear equations.
    The idea here is to express linear equations system in terms of linear maps.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>568</fr:anchor><fr:addr type="user">eg-0004</fr:addr><fr:route>eg-0004.xml</fr:route><fr:title text="Homogeneous Linear Equations System">Homogeneous Linear Equations System</fr:title><fr:taxon>Example</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Reprase in terms of a linear map the question of whether a <fr:link type="local" href="def-002Q.xml" addr="def-002Q" title="Homogeneous Linear Equations">homogeneous system linear equations</fr:link> has a nonzero solution.</fr:p><fr:p>Let <fr:tex display="inline">A</fr:tex> be the coefficient matrix of a homogeneous linear system.
        <fr:tex display="block">             A = \begin {bmatrix}                 a_{11} &amp; a_{12} &amp; \cdots  &amp; a_{1n} \\                 a_{21} &amp; a_{22} &amp; \cdots  &amp; a_{2n} \\                 \vdots  &amp; \vdots  &amp; \ddots  &amp; \vdots  \\                 a_{m1} &amp; a_{m2} &amp; \cdots  &amp; a_{mn}             \end {bmatrix}         </fr:tex>
        The equation <fr:tex display="inline">A\vec {x} = \vec {0}</fr:tex> has a trivial solution <fr:tex display="inline">\vec {x} = \vec {0}</fr:tex>.
        The question here is whether there is a nontrivial solution.</fr:p><fr:p>Define <fr:tex display="inline">T: \mathbb {F}^n \to  \mathbb {F}^m</fr:tex> by
        <fr:tex display="block">             T(\vec {x}) = A\vec {x}         </fr:tex>
        Then the question of whether the homogeneous linear system has a nontrivial solution is equivalent to 
        asking <fr:tex display="inline">\text {null } T</fr:tex> is nontrivial.
        That is, <fr:tex display="inline">T</fr:tex> is <fr:link type="local" href="thm-000G.xml" addr="thm-000G" title="Injectivity equivalent to Kernel Triviality">not injective</fr:link>.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>569</fr:anchor><fr:addr type="user">thm-000O</fr:addr><fr:route>thm-000O.xml</fr:route><fr:title text="Homogeneous system of linear equations">Homogeneous system of linear equations</fr:title><fr:taxon>Theorem</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>A homogeneous system of linear equations
    with more variables than equations has 
    a nontrivial solution.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>We have seen that <fr:link type="local" href="thm-000M.xml" addr="thm-000M" title="Map to smaller dimension is not injective">map to smaller dimension is not injective</fr:link>.
    <fr:tex display="inline">T</fr:tex> is not injective if <fr:tex display="inline">n &gt; m</fr:tex>. This results the theorem above.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>570</fr:anchor><fr:addr type="user">eg-0005</fr:addr><fr:route>eg-0005.xml</fr:route><fr:title text="Inhomogeneous Linear Equations System">Inhomogeneous Linear Equations System</fr:title><fr:taxon>Example</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Rephrase in terms of a linear map the question of whether a inhomogeneous system linear equations has no solutions
        for some choice of constant terms.</fr:p><fr:p>Let <fr:tex display="inline">A</fr:tex> be the coefficient matrix of a inhomogeneous linear system.
    <fr:tex display="block">         A = \begin {bmatrix}             a_{11} &amp; a_{12} &amp; \cdots  &amp; a_{1n} \\             a_{21} &amp; a_{22} &amp; \cdots  &amp; a_{2n} \\             \vdots  &amp; \vdots  &amp; \ddots  &amp; \vdots  \\             a_{m1} &amp; a_{m2} &amp; \cdots  &amp; a_{mn}         \end {bmatrix}     </fr:tex>
    The equation <fr:tex display="inline">A\vec {x} = \vec {b}</fr:tex> has a solution <fr:tex display="inline">\vec {x} = A^{-1}\vec {b}</fr:tex>.</fr:p><fr:p>Define <fr:tex display="inline">T: \mathbb {F}^n \to  \mathbb {F}^m</fr:tex> by
        <fr:tex display="block">             T(\vec {x}) = A\vec {x}         </fr:tex>
        Then the statement that inhomogeneous linear system has no solutions is equivalent to 
        <fr:tex display="inline">\vec {b} \not \in  \text {range } T</fr:tex>.
        Thus the question is rephrased as not having a solution for some choice of <fr:tex display="inline">\vec {b}</fr:tex>.
        What condition ensures <fr:tex display="inline">T</fr:tex> is not surjective.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>571</fr:anchor><fr:addr type="user">thm-000P</fr:addr><fr:route>thm-000P.xml</fr:route><fr:title text="Inhomogeneous system of linear equations">Inhomogeneous system of linear equations</fr:title><fr:taxon>Theorem</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>An inhomogeneous system of linear equations
    with more equations than variables has 
    no solution for some choice of the constant term.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>Let <fr:tex display="inline">v_1, v_2, \cdots , v_n</fr:tex> be a basis of <fr:tex display="inline">V</fr:tex>.
    We know that for any value of a linear map <fr:tex display="inline">T:V\to  W</fr:tex>,
    can be determined by values <fr:tex display="inline">\{ T(v_1), T(v_2), \cdots , T(v_n) \}</fr:tex>.
    This leads to the definition of the matrix representation of a linear map.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>572</fr:anchor><fr:addr type="user">def-002R</fr:addr><fr:route>def-002R.xml</fr:route><fr:title text="Matrix">Matrix</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Let <fr:tex display="inline">m,n\in  \mathbb {Z}^+</fr:tex>.
    A <fr:tex display="inline">m\times  n</fr:tex> matrix is a rectangular array of elements of a field <fr:tex display="inline">\mathbb {F}</fr:tex>
    with <fr:tex display="inline">m</fr:tex> <fr:strong>rows</fr:strong> and <fr:tex display="inline">n</fr:tex> <fr:strong>columns</fr:strong>.
    <fr:tex display="block">         A = \begin {bmatrix}             a_{11} &amp; a_{12} &amp; \cdots  &amp; a_{1n} \\             a_{21} &amp; a_{22} &amp; \cdots  &amp; a_{2n} \\             \vdots  &amp; \vdots  &amp; \ddots  &amp; \vdots  \\             a_{m1} &amp; a_{m2} &amp; \cdots  &amp; a_{mn}         \end {bmatrix}     </fr:tex>
    The notation <fr:tex display="inline">A_{jk}</fr:tex> refers to the element in the <fr:tex display="inline">j</fr:tex>-th row and <fr:tex display="inline">k</fr:tex>-th column.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>Now we can define the matrix representation of a linear map.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>573</fr:anchor><fr:addr type="user">def-002S</fr:addr><fr:route>def-002S.xml</fr:route><fr:title text="Matrix of Linear Maps">Matrix of Linear Maps</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Let <fr:tex display="inline">T\in  \mathcal {L}(V,W)</fr:tex>,
    <fr:tex display="inline">\{ v_1,\ldots ,v_n \}\subset  V</fr:tex> be a basis of <fr:tex display="inline">V</fr:tex>,
    and <fr:tex display="inline">\{ w_1,\ldots ,w_m \}\subset  W</fr:tex> be a basis of <fr:tex display="inline">W</fr:tex>.
    The <fr:strong>matrix of <fr:tex display="inline">T</fr:tex></fr:strong> with respect to these bases is
    the <fr:tex display="inline">m\times  n</fr:tex> matrix <fr:tex display="inline">\mathcal {M}(T)</fr:tex> such that
    <fr:tex display="block">         T(v_j) = \sum _{i=1}^m \mathcal {M}(T)_{ij}w_i     </fr:tex>
    Or we denote <fr:tex display="inline">\mathcal {M}(T)</fr:tex> as <fr:tex display="inline">\mathcal {M}(T, (v_1,\ldots ,v_n), (w_1,\ldots ,w_m))</fr:tex>.</fr:p><fr:p>If <fr:tex display="inline">T</fr:tex> maps <fr:tex display="inline">n</fr:tex>-dimensional vector space to <fr:tex display="inline">m</fr:tex>-dimensional vector space,
    then <fr:tex display="inline">\mathcal {M}(T)</fr:tex> is a <fr:tex display="inline">m\times  n</fr:tex> matrix.</fr:p>
    <fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>574</fr:anchor><fr:addr type="machine">#304</fr:addr><fr:route>unstable-304.xml</fr:route><fr:title text="
    Addition
">
    <fr:strong>Addition</fr:strong>
</fr:title><fr:authors></fr:authors><fr:parent>def-002S</fr:parent></fr:frontmatter><fr:mainmatter>
    For two same-size matrix <fr:tex display="inline">A,B</fr:tex>,
    the sum of <fr:tex display="inline">A</fr:tex> and <fr:tex display="inline">B</fr:tex> is the matrix <fr:tex display="inline">C</fr:tex> such that
    <fr:tex display="block">         C_{ij} = A_{ij} + B_{ij}     </fr:tex>
    In the language of linear maps <fr:tex display="inline">S,T\in  \mathcal {L}(V,W)</fr:tex>,
    <fr:tex display="block">         \mathcal {M}(T+S) = \mathcal {M}(T) + \mathcal {M}(S)     </fr:tex>
</fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree>

    <fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>575</fr:anchor><fr:addr type="machine">#305</fr:addr><fr:route>unstable-305.xml</fr:route><fr:title text="
    Scalar Multiplication
">
    <fr:strong>Scalar Multiplication</fr:strong>
</fr:title><fr:authors></fr:authors><fr:parent>def-002S</fr:parent></fr:frontmatter><fr:mainmatter>
    For a scalar <fr:tex display="inline">c</fr:tex> and a matrix <fr:tex display="inline">A</fr:tex>,
    the product of <fr:tex display="inline">c</fr:tex> and <fr:tex display="inline">A</fr:tex> is the matrix <fr:tex display="inline">B</fr:tex> such that
    <fr:tex display="block">         B_{ij} = cA_{ij}     </fr:tex>
    In the language of linear maps <fr:tex display="inline">T\in  \mathcal {L}(V,W)</fr:tex>,
    <fr:tex display="block">         \mathcal {M}(cT) = c\mathcal {M}(T)     </fr:tex>
</fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree>

    <fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>576</fr:anchor><fr:addr type="machine">#306</fr:addr><fr:route>unstable-306.xml</fr:route><fr:title text="
    Set of Matrices
">
    <fr:strong>Set of Matrices</fr:strong>
</fr:title><fr:authors></fr:authors><fr:parent>def-002S</fr:parent></fr:frontmatter><fr:mainmatter>
    The set of all <fr:tex display="inline">m\times  n</fr:tex> matrices with elements in <fr:tex display="inline">\mathbb {F}</fr:tex> is denoted as <fr:tex display="inline">\mathcal {M}_{m\times  n}(\mathbb {F})</fr:tex>
    or <fr:tex display="inline">\mathbb {F}^{m\times  n}</fr:tex>.
</fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree>
</fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>We can see that <fr:tex display="inline">\mathbb {F}^{m\times  n}</fr:tex> is itself a vector space.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>577</fr:anchor><fr:addr type="user">thm-000Q</fr:addr><fr:route>thm-000Q.xml</fr:route><fr:title text="{F}^{m n} = mn"><fr:tex display="inline">\dim \mathbb {F}^{m\times  n} = mn</fr:tex></fr:title><fr:taxon>Theorem</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p><fr:tex display="inline">\mathbb {F}^{m\times  n}</fr:tex> is a vector space with dimension <fr:tex display="inline">mn</fr:tex>.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>Consider linear maps <fr:tex display="inline">T:U\to  V</fr:tex> and <fr:tex display="inline">S:V\to  W</fr:tex>.
    The composition of linear maps is <fr:tex display="inline">ST</fr:tex>.
    Does the composition of linear maps have a matrix representation?
    <fr:tex display="block">         \mathcal {M}(ST) = \mathcal {M}(S)\mathcal {M}(T)     </fr:tex>
    This makes no sense now but indicates the definition of <fr:strong>matrix multiplication</fr:strong>.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>578</fr:anchor><fr:addr type="user">def-002T</fr:addr><fr:route>def-002T.xml</fr:route><fr:title text="Matrix Multiplication">Matrix Multiplication</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Let <fr:tex display="inline">A</fr:tex> be a <fr:tex display="inline">m\times  n</fr:tex> matrix and <fr:tex display="inline">B</fr:tex> be a <fr:tex display="inline">n\times  p</fr:tex> matrix.
    Then <fr:tex display="inline">AC</fr:tex> is defined as the <fr:tex display="inline">m\times  p</fr:tex> matrix <fr:tex display="inline">C</fr:tex> such that
    <fr:tex display="block">         C_{ij} = \sum _{k=1}^n A_{ik}B_{kj}     </fr:tex></fr:p>
    <fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>579</fr:anchor><fr:addr type="machine">#303</fr:addr><fr:route>unstable-303.xml</fr:route><fr:title text="
    Derivation
">
    <fr:strong>Derivation</fr:strong>
</fr:title><fr:authors></fr:authors><fr:parent>def-002T</fr:parent></fr:frontmatter><fr:mainmatter>
    Let <fr:tex display="inline">T:U\to  V</fr:tex> and <fr:tex display="inline">S:V\to  W</fr:tex> be linear maps.
    Denote <fr:tex display="inline">A = \mathcal {M}(S)</fr:tex> and <fr:tex display="inline">C = \mathcal {M}(T)</fr:tex>.
    Then the composition of linear maps <fr:tex display="inline">ST</fr:tex> is computed
    <fr:tex display="block">         \begin {align*}             (ST)(u)_k &amp;= S(\sum _{r=1}^n C_{rk}v_r) \\             &amp;= \sum _{r=1}^n C_{rk}S(v_r) \\             &amp;= \sum _{r=1}^n C_{rk}\sum _{s=1}^m A _{sr}w_s \\             &amp;= \sum _{s=1}^m\left (\sum _{r=1}^n C_{rk}A_{sr}\right )w_s \\         \end {align*}     </fr:tex>
    Thus <fr:tex display="inline">\mathcal {M}(ST)</fr:tex> is the <fr:tex display="inline">m\times  p</fr:tex> whose entries are
    <fr:tex display="block">         \mathcal {M}(ST)_{sk} = \sum _{r=1}^n A_{sr}C_{rk}     </fr:tex>
</fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree>
</fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>Now we see that the desired matrix multiplication holds.
    Matrix multiplication is not commutative in general.
    However, it satisfies the associative law and the distributive law.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>580</fr:anchor><fr:addr type="user">def-002Y</fr:addr><fr:route>def-002Y.xml</fr:route><fr:title text="A_{j} and A_{ j}"><fr:tex display="inline">A_{j\cdot }</fr:tex> and <fr:tex display="inline">A_{\cdot  j}</fr:tex></fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Let <fr:tex display="inline">A</fr:tex> be a <fr:tex display="inline">m\times  n</fr:tex> matrix.
    <fr:ul><fr:li>If <fr:tex display="inline">1\leq  j\leq  m</fr:tex> then <fr:tex display="inline">A_{j\cdot }</fr:tex> is the <fr:tex display="inline">j</fr:tex>-th row of <fr:tex display="inline">A</fr:tex>,
            defined as a <fr:tex display="inline">1\times  n</fr:tex> matrix. (A row vector)</fr:li>
        <fr:li>If <fr:tex display="inline">1\leq  j\leq  n</fr:tex> then <fr:tex display="inline">A_{\cdot  j}</fr:tex> is the <fr:tex display="inline">j</fr:tex>-th column of <fr:tex display="inline">A</fr:tex>,
            defined as a <fr:tex display="inline">m\times  1</fr:tex> matrix. (A column vector)</fr:li></fr:ul></fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>With the notation we can think of matrix multiplication in another perspective.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>581</fr:anchor><fr:addr type="user">thm-000R</fr:addr><fr:route>thm-000R.xml</fr:route><fr:title text="Entry pf matrix product">Entry pf matrix product</fr:title><fr:taxon>Lemma</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Suppose <fr:tex display="inline">A</fr:tex> is an <fr:tex display="inline">m\times  n</fr:tex> matrix and <fr:tex display="inline">B</fr:tex> is an <fr:tex display="inline">n\times  p</fr:tex> matrix.
    Then the entry of the product <fr:tex display="inline">AB</fr:tex> is:
    <fr:tex display="block">         (AB)_{ij} = A_{i\cdot }B_{\cdot  j}     </fr:tex>
    for <fr:tex display="inline">1\leq  i\leq  m</fr:tex> and <fr:tex display="inline">1\leq  j\leq  p</fr:tex>.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>We have an interesting observation.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>582</fr:anchor><fr:addr type="user">thm-000S</fr:addr><fr:route>thm-000S.xml</fr:route><fr:title text="Linear Combination of columns">Linear Combination of columns</fr:title><fr:taxon>Lemma</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Let <fr:tex display="inline">A</fr:tex> be an <fr:tex display="inline">m\times  n</fr:tex> matrix,
    and <fr:tex display="inline">c</fr:tex> is a <fr:tex display="inline">1\times  1</fr:tex> matrix.
    <fr:tex display="block">         c = \begin {pmatrix} c_1 \\ c_2 \\ \vdots  \\ c_n \end {pmatrix}     </fr:tex>
    Then <fr:tex display="inline">Ac = c_1A_{\cdot  1} + c_2A_{\cdot  2} + \cdots  + c_nA_{\cdot  n}</fr:tex>.
    In other words, <fr:tex display="inline">Ac</fr:tex> is a linear Combination of the columns of <fr:tex display="inline">A</fr:tex>,
    with the scalars that multiply the columns coming from <fr:tex display="inline">c</fr:tex>.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>Now we begin the study the invertibility of linear maps.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>583</fr:anchor><fr:addr type="user">def-002Z</fr:addr><fr:route>def-002Z.xml</fr:route><fr:title text="Inverse">Inverse</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>A linear map <fr:tex display="inline">T\in \mathcal {L}(V,W)</fr:tex> is said to be <fr:tex display="inline">invertible</fr:tex> if 
    there exists a linear map <fr:tex display="inline">S\in \mathcal {L}(W,V)</fr:tex> such that:
    <fr:tex display="block">         \begin {align*}             T\cdot  S &amp;= \text {id}_V \\             S\cdot  T &amp;= \text {id}_W         \end {align*}     </fr:tex>
    where <fr:tex display="inline">\text {id}</fr:tex> is the identity map.
    If a linear map <fr:tex display="inline">T</fr:tex> is invertible, 
    then the map <fr:tex display="inline">S</fr:tex> is <fr:strong>unique</fr:strong> and is called the <fr:strong>inverse</fr:strong> of <fr:tex display="inline">T</fr:tex>, denoted <fr:tex display="inline">T^{-1}</fr:tex>.</fr:p><fr:p>An <fr:strong>isomorphism</fr:strong> is a linear map that is invertible.
    Two vector spaces are said to be <fr:strong>isomorphic</fr:strong> if there exists an isomorphism between them.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>A linear map is invertible if and only if
    it is <fr:strong>bijective</fr:strong>.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>584</fr:anchor><fr:addr type="user">thm-000T</fr:addr><fr:route>thm-000T.xml</fr:route><fr:title text="Isomorphism of equal dimensions">Isomorphism of equal dimensions</fr:title><fr:taxon>Theorem</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Two finite-dimensional vector spaces over <fr:tex display="inline">\mathbb {F}</fr:tex>
    are isomorphic iff they have the same <fr:link type="local" href="def-001V.xml" addr="def-001V" title="Dimension">dimension</fr:link>.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>585</fr:anchor><fr:addr type="user">thm-000U</fr:addr><fr:route>thm-000U.xml</fr:route><fr:title text="{L}(V,W) is isomorphic to {F}^{m n}"><fr:tex display="inline">\mathcal {L}(V,W)</fr:tex> is isomorphic to <fr:tex display="inline">\mathbb {F}^{m\times  n}</fr:tex></fr:title><fr:taxon>Theorem</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Let <fr:tex display="inline">v_1, v_2, \ldots , v_n</fr:tex> be a basis for <fr:tex display="inline">V</fr:tex>,
    and <fr:tex display="inline">w_1, w_2, \ldots , w_m</fr:tex> be a basis for <fr:tex display="inline">W</fr:tex>.
    Then <fr:tex display="inline">\mathcal {M}</fr:tex> is an isomorphism between <fr:tex display="inline">\mathcal {L}(V,W)</fr:tex> and <fr:tex display="inline">\mathbb {F}^{m\times  n}</fr:tex>.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>This has a trivial corollary.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>586</fr:anchor><fr:addr type="user">thm-000V</fr:addr><fr:route>thm-000V.xml</fr:route><fr:title text="Dimension product">Dimension product</fr:title><fr:taxon>Corollary</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Let <fr:tex display="inline">V</fr:tex> and <fr:tex display="inline">W</fr:tex> be finite-dimensional vector spaces.
    Then <fr:tex display="inline">\mathcal {L}(V,W)</fr:tex> is finite-dimensional and
    <fr:tex display="block">         \dim (\mathcal {L}(V,W)) = \dim (V)\dim (W).     </fr:tex></fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:tree toc="false" numbered="false" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:title text="Backlinks">Backlinks</fr:title><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:tree toc="true" numbered="false" show-heading="true" show-metadata="true" expanded="false" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>1357</fr:anchor><fr:addr type="user">math-0007</fr:addr><fr:route>math-0007.xml</fr:route><fr:title text="Vector Calculus and Geometry of Space">Vector Calculus and Geometry of Space</fr:title><fr:taxon>Differential Geometry</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>4</fr:month><fr:day>5</fr:day></fr:date><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Notes about multi-variable calculus, geometry of space and linear algebra.
    Refer to <fr:link type="external" href="A%20Visual%20Introduction%20to%20Differential%20Forms%20and%20Calculus%20on%20Manifolds">df-cm-2018</fr:link>.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>686</fr:anchor><fr:addr type="machine">#288</fr:addr><fr:route>unstable-288.xml</fr:route><fr:title text="Review of Vector Spaces">Review of Vector Spaces</fr:title><fr:date><fr:year>2024</fr:year><fr:month>4</fr:month><fr:day>5</fr:day></fr:date><fr:authors></fr:authors><fr:parent>math-0007</fr:parent></fr:frontmatter><fr:mainmatter><fr:p>We now start with introducing the vector space over the field of real numbers <fr:tex display="inline">\mathbb {R}</fr:tex>.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>687</fr:anchor><fr:addr type="user">def-000H</fr:addr><fr:route>def-000H.xml</fr:route><fr:title text="Vector Space">Vector Space</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>A vector space over a <fr:link type="local" href="def-0006.xml" addr="def-0006" title="Field">field</fr:link> <fr:tex display="inline">F</fr:tex> is a non-empty set <fr:tex display="inline">V</fr:tex> together with a binary operation and a binary function that satisfy the axioms listed below. 
    In this context, the elements of <fr:tex display="inline">V</fr:tex> are commonly called <fr:strong>vectors</fr:strong>, and the elements of <fr:tex display="inline">F</fr:tex> are called <fr:strong>scalars</fr:strong>.
    <fr:ul><fr:li>Commutativity: <fr:tex display="inline">             \forall  x, y \in  V, x + y = y + x         </fr:tex></fr:li>
        <fr:li>Associativity: <fr:tex display="inline">             \forall  x, y, z \in  V, (x + y) + z = x + (y + z)         </fr:tex></fr:li>
        <fr:li>Additive Identity: <fr:tex display="inline">             \exists  0 \in  V \text { such that } \forall  x \in  V, x + 0 = x         </fr:tex></fr:li>
        <fr:li>Multiplicative Identity: <fr:tex display="inline">             \forall  x \in  V, 1x = x         </fr:tex></fr:li>
        <fr:li>Additive Inverse: <fr:tex display="inline">             \forall  x \in  V, \exists  y \in  V \text { such that } x + y = 0         </fr:tex></fr:li>
        <fr:li>Distributivity: <fr:tex display="inline">             \forall  x, y \in  V, \forall  c, d \in  F, c(x + y) = cx + cy, (c + d)x = cx + dx         </fr:tex></fr:li></fr:ul></fr:p><fr:p>Elements of a vector space are called <fr:strong>vectors</fr:strong> or <fr:strong>points</fr:strong>.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>Use <fr:tex display="inline">\mathbb {R}^2</fr:tex> as an example we can see (Note that we always treat elements of vector spaces as 
        column vectors and never as row vectors):
        <fr:tex display="block">             c \cdot  \begin {bmatrix}                 a \\ b             \end {bmatrix} = \begin {bmatrix}                 c \cdot  a \\ c \cdot  b             \end {bmatrix}         </fr:tex></fr:p><fr:p>Now we will consider a certain type of transformation between vector spaces called a <fr:link type="local" href="def-0025.xml" addr="def-0025" title="Linear Map"><fr:strong>linear transformation</fr:strong></fr:link>.
        Suppose <fr:tex display="inline">T</fr:tex> is a mapping between <fr:tex display="inline">\mathbb {R}^n</fr:tex> and <fr:tex display="inline">\mathbb {R}^m</fr:tex>, that is <fr:tex display="inline">T:\mathbb {R}^n\to \mathbb {R}^m</fr:tex>, then <fr:tex display="inline">T</fr:tex> is a linear transformation if:
        <fr:tex display="block">             T(c \cdot  \vec {v}) = c \cdot  T(\vec {v})             \\              T(\vec {v} + \vec {w}) = T(\vec {v}) + T(\vec {w})         </fr:tex>
        If <fr:tex display="inline">T</fr:tex> is a linear transformation from <fr:tex display="inline">\mathbb {R}^m</fr:tex> to <fr:tex display="inline">\mathbb {R}</fr:tex> we simply call it a <fr:strong>linear function</fr:strong> or a <fr:strong>linear functional</fr:strong>.</fr:p><fr:p>We now turn our attention to the relationship between linear transformation and matrices. 
        We just stick to vector spaces <fr:tex display="inline">\mathbb {R}^n</fr:tex> and the standard basis made up of the <fr:strong>Euclidian unit vectors</fr:strong>.
        In order to write linear transformation <fr:tex display="inline">T:\mathbb {R}^n\to \mathbb {R}^m</fr:tex> as a matrix we need ordered bases for both <fr:tex display="inline">\mathbb {R}^n</fr:tex> and <fr:tex display="inline">\mathbb {R}^m</fr:tex>.
        We can use the intuitively obvious order <fr:tex display="inline">e_1 &lt; e_2 &lt; \cdots  &lt; e_n</fr:tex>.
        Now we can give formal definition of the matrix representation of a linear transformation.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>688</fr:anchor><fr:addr type="user">def-003W</fr:addr><fr:route>def-003W.xml</fr:route><fr:title text="Matrix Representation of Linear Transformation over {R}^n">Matrix Representation of Linear Transformation over <fr:tex display="inline">\mathbb {R}^n</fr:tex></fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Suppose that <fr:tex display="inline">T:\mathbb {R}^n\to \mathbb {R}^m</fr:tex> is a linear transformation between vector spaces <fr:tex display="inline">\mathbb {R}^n</fr:tex> and <fr:tex display="inline">\mathbb {R}^m</fr:tex>.
    Let <fr:tex display="inline">e_1, e_2, \ldots , e_n</fr:tex> be the standard basis of <fr:tex display="inline">\mathbb {R}^n</fr:tex> and <fr:tex display="inline">f_1, f_2, \ldots , f_m</fr:tex> be the standard basis of <fr:tex display="inline">\mathbb {R}^m</fr:tex>.
    Then the matrix representation of <fr:tex display="inline">T</fr:tex> is the <fr:tex display="inline">m \times  n</fr:tex> matrix <fr:tex display="inline">A</fr:tex> such that for <fr:tex display="inline">1\leq  j\leq  n</fr:tex>:
    <fr:tex display="block">         T(e_j) = \sum _{i=1}^m A_{ij} f_i     </fr:tex>
    where the matrix representation of <fr:tex display="inline">T</fr:tex> is given by the <fr:tex display="inline">m\times  n</fr:tex> matrix with entries <fr:tex display="inline">A_{ij}</fr:tex>.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>The last major topic in this section is the definition of the <fr:link type="local" href="def-003X.xml" addr="def-003X" title="Dual Space">dual space</fr:link>.
        In our discussion, we only concern the dual space of <fr:tex display="inline">\mathbb {R}^n</fr:tex> which is denoted as <fr:tex display="inline">(\mathbb {R}^n)^*</fr:tex>.
        Now let&apos;s consider the <fr:strong>dual basis</fr:strong> of <fr:tex display="inline">(\mathbb {R}^n)^*</fr:tex> which is denoted as <fr:tex display="inline">\{T_1, \cdots , T_n\}</fr:tex>, 
        which is defined by:
        <fr:tex display="block">             T_i(e_j) = e^i(e_j) = \langle  e^i, e_j \rangle  = \delta _{j}^i         </fr:tex>
        where <fr:tex display="inline">\delta _{ij}</fr:tex> is the <fr:link type="local" href="def-001P.xml" addr="def-001P" title="Kronecker Delta">Kronecker delta</fr:link>. We say that <fr:tex display="inline">T_i</fr:tex> is dual to the vector <fr:tex display="inline">e_i</fr:tex>.
        Note that we also denote <fr:tex display="inline">T_i</fr:tex> as <fr:tex display="inline">e^i</fr:tex> using superscript notation. And the notation <fr:tex display="inline">\langle  e^i, e_j \rangle </fr:tex> d
        indicates the products of row vector <fr:tex display="inline">e^i</fr:tex> and column vector <fr:tex display="inline">e_j</fr:tex> (Usually used in quantum computing).
        <fr:tex display="block">             \alpha (v) = \langle  \alpha , v \rangle  = [a,b] \times  \begin {bmatrix}                 x \\ y             \end {bmatrix} = ax + by         </fr:tex>
        This explains wht we always denote elements of the vector space as column vectors, because elements of the dual space 
        are written as row vectors and its very important to distinguish between them.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>689</fr:anchor><fr:addr type="machine">#289</fr:addr><fr:route>unstable-289.xml</fr:route><fr:title text="Dot Products">Dot Products</fr:title><fr:date><fr:year>2024</fr:year><fr:month>4</fr:month><fr:day>5</fr:day></fr:date><fr:authors></fr:authors><fr:parent>math-0007</fr:parent></fr:frontmatter><fr:mainmatter><fr:p>In linear algebra, <fr:strong>dot product</fr:strong> or <fr:strong>scalar product</fr:strong> is an operation that takes two vectors and returns a scalar.
        Geometrically, it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them.
        Dot product is also used to define lengths and angles.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>690</fr:anchor><fr:addr type="user">def-0041</fr:addr><fr:route>def-0041.xml</fr:route><fr:title text="Dot Product (Coordinate Form)">Dot Product (Coordinate Form)</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>The <fr:strong>dot product</fr:strong> of two vectors <fr:tex display="inline">\vec {a} = (a_1, a_2, \cdots , a_n)</fr:tex> and <fr:tex display="inline">\vec {b} = (b_1, b_2, \cdots , b_n)</fr:tex> is defined as
    <fr:tex display="block">         \vec {a}\cdot \vec {b} = a_1b_1 + a_2b_2 + \cdots  + a_nb_n = \sum _{i=1}^n a_ib_i.     </fr:tex>
    The dot product is also called the <fr:strong>inner product</fr:strong> or <fr:strong>scalar product</fr:strong>.
    The dot product satisfies the following properties:
    <fr:ul><fr:li><fr:strong>Commutative</fr:strong>: <fr:tex display="inline">\vec {a}\cdot \vec {b} = \vec {b}\cdot \vec {a}</fr:tex></fr:li>
        <fr:li><fr:strong>Distributive</fr:strong>: <fr:tex display="inline">\vec {a}\cdot (\vec {b} + \vec {c}) = \vec {a}\cdot \vec {b} + \vec {a}\cdot \vec {c}</fr:tex></fr:li>
        <fr:li><fr:strong>Bilinear</fr:strong>: <fr:tex display="inline">\vec {a}\cdot (k\vec {b}) = k(\vec {a}\cdot \vec {b}) = (\vec {a}\cdot  k\vec {b})</fr:tex></fr:li>
        <fr:li><fr:strong>Scalar Multiplication</fr:strong>: <fr:tex display="inline">(c_1\vec {a}) \cdot  (c_2\vec {b}) = c_1c_2(\vec {a}\cdot \vec {b})</fr:tex></fr:li>
        <fr:li><fr:strong>Orthogonality</fr:strong>: If <fr:tex display="inline">\vec {a}\cdot \vec {b} = 0</fr:tex>, then <fr:tex display="inline">\vec {a}</fr:tex> and <fr:tex display="inline">\vec {b}</fr:tex> are <fr:strong>orthogonal</fr:strong></fr:li>
        <fr:li><fr:strong>Product Rule</fr:strong>: If <fr:tex display="inline">\vec {a}</fr:tex> and <fr:tex display="inline">\vec {b}</fr:tex> are vector valued differentiable functions then the derivative 
            of <fr:tex display="inline">\vec {a}\cdot \vec {b}</fr:tex> is given by the rule <fr:tex display="inline">(\vec {a}\cdot \vec {b})&apos; = \vec {a}&apos;\cdot \vec {b} + \vec {a}\cdot \vec {b}&apos;</fr:tex></fr:li></fr:ul></fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>691</fr:anchor><fr:addr type="user">def-0042</fr:addr><fr:route>def-0042.xml</fr:route><fr:title text="Dot Product (Geometric Form)">Dot Product (Geometric Form)</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>In <fr:strong>Euclidean space</fr:strong>, a <fr:strong>Euclidean vector</fr:strong> is a geometric object that possesses both 
    a norm and a direction. The <fr:strong>dot product</fr:strong> of two vectors <fr:tex display="inline">\vec {a}</fr:tex> and <fr:tex display="inline">\vec {b}</fr:tex> is defined as
    <fr:tex display="block">         \vec {a}\cdot \vec {b} = \lVert \vec {a}\rVert \lVert \vec {b}\rVert \cos \theta      </fr:tex>
    where <fr:tex display="inline">\theta </fr:tex> is the angle between the two vectors.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>The geometric definition of dot product helps us express the projection of one vector onto another as well as the component of 
        one vector in the direction of another. By simple geometry we can derive the formula for the <fr:strong>projection</fr:strong>
        <fr:tex display="block">             \text {proj}_{\vec {a}}\vec {b} = \frac {\vec {a}\cdot \vec {b}}{\lVert \vec {a}\rVert }\frac {\vec {a}}{\lVert \vec {a}\rVert }         </fr:tex>
        and the <fr:strong>component</fr:strong> of <fr:tex display="inline">\vec {b}</fr:tex> in the direction <fr:tex display="inline">\vec {a}</fr:tex> is given by
        <fr:tex display="block">             \text {comp}_{\vec {a}}\vec {b} = \lVert \text {proj}_{\vec {a}}\vec {b}\rVert  = \frac {\vec {a}\cdot \vec {b}}{\lVert \vec {a}\rVert }         </fr:tex></fr:p><fr:p>Two points determine a line, and so does a point and a vector. Define the base point vector <fr:tex display="inline">\vec {b}=(x,y,z)</fr:tex> and
        the direction vector <fr:tex display="inline">\vec {v}=(a,b,c)</fr:tex> then the line is given by <fr:tex display="inline">\vec {r}(t)</fr:tex>
        <fr:tex display="block">             \vec {r}(t) = t\vec {v} + \vec {b} = (at+x, bt+y, ct+z)         </fr:tex>
        Solving for <fr:tex display="inline">t</fr:tex> in the equation we get
        <fr:tex display="block">             t = \frac {x-at}{a} = \frac {y-bt}{b} = \frac {z-ct}{c}         </fr:tex>
        which is the <fr:strong>equation of line</fr:strong>.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>692</fr:anchor><fr:addr type="machine">#290</fr:addr><fr:route>unstable-290.xml</fr:route><fr:title text="Volume and Determinants">Volume and Determinants</fr:title><fr:date><fr:year>2024</fr:year><fr:month>4</fr:month><fr:day>5</fr:day></fr:date><fr:authors></fr:authors><fr:parent>math-0007</fr:parent></fr:frontmatter><fr:mainmatter><fr:p>The <fr:strong>determinant</fr:strong> has various properties and applications in linear algebra and geometry.
        For us the most useful thing about it will be how it relates to volume:
        the determinant of a matrix gives the <fr:strong>signed volume</fr:strong> of the parallelepiped that 
        is generated by the vectors given by the matrix columns.</fr:p><fr:p>Determinants can be introduced in a variety of different ways but many of them are not at all clear.
        It usually relates to volume hence we will actually use our intuitive understanding of volumes and 
        three properties that we expected volume to have to derive the determinant (It is <fr:strong>uniquely</fr:strong> determined!).</fr:p><fr:p>So how do we expect volume to behave?
        First we expect a unit cube to have a volume of one.
        Second we expect the <fr:strong>degenerate</fr:strong> parallelepiped to have a volume of zero. Basically in <fr:tex display="inline">n</fr:tex> dimensions any 
        <fr:tex display="inline">n-1</fr:tex> dimensions object has zero <fr:tex display="inline">n</fr:tex>-D volume.
        Third we expect that volumes to be <fr:strong>linear</fr:strong>.
        Now with these three properties we move to the actual mathematics.</fr:p><fr:p>Suppose we have a parallelepiped <fr:tex display="inline">\mathscr {P}\in \mathbb {R}^n</fr:tex> whose edges are given by <fr:tex display="inline">v_1, v_2, \cdots , v_n\in \mathbb {R}^n</fr:tex>.
        We sat that the parallelepiped <fr:tex display="inline">\mathscr {P}</fr:tex> is the <fr:strong>span</fr:strong> of the vectors <fr:tex display="inline">v_1, v_2, \cdots , v_n</fr:tex> and 
        write <fr:tex display="inline">\mathscr {P}=\text {span}\{v_1, v_2, \cdots , v_n\}</fr:tex> (Note that this span is different from linear span).
        We want to find function <fr:tex display="inline">D:\mathbb {R}^{n\times  n}\to \mathbb {R}</fr:tex> which takes <fr:tex display="inline">v_1, v_2, \cdots , v_n</fr:tex> or a matrix with <fr:tex display="inline">v_1, v_2, \cdots , v_n</fr:tex> as columns
        to a real number which is the volume of <fr:tex display="inline">\mathscr {P}</fr:tex>. Now we present the three properties in mathematical form.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>693</fr:anchor><fr:addr type="machine">#291</fr:addr><fr:route>unstable-291.xml</fr:route><fr:title text="Properties of Volume">Properties of Volume</fr:title><fr:date><fr:year>2024</fr:year><fr:month>4</fr:month><fr:day>5</fr:day></fr:date><fr:authors></fr:authors><fr:parent>#290</fr:parent></fr:frontmatter><fr:mainmatter><fr:ul><fr:li><fr:tex display="inline"> D(I) = I </fr:tex> where <fr:tex display="inline">I = [e_1, e_2, \cdots , e_n]</fr:tex> is the identity matrix.</fr:li>
            <fr:li><fr:tex display="inline"> D(v_1, v_2, \cdots , v_n) = 0 </fr:tex> if <fr:tex display="inline">v_i = v_j</fr:tex> for any <fr:tex display="inline">i\neq  j</fr:tex>.</fr:li>
            <fr:li><fr:tex display="inline"> D(v_1, \cdots , v_{j-1}, v+cw, v_{j+1}, \cdots , v_n) \\                  = D(v_1, \cdots , v_{j-1}, v, v_{j+1}, \cdots , v_n) + cD(v_1, \cdots , v_{j-1}, w, v_{j+1}, \cdots , v_n) </fr:tex>
                for any <fr:tex display="inline">1 \leq  j \leq  n</fr:tex>, that is, <fr:tex display="inline">D</fr:tex> is linear.</fr:li></fr:ul></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>Now we use these properties of volume to derive several other useful properties.
        The first property is that the volumes are signed.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>694</fr:anchor><fr:addr type="machine">#292</fr:addr><fr:route>unstable-292.xml</fr:route><fr:title text="Derived Properties of Volume Function">Derived Properties of Volume Function</fr:title><fr:date><fr:year>2024</fr:year><fr:month>4</fr:month><fr:day>5</fr:day></fr:date><fr:authors></fr:authors><fr:parent>#290</fr:parent></fr:frontmatter><fr:mainmatter><fr:ul><fr:li><fr:tex display="inline">D</fr:tex> is alternating, if we switch any two vectors the sign changes.
                <fr:tex display="block">                     D(v_1, \cdots , v_i, \cdots , v_j, \cdots , v_n) = -D(v_1, \cdots , v_j, \cdots , v_i, \cdots , v_n)                 </fr:tex></fr:li> 
            <fr:li>If <fr:tex display="inline">v_1, v_2, \cdots , v_n</fr:tex> are <fr:link type="local" href="def-000Q.xml" addr="def-000Q" title="Linearly dependent">linear dependent</fr:link> then
                <fr:tex display="block">                     D(v_1, v_2, \cdots , v_n) = 0                 </fr:tex></fr:li>
            <fr:li>Adding a multiple of one vector to another does not change the determinant.</fr:li></fr:ul></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>We almost ready to derive the formula for determinant. The final ingredient we need to do is <fr:strong>permutations</fr:strong>.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>695</fr:anchor><fr:addr type="user">def-003Y</fr:addr><fr:route>def-003Y.xml</fr:route><fr:title text="Permutation">Permutation</fr:title><fr:taxon>Defintion</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>A <fr:strong>permutation</fr:strong> of a set <fr:tex display="inline">S</fr:tex> is a bijection from <fr:tex display="inline">S</fr:tex> to itself.
    The set of permutation of <fr:tex display="inline">\{1,\cdots , n\}</fr:tex> is usually denoted by <fr:tex display="inline">S_n</fr:tex>.
    We often denote a particular permutation <fr:tex display="inline">\sigma </fr:tex> by <fr:strong>Cauchy&apos;s two-line notation</fr:strong>:
    <fr:tex display="block">         \begin {bmatrix}             1 &amp; 2 &amp; \cdots  &amp; n \\             \sigma (1) &amp; \sigma (2) &amp; \cdots  &amp; \sigma (n)         \end {bmatrix}     </fr:tex>
    or <fr:strong>Cauchy&apos;s one-line notation</fr:strong>: <fr:tex display="inline">(\sigma (1),\sigma (2),\cdots ,\sigma (n))</fr:tex>.
    Another common notation is the <fr:strong>cycle notation</fr:strong>:
    <fr:tex display="block">         (i_1\ i_2\ \cdots \ i_k)     </fr:tex> which means <fr:tex display="inline">i_1 \to  i_2 \to  \cdots  \to  i_k \to  i_1</fr:tex>.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>696</fr:anchor><fr:addr type="user">def-003Z</fr:addr><fr:route>def-003Z.xml</fr:route><fr:title text="Transposition">Transposition</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>A <fr:link type="local" href="def-003Y.xml" addr="def-003Y" title="Permutation">permutation</fr:link> in which only two elements are exchanged is called a <fr:strong>transposition</fr:strong>.
    The notation is <fr:tex display="inline">\tau _{i,j}</fr:tex> where <fr:tex display="inline">i</fr:tex> and <fr:tex display="inline">j</fr:tex> are the two elements exchanged while the others remain fixed.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>Notice that the composition of two permutations is also a permutation. 
        And for any permutation <fr:tex display="inline">\sigma </fr:tex> we can perform a series of transpositions to get the identity permutation.
        It turns out that the count of the number of transpositions needed to get the identity permutation is always the same,
        which is called the <fr:strong>parity</fr:strong> of the permutation.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>697</fr:anchor><fr:addr type="user">def-0040</fr:addr><fr:route>def-0040.xml</fr:route><fr:title text="Sign of Permutation">Sign of Permutation</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>The <fr:strong>sign</fr:strong> of a <fr:link type="local" href="def-003Y.xml" addr="def-003Y" title="Permutation">permutation</fr:link> <fr:tex display="inline">\sigma \in  S_n</fr:tex> is a function <fr:tex display="inline">\text {sgn}:S_n\to \{-1,1\}</fr:tex> defined as
    <fr:tex display="inline">\text {sgn}(\sigma ) = 1</fr:tex> if <fr:tex display="inline">\sigma </fr:tex> requires an even number of permutations and 
    <fr:tex display="inline">\text {sgn}(\sigma ) = -1</fr:tex> if <fr:tex display="inline">\sigma </fr:tex> requires an odd number of permutations to get the identity permutation.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>Now we define the permutation of unit vectors <fr:tex display="inline">E_\sigma  = [e_{\sigma (1)}, e_{\sigma (2)}, \cdots , e_{\sigma (n)}]</fr:tex>.
        We got the property that
        <fr:tex display="block">             D(E_\sigma ) = \text {sgn}(\sigma )D(I) = \text {sgn}(\sigma )         </fr:tex>
        Now we have all the pieces necessary to find a formula that will give the volume of the parallelepiped spanned
        by <fr:tex display="inline">n</fr:tex> vectors.
        <fr:tex display="block">             \begin {align*}                 D\left (\begin {bmatrix}                     a_{11} &amp; a_{12} &amp; \cdots  &amp; a_{1n} \\                     a_{21} &amp; a_{22} &amp; \cdots  &amp; a_{2n} \\                     \vdots  &amp; \vdots  &amp; \ddots  &amp; \vdots  \\                     a_{n1} &amp; a_{n2} &amp; \cdots  &amp; a_{nn}                 \end {bmatrix}\right ) &amp;= \sum _{i_1=1}^n a_{i_11}D\left (\begin {bmatrix}                     | &amp; a_{12} &amp; \cdots  &amp; a_{1n} \\                     e_{i_1} &amp; a_{22} &amp; \cdots  &amp; a_{2n} \\                     | &amp; \vdots  &amp; \ddots  &amp; \vdots  \\                     | &amp; a_{n2} &amp; \cdots  &amp; a_{nn}                 \end {bmatrix}\right ) \\                  &amp;= \sum _{i_1=1}^n a_{i_11} \sum _{i_2=1}^n a_{i_22}D\left (\begin {bmatrix}                     | &amp; | &amp; \cdots  &amp; a_{1n} \\                     e_{i_1} &amp; e_{i_2} &amp; \cdots  &amp; a_{2n} \\                     | &amp; | &amp; \vdots  &amp; \vdots  \\                     | &amp; | &amp; \cdots  &amp; a_{nn}                 \end {bmatrix}\right ) \\                 &amp;= \vdots  \\                  &amp;= \sum _{i_1, i_2, \cdots , i_n = 1}^{n} a_{i_11}a_{i_22}\cdots  a_{i_nn}D\left (                     \begin {bmatrix}                         | &amp; | &amp;  &amp; | \\                         e_{i_1} &amp; e_{i_2} &amp; \cdots  &amp; e_{i_n} \\                         | &amp; | &amp;  &amp; | \\                     \end {bmatrix}                 \right ) \\                  &amp;= \sum _{\sigma \in  S_n} a_{\sigma (1)1}\cdots  a_{\sigma (n)n}                 D\left (                     \begin {bmatrix}                         | &amp; | &amp;  &amp; | \\                         e_{\sigma (1)} &amp; e_{\sigma (2)} &amp; \cdots  &amp; e_{\sigma (n)} \\                         | &amp; | &amp;  &amp; | \\                     \end {bmatrix}                 \right ) \\                  &amp;= \sum _{\sigma \in  S_n} a_{\sigma (1)1}a_{\sigma (2)2}\cdots  a_{\sigma (n)n} \text {sgn}(\sigma ) \\                  &amp;= \sum _{\sigma \in  S_n} \text {sgn}(\sigma ) \prod _{i=1}^n a_{\sigma (i)i}             \end {align*}         </fr:tex>
        In the forth step we transform the terms because the value of <fr:tex display="inline">D</fr:tex> is zero for any <fr:tex display="inline">{i_j} = {i_k}</fr:tex>,
        non-zero terms should be permutation of <fr:tex display="inline">S_n</fr:tex>.</fr:p><fr:p>It&apos;s easy to validate that the following properties of the determinant holds:
        <fr:ul><fr:li><fr:tex display="inline">D(AB) = D(A)D(B)</fr:tex></fr:li>
            <fr:li><fr:tex display="inline">D(A) = D(A^T)</fr:tex></fr:li></fr:ul>
        The second statement for transpose of <fr:tex display="inline">A</fr:tex> indicates that all the properties above also holds for row as well.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>698</fr:anchor><fr:addr type="machine">#293</fr:addr><fr:route>unstable-293.xml</fr:route><fr:title text="Derivatives of Multivariable Functions">Derivatives of Multivariable Functions</fr:title><fr:date><fr:year>2024</fr:year><fr:month>4</fr:month><fr:day>5</fr:day></fr:date><fr:authors></fr:authors><fr:parent>math-0007</fr:parent></fr:frontmatter><fr:mainmatter><fr:p>In this section we will introduce the idea of the derivative of a multivariable function. 
        Recall that a function <fr:tex display="inline">f:\mathbb {R}\to \mathbb {R}</fr:tex> the derivative of <fr:tex display="inline">f</fr:tex> at <fr:tex display="inline">x_0\in \mathbb {R}</fr:tex> is given by
        <fr:tex display="block">             f&apos;(x_0) = \lim _{h\to  0} \frac {f(x_0+h) - f(x_0)}{h}         </fr:tex>
        if the limit exists. Now let&apos;s do some transformations:
        <fr:tex display="block">             \begin {align*}                 &amp; f&apos;(x_0) = \lim _{h\to  0} \frac {f(x_0+h) - f(x_0)}{h} \\                 \implies  &amp; \lim _{h\to  0} \frac {f(x_0+h) - f(x_0) -f&apos;(x_0)h }{h} = 0 \\                 \implies  &amp; \lim _{x\to  x_0} \frac {f(x) - f(x_0) - f&apos;(x_0)(x-x_0)}{x-x_0} = 0 \\                  \implies  &amp; \lim _{x\to  x_0} \frac {|f(x) - f(x_0) - f&apos;(x_0)(x-x_0)|}{|x-x_0|} = 0             \end {align*}         </fr:tex></fr:p><fr:p>Since <fr:tex display="inline">f&apos;(x_0)</fr:tex> represents the slope of the line tangent to the graph of <fr:tex display="inline">f</fr:tex> at <fr:tex display="inline">(x_0, f(x_0))</fr:tex>,
        differentiability of <fr:tex display="inline">f</fr:tex> at <fr:tex display="inline">x_0</fr:tex> means that there exists a number <fr:tex display="inline">m</fr:tex> st
        <fr:tex display="block">             \lim _{x\to  x_0} \frac {|f(x) - f(x_0) - m(x-x_0)|}{|x-x_0|} = 0         </fr:tex>
        Now consider the function <fr:tex display="inline">T:\mathbb {R}\to \mathbb {R}</fr:tex> where <fr:tex display="inline">T(s) = ms</fr:tex>
        <fr:tex display="block">             T(s+t) = m(s+t) = ms + mt = T(s) + T(t)             \\              T(cs) = mcs = c(ms) = cT(s)         </fr:tex>
        then <fr:tex display="inline">T</fr:tex> is a linear transformation. In fact <fr:tex display="inline">T</fr:tex> is the linear function that most closely approximates the 
        function <fr:tex display="inline">f</fr:tex> at the point <fr:tex display="inline">(x_0, f(x_0))</fr:tex>. So for <fr:tex display="inline">x</fr:tex> values that are very close to <fr:tex display="inline">x_0</fr:tex> we have
        <fr:tex display="block">             f(x) \approx  m(x-x_0) + f(x_0)         </fr:tex></fr:p><fr:p>Now let&apos;s generalize the concept of derivatives to functions of the form <fr:tex display="inline">f:\mathbb {R}^n\to \mathbb {R}^m</fr:tex>.
        We assume the function <fr:tex display="inline">f</fr:tex> has the form
        <fr:tex display="block">             \begin {align*}                 &amp;f(x_1, x_2, \cdots , x_n) =                  \\                  &amp;(f_1(x_1, x_2, \cdots , x_n), f_2(x_1, x_2, \cdots , x_n), \cdots , f_m(x_1, x_2, \cdots , x_n))             \end {align*}         </fr:tex>
        We want to search for this linear transformation which we will denoted by <fr:tex display="inline">Df</fr:tex>,
        that most closely approximates this function <fr:tex display="inline">f:\mathbb {R}^n\to \mathbb {R}^m</fr:tex> at some specific point <fr:tex display="inline">x_0=(x_{1_0}, x_{2_0}, \cdots , x_{n_0}) \in  \mathbb {R}^n</fr:tex>.
        If <fr:tex display="inline">f</fr:tex> is differentiable at <fr:tex display="inline">x_0</fr:tex> then there exists a linear transformation <fr:tex display="inline">Df(x_0):\mathbb {R}^n\to \mathbb {R}^m</fr:tex> such that
        <fr:tex display="block">             \lim _{x\to  x_0} \frac {                 \lVert                      f(x) - f(x_0) - Df(x_0)(x-x_0)                 \rVert              }{\lVert x-x_0\rVert } = 0         </fr:tex>
        The <fr:tex display="inline">\lVert \cdot \rVert </fr:tex> represents the <fr:strong>Euclidean norm</fr:strong> of the vector (Multi-dimensional version of the absolute value)
        <fr:tex display="block">             \lVert \vec {x}\rVert  = \sqrt {x_1^2 + x_2^2 + \cdots  + x_n^2} = \sqrt {\sum _{i=1}^n x_i^2}         </fr:tex>
        which is just the length of the vector. This allows us to perform dividing.</fr:p><fr:p>As before we have
        <fr:tex display="block">             f(x) \approx  Df(x_0)(x-x_0) + f(x_0)         </fr:tex>
        Now we want to write <fr:tex display="inline">Df(x)</fr:tex> as a matrix. Denote the basis of <fr:tex display="inline">\mathbb {R}^n</fr:tex> as <fr:tex display="inline">e_j</fr:tex> and the basis of <fr:tex display="inline">\mathbb {R}^m</fr:tex> as <fr:tex display="inline">f_i</fr:tex>.
        Then we want to find <fr:tex display="inline">a_{ij}</fr:tex> st
        <fr:tex display="block">             Df(x)(e_j) = \sum _{i=1}^{m} a_{ij} f_j = \begin {bmatrix}                 a_{1j} \\ a_{2j} \\ \vdots  \\ a_{mj}             \end {bmatrix}         </fr:tex>
        In other words, the <fr:tex display="inline">i</fr:tex>-th component of the <fr:tex display="inline">j</fr:tex>-th column of <fr:tex display="inline">Df(x)</fr:tex> is just the <fr:tex display="inline">i</fr:tex>-th component of the <fr:tex display="inline">Df(x)(e_j)</fr:tex></fr:p><fr:p>Recall from vector calculus that given a function <fr:tex display="inline">f:\mathbb {R}^n\to \mathbb {R}</fr:tex> we defined the <fr:strong>partial derivative</fr:strong> of <fr:tex display="inline">f</fr:tex> with respect to the <fr:tex display="inline">x_j</fr:tex> as 
        <fr:tex display="block">             \frac {\partial  f}{\partial  x_j} = \lim _{h\to 0}             \frac {f(x_1,\cdots ,x_j+h,\cdots ,x_n) - f(x_1,\cdots ,x_n)}{h}         </fr:tex>
        Hence we can define the partial derivatives for each <fr:tex display="inline">f_i (1\leq  i\leq  m)</fr:tex> with respect to each <fr:tex display="inline">x_j (1\leq  j\leq  n)</fr:tex>.
        <fr:tex display="block">             \frac {\partial  f_i}{\partial  x_j} = \lim _{h\to 0}             \frac {f_i(x_1,\cdots ,x_j+h,\cdots ,x_n) - f_i(x_1,\cdots ,x_n)}{h}         </fr:tex>
        Thus we have
        <fr:tex display="block">             \frac {\partial  f_i}{\partial  x_j} = a_{ij}         </fr:tex>
        To find <fr:tex display="inline">a_{ij}</fr:tex> of <fr:tex display="inline">Df(x_0)</fr:tex> we need to find the <fr:tex display="inline">i</fr:tex>-th element of <fr:tex display="inline">Df(x_0)(e_j)</fr:tex>. Let
        <fr:tex display="block">             x = \begin {bmatrix}                 x_{1_0} \\ x_{2_0} \\ \vdots  \\ x_{n_0}             \end {bmatrix} + \begin {bmatrix}                 0 \\ \vdots  \\ 1 \\ \vdots  \\ 0             \end {bmatrix} = x_0 + he_j         </fr:tex>
        We have 
        <fr:tex display="block">             \lim _{x\to  x_0}\frac {\lVert f(x)-f(x_0)-Df(x_0)(he_j)\rVert }{\lVert he_j\rVert }             \\ \implies               \lim _{h\to 0}\frac {\lVert                  f(x_0+he_j) - f(x_0) -hDf(x_0)(e_j)             \rVert }{\lVert h\rVert } = 0         </fr:tex>
        The component is given by 
        <fr:tex display="block">             \lim _{h\to 0}\frac {\lVert f_i(x_0+he_j)-f_i(x_0)-ha_{ij}\rVert }{\lVert h\rVert } = 0             \\ \implies               a_{ij} = \lim _{h\to 0}\frac {f_i(x_0+he_j) - f_i(x_0)}{h}         </fr:tex>
        which is exactly <fr:tex display="inline">\frac {\partial  f_i}{\partial  x_j}</fr:tex>. Thus the matrix representation of <fr:tex display="inline">Df(x)</fr:tex> is given by 
        a matrix called the <fr:strong>Jacobin matrix</fr:strong> of <fr:tex display="inline">f</fr:tex>.
        <fr:tex display="block">             Df(x) = \begin {bmatrix}                 \frac {\partial  f_1}{\partial  x_1} &amp; \frac {\partial  f_1}{\partial  x_2} &amp; \cdots  &amp; \frac {\partial  f_1}{\partial  x_n} \\                 \frac {\partial  f_2}{\partial  x_1} &amp; \frac {\partial  f_2}{\partial  x_2} &amp; \cdots  &amp; \frac {\partial  f_2}{\partial  x_n} \\                 \vdots  &amp; \vdots  &amp; \ddots  &amp; \vdots  \\                 \frac {\partial  f_m}{\partial  x_1} &amp; \frac {\partial  f_m}{\partial  x_2} &amp; \cdots  &amp; \frac {\partial  f_m}{\partial  x_n}             \end {bmatrix} = \left [                 \frac {\partial  f_i}{\partial  x_j}             \right ]         </fr:tex>
        where <fr:tex display="inline">i</fr:tex> ranges row and <fr:tex display="inline">j</fr:tex> ranges column.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree></fr:backmatter></fr:tree>