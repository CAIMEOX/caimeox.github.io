<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="true" expanded="true" root="false"><fr:frontmatter><fr:anchor>1339</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-003Z</fr:addr><fr:route>def-003Z.xml</fr:route><fr:title>Transposition</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    A <fr:link href="def-003Y.xml" type="local" addr="def-003Y">permutation</fr:link> in which only two elements are exchanged is called a <fr:strong>transposition</fr:strong>.
    The notation is <fr:tex>\tau _{i,j}</fr:tex> where <fr:tex>i</fr:tex> and <fr:tex>j</fr:tex> are the two elements exchanged while the others remain fixed.
</fr:p></fr:mainmatter><fr:backmatter><fr:contributions></fr:contributions><fr:context><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="false" show-heading="true" show-metadata="true" expanded="true" root="false"><fr:frontmatter><fr:anchor>1338</fr:anchor><fr:taxon>Differential Geometry</fr:taxon><fr:addr>math-0007</fr:addr><fr:route>math-0007.xml</fr:route><fr:title>Vector Calculus and Geometry of Space</fr:title><fr:date><fr:year>2024</fr:year><fr:month>4</fr:month><fr:day>5</fr:day></fr:date></fr:frontmatter><fr:mainmatter><fr:p>
    Notes about multi-variable calculus, geometry of space and linear algebra.
    Refer to <fr:link href="A%20Visual%20Introduction%20to%20Differential%20Forms%20and%20Calculus%20on%20Manifolds" type="external">df-cm-2018</fr:link>.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>458</fr:anchor><fr:title>Review of Vector Spaces</fr:title><fr:date><fr:year>2024</fr:year><fr:month>4</fr:month><fr:day>5</fr:day></fr:date><fr:parent>math-0007</fr:parent></fr:frontmatter><fr:mainmatter><fr:p>
        We now start with introducing the vector space over the field of real numbers <fr:tex>\mathbb {R}</fr:tex>.
    </fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>456</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-000H</fr:addr><fr:route>def-000H.xml</fr:route><fr:title>Vector Space</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    A vector space over a <fr:link href="def-0006.xml" type="local" addr="def-0006">field</fr:link> <fr:tex>F</fr:tex> is a non-empty set <fr:tex>V</fr:tex> together with a binary operation and a binary function that satisfy the axioms listed below. 
    In this context, the elements of <fr:tex>F</fr:tex> are commonly called <fr:strong>vectors</fr:strong>, and the elements of <fr:tex>F</fr:tex> are called <fr:strong>scalars</fr:strong>.
    <fr:ul><fr:li>Commutativity: <fr:tex>              \forall  x, y  \in  V, x + y = y + x         </fr:tex></fr:li>
        <fr:li>Associativity: <fr:tex>              \forall  x, y, z  \in  V, (x + y) + z = x + (y + z)         </fr:tex></fr:li>
        <fr:li>Additive Identity: <fr:tex>              \exists  0  \in  V  \text { such that }  \forall  x  \in  V, x + 0 = x         </fr:tex></fr:li>
        <fr:li>Multiplicative Identity: <fr:tex>              \forall  x  \in  V, 1x = x         </fr:tex></fr:li>
        <fr:li>Additive Inverse: <fr:tex>              \forall  x  \in  V,  \exists  y  \in  V  \text { such that } x + y = 0         </fr:tex></fr:li>
        <fr:li>Distributivity: <fr:tex>              \forall  x, y  \in  V,  \forall  c, d  \in  F, c(x + y) = cx + cy, (c + d)x = cx + dx         </fr:tex></fr:li></fr:ul></fr:p><fr:p>
    Elements of a vector space are called <fr:strong>vectors</fr:strong> or <fr:strong>points</fr:strong>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
        Use <fr:tex>\mathbb {R} ^2</fr:tex> as an example we can see (Note that we always treat elements of vector spaces as 
        column vectors and never as row vectors):
        <fr:tex display="block">             c  \cdot   \begin {bmatrix}                 a  \\  b              \end {bmatrix} =  \begin {bmatrix}                 c  \cdot  a  \\  c  \cdot  b              \end {bmatrix}         </fr:tex></fr:p><fr:p>
        Now we will consider a certain type of transformation between vector spaces called a <fr:link href="def-0025.xml" type="local" addr="def-0025"><fr:strong>linear transformation</fr:strong></fr:link>.
        Suppose <fr:tex>T</fr:tex> is a mapping between <fr:tex>\mathbb {R} ^n</fr:tex> and <fr:tex>\mathbb {R} ^m</fr:tex>, that is <fr:tex>T: \mathbb {R} ^n \to \mathbb {R} ^m</fr:tex>, then <fr:tex>T</fr:tex> is a linear transformation if:
        <fr:tex display="block">             T(c  \cdot   \vec {v}) = c  \cdot  T( \vec {v})              \\               T( \vec {v} +  \vec {w}) = T( \vec {v}) + T( \vec {w})         </fr:tex>
        If <fr:tex>T</fr:tex> is a linear transformation from <fr:tex>\mathbb {R} ^m</fr:tex> to <fr:tex>\mathbb {R}</fr:tex> we simply call it a <fr:strong>linear function</fr:strong> or a <fr:strong>linear functional</fr:strong>.
    </fr:p><fr:p>
        We now turn our attention to the relationship between linear transformation and matrices. 
        We just stick to vector spaces <fr:tex>\mathbb {R} ^n</fr:tex> and the standard basis made up of the <fr:strong>Euclidian unit vectors</fr:strong>.
        In order to write linear transformation <fr:tex>T: \mathbb {R} ^n \to \mathbb {R} ^m</fr:tex> as a matrix we need ordered bases for both <fr:tex>\mathbb {R} ^n</fr:tex> and <fr:tex>\mathbb {R} ^m</fr:tex>.
        We can use the intuitively obvious order <fr:tex>e_1 &lt; e_2 &lt;  \cdots  &lt; e_n</fr:tex>.
        Now we can give formal definition of the matrix representation of a linear transformation.
    </fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>457</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-003W</fr:addr><fr:route>def-003W.xml</fr:route><fr:title>Matrix Representation of Linear Transformation over <fr:tex>\mathbb {R} ^n</fr:tex></fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Suppose that <fr:tex>T: \mathbb {R} ^n \to \mathbb {R} ^m</fr:tex> is a linear transformation between vector spaces <fr:tex>\mathbb {R} ^n</fr:tex> and <fr:tex>\mathbb {R} ^m</fr:tex>.
    Let <fr:tex>e_1, e_2,  \ldots , e_n</fr:tex> be the standard basis of <fr:tex>\mathbb {R} ^n</fr:tex> and <fr:tex>f_1, f_2,  \ldots , f_m</fr:tex> be the standard basis of <fr:tex>\mathbb {R} ^m</fr:tex>.
    Then the matrix representation of <fr:tex>T</fr:tex> is the <fr:tex>m  \times  n</fr:tex> matrix <fr:tex>A</fr:tex> such that for <fr:tex>1 \leq  j \leq  n</fr:tex>:
    <fr:tex display="block">         T(e_j) =  \sum _{i=1}^m A_{ij} f_i     </fr:tex>
    where the matrix representation of <fr:tex>T</fr:tex> is given by the <fr:tex>m \times  n</fr:tex> matrix with entries <fr:tex>A_{ij}</fr:tex>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
        The last major topic in this section is the definition of the <fr:link href="def-003X.xml" type="local" addr="def-003X">dual space</fr:link>.
        In our discussion, we only concern the dual space of <fr:tex>\mathbb {R} ^n</fr:tex> which is denoted as <fr:tex>( \mathbb {R} ^n)^*</fr:tex>.
        Now let&apos;s consider the <fr:strong>dual basis</fr:strong> of <fr:tex>( \mathbb {R} ^n)^*</fr:tex> which is denoted as <fr:tex>\{ T_1,  \cdots , T_n \}</fr:tex>, 
        which is defined by:
        <fr:tex display="block">             T_i(e_j) = e^i(e_j) =  \langle  e^i, e_j  \rangle  =  \delta _{j}^i         </fr:tex>
        where <fr:tex>\delta _{ij}</fr:tex> is the <fr:link href="def-001P.xml" type="local" addr="def-001P">Kronecker delta</fr:link>. We say that <fr:tex>T_i</fr:tex> is dual to the vector <fr:tex>e_i</fr:tex>.
        Note that we also denote <fr:tex>T_i</fr:tex> as <fr:tex>e^i</fr:tex> using superscript notation. And the notation <fr:tex>\langle  e^i, e_j  \rangle</fr:tex> d
        indicates the products of row vector <fr:tex>e^i</fr:tex> and column vector <fr:tex>e_j</fr:tex> (Usually used in quantum computing).
        <fr:tex display="block">              \alpha (v) =  \langle   \alpha , v  \rangle  = [a,b]  \times   \begin {bmatrix}                 x  \\  y              \end {bmatrix} = ax + by         </fr:tex>
        This explains wht we always denote elements of the vector space as column vectors, because elements of the dual space 
        are written as row vectors and its very important to distinguish between them.
    </fr:p></fr:mainmatter></fr:tree><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>461</fr:anchor><fr:title>Dot Products</fr:title><fr:date><fr:year>2024</fr:year><fr:month>4</fr:month><fr:day>5</fr:day></fr:date><fr:parent>math-0007</fr:parent></fr:frontmatter><fr:mainmatter><fr:p>
        In linear algebra, <fr:strong>dot product</fr:strong> or <fr:strong>scalar product</fr:strong> is an operation that takes two vectors and returns a scalar.
        Geometrically, it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them.
        Dot product is also used to define lengths and angles.
    </fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>459</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-0041</fr:addr><fr:route>def-0041.xml</fr:route><fr:title>Dot Product (Coordinate Form)</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    The <fr:strong>dot product</fr:strong> of two vectors <fr:tex>\vec {a} = (a_1, a_2,  \cdots , a_n)</fr:tex> and <fr:tex>\vec {b} = (b_1, b_2,  \cdots , b_n)</fr:tex> is defined as
    <fr:tex display="block">          \vec {a} \cdot \vec {b} = a_1b_1 + a_2b_2 +  \cdots  + a_nb_n =  \sum _{i=1}^n a_ib_i.     </fr:tex>
    The dot product is also called the <fr:strong>inner product</fr:strong> or <fr:strong>scalar product</fr:strong>.
    The dot product satisfies the following properties:
    <fr:ul><fr:li><fr:strong>Commutative</fr:strong>: <fr:tex>\vec {a} \cdot \vec {b} =  \vec {b} \cdot \vec {a}</fr:tex></fr:li>
        <fr:li><fr:strong>Distributive</fr:strong>: <fr:tex>\vec {a} \cdot ( \vec {b} +  \vec {c}) =  \vec {a} \cdot \vec {b} +  \vec {a} \cdot \vec {c}</fr:tex></fr:li>
        <fr:li><fr:strong>Bilinear</fr:strong>: <fr:tex>\vec {a} \cdot (k \vec {b}) = k( \vec {a} \cdot \vec {b}) = ( \vec {a} \cdot  k \vec {b})</fr:tex></fr:li>
        <fr:li><fr:strong>Scalar Multiplication</fr:strong>: <fr:tex>(c_1 \vec {a})  \cdot  (c_2 \vec {b}) = c_1c_2( \vec {a} \cdot \vec {b})</fr:tex></fr:li>
        <fr:li><fr:strong>Orthogonality</fr:strong>: If <fr:tex>\vec {a} \cdot \vec {b} = 0</fr:tex>, then <fr:tex>\vec {a}</fr:tex> and <fr:tex>\vec {b}</fr:tex> are <fr:strong>orthogonal</fr:strong></fr:li>
        <fr:li><fr:strong>Product Rule</fr:strong>: If <fr:tex>\vec {a}</fr:tex> and <fr:tex>\vec {b}</fr:tex> are vector valued differentiable functions then the derivative 
            of <fr:tex>\vec {a} \cdot \vec {b}</fr:tex> is given by the rule <fr:tex>( \vec {a} \cdot \vec {b})&apos; =  \vec {a}&apos; \cdot \vec {b} +  \vec {a} \cdot \vec {b}&apos;</fr:tex></fr:li></fr:ul></fr:p></fr:mainmatter></fr:tree><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>460</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-0042</fr:addr><fr:route>def-0042.xml</fr:route><fr:title>Dot Product (Geometric Form)</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    In <fr:strong>Euclidean space</fr:strong>, a <fr:strong>Euclidean vector</fr:strong> is a geometric object that possesses both 
    a norm and a direction. The <fr:strong>dot product</fr:strong> of two vectors <fr:tex>\vec {a}</fr:tex> and <fr:tex>\vec {b}</fr:tex> is defined as
    <fr:tex display="block">          \vec {a} \cdot \vec {b} =  \lVert \vec {a} \rVert \lVert \vec {b} \rVert \cos \theta      </fr:tex>
    where <fr:tex>\theta</fr:tex> is the angle between the two vectors.
</fr:p></fr:mainmatter></fr:tree><fr:p>
        The geometric definition of dot product helps us express the projection of one vector onto another as well as the component of 
        one vector in the direction of another. By simple geometry we can derive the formula for the <fr:strong>projection</fr:strong>
        <fr:tex display="block">              \text {proj}_{ \vec {a}} \vec {b} =  \frac { \vec {a} \cdot \vec {b}}{ \lVert \vec {a} \rVert } \frac { \vec {a}}{ \lVert \vec {a} \rVert }         </fr:tex>
        and the <fr:strong>component</fr:strong> of <fr:tex>\vec {b}</fr:tex> in the direction <fr:tex>\vec {a}</fr:tex> is given by
        <fr:tex display="block">              \text {comp}_{ \vec {a}} \vec {b} =  \lVert \text {proj}_{ \vec {a}} \vec {b} \rVert  =  \frac { \vec {a} \cdot \vec {b}}{ \lVert \vec {a} \rVert }         </fr:tex></fr:p><fr:p>
        Two points determine a line, and so does a point and a vector. Define the base point vector <fr:tex>\vec {b}=(x,y,z)</fr:tex> and
        the direction vector <fr:tex>\vec {v}=(a,b,c)</fr:tex> then the line is given by <fr:tex>\vec {r}(t)</fr:tex>
        <fr:tex display="block">              \vec {r}(t) = t \vec {v} +  \vec {b} = (at+x, bt+y, ct+z)         </fr:tex>
        Solving for <fr:tex>t</fr:tex> in the equation we get
        <fr:tex display="block">             t =  \frac {x-at}{a} =  \frac {y-bt}{b} =  \frac {z-ct}{c}         </fr:tex>
        which is the <fr:strong>equation of line</fr:strong>.
    </fr:p></fr:mainmatter></fr:tree><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>467</fr:anchor><fr:title>Volume and Determinants</fr:title><fr:date><fr:year>2024</fr:year><fr:month>4</fr:month><fr:day>5</fr:day></fr:date><fr:parent>math-0007</fr:parent></fr:frontmatter><fr:mainmatter><fr:p>
        The <fr:strong>determinant</fr:strong> has various properties and applications in linear algebra and geometry.
        For us the most useful thing about it will be how it relates to volume:
        the determinant of a matrix gives the <fr:strong>signed volume</fr:strong> of the parallelepiped that 
        is generated by the vectors given by the matrix columns.
    </fr:p><fr:p>
        Determinants can be introduced in a variety of different ways but many of them are not at all clear.
        It usually relates to volume hence we will actually use our intuitive understanding of volumes and 
        three properties that we expected volume to have to derive the determinant (It is <fr:strong>uniquely</fr:strong> determined!).
    </fr:p><fr:p>
        So how do we expect volume to behave?
        First we expect a unit cube to have a volume of one.
        Second we expect the <fr:strong>degenerate</fr:strong> parallelepiped to have a volume of zero. Basically in <fr:tex>n</fr:tex> dimensions any 
        <fr:tex>n-1</fr:tex> dimensions object has zero <fr:tex>n</fr:tex>-D volume.
        Third we expect that volumes to be <fr:strong>linear</fr:strong>.
        Now with these three properties we move to the actual mathematics.
    </fr:p><fr:p>
        Suppose we have a parallelepiped <fr:tex>\mathscr {P} \in \mathbb {R} ^n</fr:tex> whose edges are given by <fr:tex>v_1, v_2,  \cdots , v_n \in \mathbb {R} ^n</fr:tex>.
        We sat that the parallelepiped <fr:tex>\mathscr {P}</fr:tex> is the <fr:strong>span</fr:strong> of the vectors <fr:tex>v_1, v_2,  \cdots , v_n</fr:tex> and 
        write <fr:tex>\mathscr {P}= \text {span} \{ v_1, v_2,  \cdots , v_n \}</fr:tex> (Note that this span is different from linear span).
        We want to find function <fr:tex>D: \mathbb {R} ^{n \times  n} \to \mathbb {R}</fr:tex> which takes <fr:tex>v_1, v_2,  \cdots , v_n</fr:tex> or a matrix with <fr:tex>v_1, v_2,  \cdots , v_n</fr:tex> as columns
        to a real number which is the volume of <fr:tex>\mathscr {P}</fr:tex>. Now we present the three properties in mathematical form.
    </fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>462</fr:anchor><fr:title>Properties of Volume</fr:title><fr:date><fr:year>2024</fr:year><fr:month>4</fr:month><fr:day>5</fr:day></fr:date></fr:frontmatter><fr:mainmatter><fr:ul><fr:li><fr:tex> D(I) = I </fr:tex> where <fr:tex>I = [e_1, e_2,  \cdots , e_n]</fr:tex> is the identity matrix.
            </fr:li>
            <fr:li><fr:tex> D(v_1, v_2,  \cdots , v_n) = 0 </fr:tex> if <fr:tex>v_i = v_j</fr:tex> for any <fr:tex>i \neq  j</fr:tex>.
            </fr:li>
            <fr:li><fr:tex> D(v_1,  \cdots , v_{j-1}, v+cw, v_{j+1},  \cdots , v_n)  \\                   = D(v_1,  \cdots , v_{j-1}, v, v_{j+1},  \cdots , v_n) + cD(v_1,  \cdots , v_{j-1}, w, v_{j+1},  \cdots , v_n) </fr:tex>
                for any <fr:tex>1  \leq  j  \leq  n</fr:tex>, that is, <fr:tex>D</fr:tex> is linear.
            </fr:li></fr:ul></fr:mainmatter></fr:tree><fr:p>
        Now we use these properties of volume to derive several other useful properties.
        The first property is that the volumes are signed.
    </fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>463</fr:anchor><fr:title>Derived Properties of Volume Function</fr:title><fr:date><fr:year>2024</fr:year><fr:month>4</fr:month><fr:day>5</fr:day></fr:date></fr:frontmatter><fr:mainmatter><fr:ul><fr:li><fr:tex>D</fr:tex> is alternating, if we switch any two vectors the sign changes.
                <fr:tex display="block">                     D(v_1,  \cdots , v_i,  \cdots , v_j,  \cdots , v_n) = -D(v_1,  \cdots , v_j,  \cdots , v_i,  \cdots , v_n)                 </fr:tex></fr:li> 
            <fr:li>
                If <fr:tex>v_1, v_2,  \cdots , v_n</fr:tex> are <fr:link href="def-000Q.xml" type="local" addr="def-000Q">linear dependent</fr:link> then
                <fr:tex display="block">                     D(v_1, v_2,  \cdots , v_n) = 0                 </fr:tex></fr:li>
            <fr:li>
                Adding a multiple of one vector to another does not change the determinant.
            </fr:li></fr:ul></fr:mainmatter></fr:tree><fr:p>
        We almost ready to derive the formula for determinant. The final ingredient we need to do is <fr:strong>permutations</fr:strong>.
    </fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>464</fr:anchor><fr:taxon>Defintion</fr:taxon><fr:addr>def-003Y</fr:addr><fr:route>def-003Y.xml</fr:route><fr:title>Permutation</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    A <fr:strong>permutation</fr:strong> of a set <fr:tex>S</fr:tex> is a bijection from <fr:tex>S</fr:tex> to itself.
    The set of permutation of <fr:tex>\{ 1, \cdots , n \}</fr:tex> is usually denoted by <fr:tex>S_n</fr:tex>.
    We often denote a particular permutation <fr:tex>\sigma</fr:tex> by <fr:strong>Cauchy&apos;s two-line notation</fr:strong>:
    <fr:tex display="block">          \begin {bmatrix}             1 &amp; 2 &amp;  \cdots  &amp; n  \\               \sigma (1) &amp;  \sigma (2) &amp;  \cdots  &amp;  \sigma (n)          \end {bmatrix}     </fr:tex>
    or <fr:strong>Cauchy&apos;s one-line notation</fr:strong>: <fr:tex>( \sigma (1), \sigma (2), \cdots , \sigma (n))</fr:tex>.
    Another common notation is the <fr:strong>cycle notation</fr:strong>:
    <fr:tex display="block">         (i_1 \  i_2 \  \cdots \  i_k)     </fr:tex> which means <fr:tex>i_1  \to  i_2  \to   \cdots   \to  i_k  \to  i_1</fr:tex>.
</fr:p></fr:mainmatter></fr:tree><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>465</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-003Z</fr:addr><fr:route>def-003Z.xml</fr:route><fr:title>Transposition</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    A <fr:link href="def-003Y.xml" type="local" addr="def-003Y">permutation</fr:link> in which only two elements are exchanged is called a <fr:strong>transposition</fr:strong>.
    The notation is <fr:tex>\tau _{i,j}</fr:tex> where <fr:tex>i</fr:tex> and <fr:tex>j</fr:tex> are the two elements exchanged while the others remain fixed.
</fr:p></fr:mainmatter></fr:tree><fr:p>
        Notice that the composition of two permutations is also a permutation. 
        And for any permutation <fr:tex>\sigma</fr:tex> we can perform a series of transpositions to get the identity permutation.
        It turns out that the count of the number of transpositions needed to get the identity permutation is always the same,
        which is called the <fr:strong>parity</fr:strong> of the permutation.
    </fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>466</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-0040</fr:addr><fr:route>def-0040.xml</fr:route><fr:title>Sign of Permutation</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    The <fr:strong>sign</fr:strong> of a <fr:link href="def-003Y.xml" type="local" addr="def-003Y">permutation</fr:link> <fr:tex>\sigma \in  S_n</fr:tex> is a function <fr:tex>\text {sgn} :S_n \to \{ -1,1 \}</fr:tex> defined as
    <fr:tex>\text {sgn} ( \sigma ) = 1</fr:tex> if <fr:tex>\sigma</fr:tex> requires an even number of permutations and 
    <fr:tex>\text {sgn} ( \sigma ) = -1</fr:tex> if <fr:tex>\sigma</fr:tex> requires an odd number of permutations to get the identity permutation.
</fr:p></fr:mainmatter></fr:tree><fr:p>
        Now we define the permutation of unit vectors <fr:tex>E_ \sigma  = [e_{ \sigma (1)}, e_{ \sigma (2)},  \cdots , e_{ \sigma (n)}]</fr:tex>.
        We got the property that
        <fr:tex display="block">             D(E_ \sigma ) =  \text {sgn} ( \sigma )D(I) =  \text {sgn} ( \sigma )         </fr:tex>
        Now we have all the pieces necessary to find a formula that will give the volume of the parallelepiped spanned
        by <fr:tex>n</fr:tex> vectors.
        <fr:tex display="block">              \begin {align*}                 D \left ( \begin {bmatrix}                     a_{11} &amp; a_{12} &amp;  \cdots  &amp; a_{1n}  \\                      a_{21} &amp; a_{22} &amp;  \cdots  &amp; a_{2n}  \\                       \vdots  &amp;  \vdots  &amp;  \ddots  &amp;  \vdots   \\                      a_{n1} &amp; a_{n2} &amp;  \cdots  &amp; a_{nn}                  \end {bmatrix} \right ) &amp;=  \sum _{i_1=1}^n a_{i_11}D \left ( \begin {bmatrix}                     | &amp; a_{12} &amp;  \cdots  &amp; a_{1n}  \\                      e_{i_1} &amp; a_{22} &amp;  \cdots  &amp; a_{2n}  \\                      | &amp;  \vdots  &amp;  \ddots  &amp;  \vdots   \\                      | &amp; a_{n2} &amp;  \cdots  &amp; a_{nn}                  \end {bmatrix} \right )  \\                   &amp;=  \sum _{i_1=1}^n a_{i_11}  \sum _{i_2=1}^n a_{i_22}D \left ( \begin {bmatrix}                     | &amp; | &amp;  \cdots  &amp; a_{1n}  \\                      e_{i_1} &amp; e_{i_2} &amp;  \cdots  &amp; a_{2n}  \\                      | &amp; | &amp;  \vdots  &amp;  \vdots   \\                      | &amp; | &amp;  \cdots  &amp; a_{nn}                  \end {bmatrix} \right )  \\                  &amp;=  \vdots   \\                   &amp;=  \sum _{i_1, i_2,  \cdots , i_n = 1}^{n} a_{i_11}a_{i_22} \cdots  a_{i_nn}D \left (                      \begin {bmatrix}                         | &amp; | &amp;  &amp; |  \\                          e_{i_1} &amp; e_{i_2} &amp;  \cdots  &amp; e_{i_n}  \\                          | &amp; | &amp;  &amp; |  \\                       \end {bmatrix}                  \right )  \\                   &amp;=  \sum _{ \sigma \in  S_n} a_{ \sigma (1)1} \cdots  a_{ \sigma (n)n}                 D \left (                      \begin {bmatrix}                         | &amp; | &amp;  &amp; |  \\                          e_{ \sigma (1)} &amp; e_{ \sigma (2)} &amp;  \cdots  &amp; e_{ \sigma (n)}  \\                          | &amp; | &amp;  &amp; |  \\                       \end {bmatrix}                  \right )  \\                   &amp;=  \sum _{ \sigma \in  S_n} a_{ \sigma (1)1}a_{ \sigma (2)2} \cdots  a_{ \sigma (n)n}  \text {sgn} ( \sigma )  \\                   &amp;=  \sum _{ \sigma \in  S_n}  \text {sgn} ( \sigma )  \prod _{i=1}^n a_{ \sigma (i)i}              \end {align*}         </fr:tex>
        In the forth step we transform the terms because the value of <fr:tex>D</fr:tex> is zero for any <fr:tex>{i_j} = {i_k}</fr:tex>,
        non-zero terms should be permutation of <fr:tex>S_n</fr:tex>.
    </fr:p><fr:p>
        It&apos;s easy to validate that the following properties of the determinant holds:
        <fr:ul><fr:li><fr:tex>D(AB) = D(A)D(B)</fr:tex></fr:li>
            <fr:li><fr:tex>D(A) = D(A^T)</fr:tex></fr:li></fr:ul>
        The second statement for transpose of <fr:tex>A</fr:tex> indicates that all the properties above also holds for row as well. 
    </fr:p></fr:mainmatter></fr:tree><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>468</fr:anchor><fr:title>Derivatives of Multivariable Functions</fr:title><fr:date><fr:year>2024</fr:year><fr:month>4</fr:month><fr:day>5</fr:day></fr:date><fr:parent>math-0007</fr:parent></fr:frontmatter><fr:mainmatter><fr:p>
        In this section we will introduce the idea of the derivative of a multivariable function. 
        Recall that a function <fr:tex>f: \mathbb {R} \to \mathbb {R}</fr:tex> the derivative of <fr:tex>f</fr:tex> at <fr:tex>x_0 \in \mathbb {R}</fr:tex> is given by
        <fr:tex display="block">             f&apos;(x_0) =  \lim _{h \to  0}  \frac {f(x_0+h) - f(x_0)}{h}         </fr:tex>
        if the limit exists. Now let&apos;s do some transformations:
        <fr:tex display="block">              \begin {align*}                 &amp; f&apos;(x_0) =  \lim _{h \to  0}  \frac {f(x_0+h) - f(x_0)}{h}  \\                   \implies  &amp;  \lim _{h \to  0}  \frac {f(x_0+h) - f(x_0) -f&apos;(x_0)h }{h} = 0  \\                   \implies  &amp;  \lim _{x \to  x_0}  \frac {f(x) - f(x_0) - f&apos;(x_0)(x-x_0)}{x-x_0} = 0  \\                    \implies  &amp;  \lim _{x \to  x_0}  \frac {|f(x) - f(x_0) - f&apos;(x_0)(x-x_0)|}{|x-x_0|} = 0              \end {align*}         </fr:tex></fr:p><fr:p>
        Since <fr:tex>f&apos;(x_0)</fr:tex> represents the slope of the line tangent to the graph of <fr:tex>f</fr:tex> at <fr:tex>(x_0, f(x_0))</fr:tex>,
        differentiability of <fr:tex>f</fr:tex> at <fr:tex>x_0</fr:tex> means that there exists a number <fr:tex>m</fr:tex> st
        <fr:tex display="block">              \lim _{x \to  x_0}  \frac {|f(x) - f(x_0) - m(x-x_0)|}{|x-x_0|} = 0         </fr:tex>
        Now consider the function <fr:tex>T: \mathbb {R} \to \mathbb {R}</fr:tex> where <fr:tex>T(s) = ms</fr:tex>
        <fr:tex display="block">             T(s+t) = m(s+t) = ms + mt = T(s) + T(t)              \\               T(cs) = mcs = c(ms) = cT(s)         </fr:tex>
        then <fr:tex>T</fr:tex> is a linear transformation. In fact <fr:tex>T</fr:tex> is the linear function that most closely approximates the 
        function <fr:tex>f</fr:tex> at the point <fr:tex>(x_0, f(x_0))</fr:tex>. So for <fr:tex>x</fr:tex> values that are very close to <fr:tex>x_0</fr:tex> we have
        <fr:tex display="block">             f(x)  \approx  m(x-x_0) + f(x_0)         </fr:tex></fr:p><fr:p>
        Now let&apos;s generalize the concept of derivatives to functions of the form <fr:tex>f: \mathbb {R} ^n \to \mathbb {R} ^m</fr:tex>.
        We assume the function <fr:tex>f</fr:tex> has the form
        <fr:tex display="block">              \begin {align*}                 &amp;f(x_1, x_2,  \cdots , x_n) =                   \\                   &amp;(f_1(x_1, x_2,  \cdots , x_n), f_2(x_1, x_2,  \cdots , x_n),  \cdots , f_m(x_1, x_2,  \cdots , x_n))              \end {align*}         </fr:tex>
        We want to search for this linear transformation which we will denoted by <fr:tex>Df</fr:tex>,
        that most closely approximates this function <fr:tex>f: \mathbb {R} ^n \to \mathbb {R} ^m</fr:tex> at some specific point <fr:tex>x_0=(x_{1_0}, x_{2_0},  \cdots , x_{n_0})  \in   \mathbb {R} ^n</fr:tex>.
        If <fr:tex>f</fr:tex> is differentiable at <fr:tex>x_0</fr:tex> then there exists a linear transformation <fr:tex>Df(x_0): \mathbb {R} ^n \to \mathbb {R} ^m</fr:tex> such that
        <fr:tex display="block">              \lim _{x \to  x_0}  \frac {                  \lVert                      f(x) - f(x_0) - Df(x_0)(x-x_0)                  \rVert              }{ \lVert x-x_0 \rVert } = 0         </fr:tex>
        The <fr:tex>\lVert \cdot \rVert</fr:tex> represents the <fr:strong>Euclidean norm</fr:strong> of the vector (Multi-dimensional version of the absolute value)
        <fr:tex display="block">              \lVert \vec {x} \rVert  =  \sqrt {x_1^2 + x_2^2 +  \cdots  + x_n^2} =  \sqrt { \sum _{i=1}^n x_i^2}         </fr:tex>
        which is just the length of the vector. This allows us to perform dividing.
    </fr:p><fr:p>
        As before we have
        <fr:tex display="block">             f(x)  \approx  Df(x_0)(x-x_0) + f(x_0)         </fr:tex>
        Now we want to write <fr:tex>Df(x)</fr:tex> as a matrix. Denote the basis of <fr:tex>\mathbb {R} ^n</fr:tex> as <fr:tex>e_j</fr:tex> and the basis of <fr:tex>\mathbb {R} ^m</fr:tex> as <fr:tex>f_i</fr:tex>.
        Then we want to find <fr:tex>a_{ij}</fr:tex> st
        <fr:tex display="block">             Df(x)(e_j) =  \sum _{i=1}^{m} a_{ij} f_j =  \begin {bmatrix}                 a_{1j}  \\  a_{2j}  \\   \vdots   \\  a_{mj}              \end {bmatrix}         </fr:tex>
        In other words, the <fr:tex>i</fr:tex>-th component of the <fr:tex>j</fr:tex>-th column of <fr:tex>Df(x)</fr:tex> is just the <fr:tex>i</fr:tex>-th component of the <fr:tex>Df(x)(e_j)</fr:tex></fr:p><fr:p>
        Recall from vector calculus that given a function <fr:tex>f: \mathbb {R} ^n \to \mathbb {R}</fr:tex> we defined the <fr:strong>partial derivative</fr:strong> of <fr:tex>f</fr:tex> with respect to the <fr:tex>x_j</fr:tex> as 
        <fr:tex display="block">              \frac { \partial  f}{ \partial  x_j} =  \lim _{h \to0 }              \frac {f(x_1, \cdots ,x_j+h, \cdots ,x_n) - f(x_1, \cdots ,x_n)}{h}         </fr:tex>
        Hence we can define the partial derivatives for each <fr:tex>f_i (1 \leq  i \leq  m)</fr:tex> with respect to each <fr:tex>x_j (1 \leq  j \leq  n)</fr:tex>.
        <fr:tex display="block">              \frac { \partial  f_i}{ \partial  x_j} =  \lim _{h \to0 }              \frac {f_i(x_1, \cdots ,x_j+h, \cdots ,x_n) - f_i(x_1, \cdots ,x_n)}{h}         </fr:tex>
        Thus we have
        <fr:tex display="block">              \frac { \partial  f_i}{ \partial  x_j} = a_{ij}         </fr:tex>
        To find <fr:tex>a_{ij}</fr:tex> of <fr:tex>Df(x_0)</fr:tex> we need to find the <fr:tex>i</fr:tex>-th element of <fr:tex>Df(x_0)(e_j)</fr:tex>. Let
        <fr:tex display="block">             x =  \begin {bmatrix}                 x_{1_0}  \\  x_{2_0}  \\   \vdots   \\  x_{n_0}              \end {bmatrix} +  \begin {bmatrix}                 0  \\   \vdots   \\  1  \\   \vdots   \\  0              \end {bmatrix} = x_0 + he_j         </fr:tex>
        We have 
        <fr:tex display="block">              \lim _{x \to  x_0} \frac { \lVert f(x)-f(x_0)-Df(x_0)(he_j) \rVert }{ \lVert he_j \rVert }              \\   \implies                \lim _{h \to0 } \frac { \lVert                  f(x_0+he_j) - f(x_0) -hDf(x_0)(e_j)              \rVert }{ \lVert h \rVert } = 0         </fr:tex>
        The component is given by 
        <fr:tex display="block">              \lim _{h \to0 } \frac { \lVert f_i(x_0+he_j)-f_i(x_0)-ha_{ij} \rVert }{ \lVert h \rVert } = 0              \\   \implies               a_{ij} =  \lim _{h \to0 } \frac {f_i(x_0+he_j) - f_i(x_0)}{h}         </fr:tex>
        which is exactly <fr:tex>\frac { \partial  f_i}{ \partial  x_j}</fr:tex>. Thus the matrix representation of <fr:tex>Df(x)</fr:tex> is given by 
        a matrix called the <fr:strong>Jacobin matrix</fr:strong> of <fr:tex>f</fr:tex>.
        <fr:tex display="block">             Df(x) =  \begin {bmatrix}                  \frac { \partial  f_1}{ \partial  x_1} &amp;  \frac { \partial  f_1}{ \partial  x_2} &amp;  \cdots  &amp;  \frac { \partial  f_1}{ \partial  x_n}  \\                   \frac { \partial  f_2}{ \partial  x_1} &amp;  \frac { \partial  f_2}{ \partial  x_2} &amp;  \cdots  &amp;  \frac { \partial  f_2}{ \partial  x_n}  \\                   \vdots  &amp;  \vdots  &amp;  \ddots  &amp;  \vdots   \\                   \frac { \partial  f_m}{ \partial  x_1} &amp;  \frac { \partial  f_m}{ \partial  x_2} &amp;  \cdots  &amp;  \frac { \partial  f_m}{ \partial  x_n}              \end {bmatrix} =  \left [                  \frac { \partial  f_i}{ \partial  x_j}              \right ]         </fr:tex>
        where <fr:tex>i</fr:tex> ranges row and <fr:tex>j</fr:tex> ranges column.
    </fr:p></fr:mainmatter></fr:tree></fr:mainmatter></fr:tree></fr:context><fr:related><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="false" show-heading="true" show-metadata="true" expanded="true" root="false"><fr:frontmatter><fr:anchor>1337</fr:anchor><fr:taxon>Defintion</fr:taxon><fr:addr>def-003Y</fr:addr><fr:route>def-003Y.xml</fr:route><fr:title>Permutation</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    A <fr:strong>permutation</fr:strong> of a set <fr:tex>S</fr:tex> is a bijection from <fr:tex>S</fr:tex> to itself.
    The set of permutation of <fr:tex>\{ 1, \cdots , n \}</fr:tex> is usually denoted by <fr:tex>S_n</fr:tex>.
    We often denote a particular permutation <fr:tex>\sigma</fr:tex> by <fr:strong>Cauchy&apos;s two-line notation</fr:strong>:
    <fr:tex display="block">          \begin {bmatrix}             1 &amp; 2 &amp;  \cdots  &amp; n  \\               \sigma (1) &amp;  \sigma (2) &amp;  \cdots  &amp;  \sigma (n)          \end {bmatrix}     </fr:tex>
    or <fr:strong>Cauchy&apos;s one-line notation</fr:strong>: <fr:tex>( \sigma (1), \sigma (2), \cdots , \sigma (n))</fr:tex>.
    Another common notation is the <fr:strong>cycle notation</fr:strong>:
    <fr:tex display="block">         (i_1 \  i_2 \  \cdots \  i_k)     </fr:tex> which means <fr:tex>i_1  \to  i_2  \to   \cdots   \to  i_k  \to  i_1</fr:tex>.
</fr:p></fr:mainmatter></fr:tree></fr:related><fr:backlinks></fr:backlinks><fr:references></fr:references></fr:backmatter></fr:tree>