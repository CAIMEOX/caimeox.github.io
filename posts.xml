<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="forest.xsl"?>
<tree expanded="true" show-heading="true" show-metadata="true" toc="false" numbered="false" root="false"><frontmatter><anchor>1368</anchor>   <addr>posts</addr>  <route>posts.xml</route>  <authors> <contributor>CAIMEO</contributor></authors> <title>Blog posts</title> </frontmatter> <mainmatter><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1369</anchor>  <taxon>Type Theory</taxon> <addr>tt-0002</addr>  <route>tt-0002.xml</route> <date><year>2024</year> <month>1</month> <day>27</day></date>  <title>Introduction to Type Theory</title> </frontmatter> <mainmatter><p>This is a note on dependent type theory.</p><p><strong>Homotopy type theory</strong> is a foundational language for mathematics, an alternative to Zermelo-Fraenkel set theory.
    The set-theoretic foundation has two two layers:
    <ul><li>the deductive system of first-order logic</li>
        <li>the theory of a particular theory, such as ZFC</li></ul> 
    Type theory itself is a deductive system, which has one basic notation: <em>types</em>.
    Propositions are identified with types.
    
    Thus, the activity of proving a theorem is identified with 
    constructing a <em>inhabitant</em> of a certain type.
</p><p>
    Informally, a deductive system is a collection of rules for deriving <strong>judgments</strong>. 
    The judgment is considered to be the external of the theory, living in the <strong>metatheory</strong>.
</p><p>
    In first order logic, there is only one kind of judgment: a proposition has a proof.
    A proposition <tex>P</tex> gives rise to a judgment &quot;<tex>P</tex> has a proof&quot;.
    The proposition <tex>P</tex> lives inside the theory, while the judgment &quot;<tex>P</tex> has a proof&quot; lives in the metatheory. 
</p><p>
    In type theory, analogous to first order logic,
    &quot;<tex>P</tex> has a proof&quot; is written as &quot;<tex>p:P</tex>&quot; (Type <tex>P</tex> has a term <tex>p</tex>).
    <ul><li>If <tex>P</tex> is a proposition, then <tex>p</tex> is a <strong>witness</strong> to the provability of <tex>P</tex>, 
        or <strong>evidence</strong> of the truth of <tex>P</tex>.</li>
        <li><tex>p:P</tex> can also be interpreted as <tex>p \in  P</tex>,
        but <tex>p:P</tex> is a judgment while <tex>p \in  P</tex> is a proposition.</li></ul>
    Working inside type theory we can't write down statements like
    &quot;if <tex>p:P</tex> then ...&quot; nor can we disprove the judgment &quot;<tex>p:P</tex>&quot;.
</p><p>
    A key difference between type theory and set theory is the equality.
    The notion of equality in set theory is simply a proposition.
    Howerver, in type theory, there are two kinds of equality.
    <ul><li>The first kind is the <strong>propositional equality</strong> <tex>a=_Ab</tex>.
        This is a proposition</li>
        <li>The second kind is the <strong>judgmental equality</strong> <tex>a \equiv  b:A</tex>.
        This is a judgment</li></ul>
    Two terms <tex>a:A</tex> and <tex>b:A</tex> are propositionally equal if you can prove <tex>a =_A b</tex> , 
    or equivalently if you can construct a term <tex>h : a =_A b</tex>.
</p>
    <p>
        In type theory there is also a requirement for a judgment-level equality.
        This is called <strong>judgmental equality</strong>, meaning &quot;equal by definition&quot;.
    </p>
    <tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1370</anchor>  <taxon>Definition</taxon> <addr>def-0015</addr>  <route>def-0015.xml</route>   <title>Judgemental Equality</title> </frontmatter> <mainmatter><p><strong>Judgemental equality</strong> of terms is given by the following judgement:
    <tex display="block">
         \Gamma \vdash  a \equiv  a':A
    </tex>
    <tex>a</tex> and <tex>a'</tex> are judgementally equal terms of type <tex>A</tex> in context <tex>\Gamma</tex>.
</p><p>
    Note that the notation <tex>\equiv</tex> binds more loosely than anything else.
</p></mainmatter> </tree>
    <p>
        judgments may depend on <em>assumptions</em> of the form <tex>x:A</tex> where <tex>x</tex> is a
        variable and <tex>A</tex> is a type. And the collection (actually ordered list) of such assumptions is called 
        the <strong>context</strong>, denoted <tex>\Gamma</tex>. (from a topological point of view it 
        may be thought of as a <strong>parameters space</strong>).
        The role of a context is to declare what <strong>hypothetical terms</strong> are assumed, 
        along with their types.
        The notation <tex>\vdash</tex> means making conclusion from assumptions.
    </p>
<p>
    Remember the difference between axiom and (inference) rules.
    <ul><li>Rules allow us to conclude one judgment from a collection of other judgments.</li>
        <li>Axioms are judgments that are assumed to be true without proof.</li></ul></p><p>
    We start by introduction to Matrin Lof's dependent type theory. 
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1371</anchor>  <taxon>Definition</taxon> <addr>def-0017</addr>  <route>def-0017.xml</route>   <title>Dependent type theory: Judgments</title> </frontmatter> <mainmatter>
    <p>
        There are four kinds of judgments in Martin Lof's dependent type theory:
    </p>
    <ul><li><tex>A</tex> is a well-formed type in context <tex>\Gamma</tex>
            <tex display="block">
                 \Gamma   \vdash  A  \space \text {type} 
            </tex></li>
        <li><tex>A</tex> and <tex>B</tex> are judgmentally equal types in context <tex>\Gamma</tex>
            <tex display="block">
                 \Gamma   \vdash  A  \equiv  B  \space \text {type} 
            </tex></li>
        <li><tex>a</tex> is a term of type <tex>A</tex> in context <tex>\Gamma</tex>
            <tex display="block">
                 \Gamma   \vdash  a : A
            </tex></li>
        <li><tex>a</tex> and <tex>b</tex> are judgmentally equal terms of type <tex>A</tex> in context <tex>\Gamma</tex>
            <tex display="block">
                 \Gamma   \vdash  a  \equiv  b : A
            </tex></li></ul>
</mainmatter> </tree><p>
    All judgments are context dependent, 
    and indeed that even the types of the variables in a context
    may depend on any previous declared variables.

    To introduce types dependent on terms, 
    we need the notation of type families.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1372</anchor>  <taxon>Definition</taxon> <addr>def-0018</addr>  <route>def-0018.xml</route>   <title>Type Family</title> </frontmatter> <mainmatter><p>
    Consider a type <tex>A</tex> in context <tex>\Gamma</tex>.
    A <strong>family</strong> of types over <tex>A</tex> in context <tex>\Gamma</tex>
    is a type <tex>B(x)</tex> in context <tex>\Gamma , x:A</tex>.
    <tex display="block">
         \Gamma , x:A  \vdash  B(x)  \space \text {type} 
    </tex>
    <tex>B</tex> is a family of types over <tex>A</tex> in context <tex>\Gamma</tex>.
    Alternatively, we say that <tex>B(x)</tex> is a type <strong>indexed</strong> by <tex>x:A</tex> in context <tex>\Gamma</tex>.
</p></mainmatter> </tree><p>
    Now we can define a term of a type family, that is, a section of a type family.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1373</anchor>  <taxon>Definition</taxon> <addr>def-0019</addr>  <route>def-0019.xml</route>   <title>Section of Type Family</title> </frontmatter> <mainmatter><p>
    Let <tex>B</tex> be a <link href="def-0018.xml" type="local" addr="def-0018" title="Type Family">type family</link> over <tex>A</tex> in context <tex>\Gamma</tex>.
    A <strong>section</strong> of <tex>B</tex> is a term <tex>b</tex> of type <tex>B(x)</tex> in context <tex>\Gamma ,x:A</tex>.
    <tex display="block">
         \Gamma , x:A  \vdash  b : B(x)
    </tex>
    Alternatively, we say that <tex>b</tex> is a term of <tex>B(x)</tex> indexed by <tex>x:A</tex> in context <tex>\Gamma</tex>.
</p></mainmatter> </tree><p>
    We now ready to present the inference rules for dependent type theory.
    These rules are known as the <strong>structual rules</strong> of the theory.
    There are 6 sets of rules:
    <ul><li>Formation contexts, types and terms</li>
        <li>Postulating that judgmental equality is an equivalence relation</li>
        <li>Vairable conversion</li>
        <li>Substitution</li>
        <li>Weakening</li>
        <li>Generic element</li></ul></p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1374</anchor>  <taxon>Definition</taxon> <addr>def-001A</addr>  <route>def-001A.xml</route>   <title>
    Formation of contexts, types and terms
</title> </frontmatter> <mainmatter><p>
    The following rules follow from the presuppotion
    about contexts, types and terms, can be used freely.
</p><ul><li><tex display="block">
             \frac {
                 \Gamma ,x:A \vdash  B(x) \space \text {type} 
            }{
                 \Gamma \vdash  A \space \text {type} 
            }
        </tex></li>
    <li><tex display="block">
             \frac {
                 \Gamma \vdash  A \equiv  B \space \text {type} 
            }{
                 \Gamma \vdash  A \space \text {type} 
            }
             \quad 
             \frac {
                 \Gamma \vdash  A \equiv  B \space \text {type} 
            }{
                 \Gamma \vdash  B \space \text {type} 
            }
        </tex></li>
    <li><tex display="block">
             \frac {
                 \Gamma \vdash  a:A
            }{
                 \Gamma \vdash  A \space \text {type} 
            }
        </tex></li>
    <li><tex display="block">
             \frac {
                 \Gamma \vdash  a \equiv  b:A
            }{
                 \Gamma \vdash  a:A
            }
             \quad  
             \frac {
                 \Gamma \vdash  a \equiv  b:A
            }{
                 \Gamma \vdash  b:A
            }
        </tex></li></ul></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1375</anchor>  <taxon>Definition</taxon> <addr>def-001B</addr>  <route>def-001B.xml</route>   <title>
    Judgmental equality is equivalence relation
</title> </frontmatter> <mainmatter><p>
    Judgmental equality on types and on elements is an <link href="def-000X.xml" type="local" addr="def-000X" title="Equivalence Relation">equivalence relation</link> 
    simply postulate that these relations are reflexive, symmetric, and transitive:
</p><ul><li><tex display="block">
             \frac {
                 \Gamma \vdash  a:A
            }{
                 \Gamma \vdash  a \equiv  a:A
            }
             \quad 
             \frac {
                 \Gamma \vdash  a \equiv  b:A
            }{
                 \Gamma \vdash  b \equiv  a:A
            }
             \quad 
             \frac {
                 \Gamma \vdash  a \equiv  b:A
                 \quad 
                 \Gamma \vdash  b \equiv  c:A
            }{
                 \Gamma \vdash  a \equiv  c:A
            }
        </tex></li>
    <li><tex display="block">
             \frac {
                 \Gamma \vdash  A \space \text {type} 
            }{
                 \Gamma \vdash  A \equiv  A \space \text {type} 
            }
             \quad 
             \frac {
                 \Gamma \vdash  A \equiv  B \space \text {type} 
            }{
                 \Gamma \vdash  B \equiv  A \space \text {type} 
            }
             \quad 
             \frac {
                 \Gamma \vdash  A \equiv  B \space \text {type} 
                 \quad 
                 \Gamma \vdash  B \equiv  C \space \text {type} 
            }{
                 \Gamma \vdash  A \equiv  C \space \text {type} 
            }
        </tex></li></ul></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1376</anchor>  <taxon>Definition</taxon> <addr>def-001C</addr>  <route>def-001C.xml</route>   <title>Variable Conversion</title> </frontmatter> <mainmatter><p>
    This rule postulates that we can
    convert the type of a variable to a judgmentally equal type.
    <tex display="block">
         \frac {
             \Gamma \vdash  A \equiv  A' \space \text {type} 
             \quad 
             \Gamma ,x:A, \Delta \vdash  B(x) \space \text {type} 
        }{
             \Gamma ,x:A', \Delta \vdash  B(x) \space \text {type} 
        }
    </tex>
    Similarly, we can convert judgmental equality of types and terms.
    We state all of them at once using a <em>generic judgment thesis</em> <tex>\mathcal {J}</tex>.
    <tex display="block">
         \frac {
             \Gamma \vdash  A \equiv  A' \space \text {type} 
             \quad 
             \Gamma ,x:A, \Delta \vdash   \mathcal {J}
        }{
             \Gamma ,x:A', \Delta \vdash   \mathcal {J}
        }VC
    </tex></p></mainmatter> </tree><p>
    Consider a term <tex>f:B(x)</tex> indexed by <tex>x:A</tex> in context <tex>\Gamma</tex>,
    and we also have a term <tex>a:A</tex>.
    We can simultaneously substitute <tex>a</tex> for all occurrences of <tex>x</tex> in <tex>f</tex>
    to obtain a new term <tex>f[x:=a]:B(a)</tex>.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1377</anchor>  <taxon>Definition</taxon> <addr>def-001D</addr>  <route>def-001D.xml</route>   <title>Substitution</title> </frontmatter> <mainmatter><p>
    The substitution rule postulates that we can substitute a term for a variable.
    <tex display="block">
         \frac {
             \Gamma \vdash  a:A
             \quad 
             \Gamma ,x:A, \Delta \vdash   \mathcal {J}
        }{
             \Gamma , \Delta [x:=a] \vdash   \mathcal {J}[x:=a]
        }S
    </tex>
    The notation <tex>\Gamma , \Delta [x:=a]</tex> means that we substitute <tex>a</tex> for <tex>x</tex> in <tex>\Delta</tex>.
</p><p>
    With the substitution rule, we need two more <em>congruence rules</em> to
    convert judgmental equality of terms and types.
    <tex display="block">
         \frac {
             \Gamma \vdash  a \equiv  a':A
             \quad 
             \Gamma ,x:A, \Delta \vdash  B  \space \text {type} 
        }{
             \Gamma , \Delta [x:=a] \vdash  B[x:=a] \equiv  B[x:=a']  \space \text {type} 
        }
    </tex>

    <tex display="block">
         \frac {
             \Gamma \vdash  A \equiv  A' \space \text {type} 
             \quad 
             \Gamma ,x:A, \Delta \vdash  b:A
        }{
             \Gamma , \Delta [x:=a] \vdash  b[x:=a] \equiv  b[x:=a']:A'[x:=a]  \space \text {type} 
        }
    </tex></p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1378</anchor>  <taxon>Definition</taxon> <addr>def-001G</addr>  <route>def-001G.xml</route>   <title>Fiber and Value</title> </frontmatter> <mainmatter><p>
    Let <tex>B</tex> be a <link href="def-0018.xml" type="local" addr="def-0018" title="Type Family">type family</link> over <tex>A</tex> in context <tex>\Gamma</tex>,
    an a well-formed term <tex>a:A</tex>,
    then we say that <tex>B[x:=a]</tex> is the <strong>fiber</strong> of <tex>B</tex> at <tex>a</tex>, denoted <tex>B(a)</tex>.
</p><p>
    Let <tex>b</tex> a <link href="def-0019.xml" type="local" addr="def-0019" title="Section of Type Family">section</link> of <tex>B</tex> over <tex>A</tex> in context <tex>\Gamma</tex>,
    then we say that <tex>b(a): \equiv  b[x:=a]</tex> is the <strong>value</strong> of <tex>b</tex> at <tex>a</tex>.

</p></mainmatter> </tree><p>
    The process of expanding the context by a fresh variable of type <tex>A</tex> is called weakening (by <tex>A</tex>).
    Intuitively, weakening is the process of adding a new hypothesis to the context.
    And the hypothesis will weaken the conclusion.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1379</anchor>  <taxon>Definition</taxon> <addr>def-001E</addr>  <route>def-001E.xml</route>   <title>Weakening</title> </frontmatter> <mainmatter><p>
    Weakening rule asserts that we can add a variable to the context.
    <tex display="block">
         \frac {
             \Gamma \vdash  A \space \text {type} 
             \quad  
             \Gamma , \Delta \vdash   \mathcal {J}
        }{
             \Gamma ,x:A, \Delta \vdash   \mathcal {J}
        }W 
    </tex></p></mainmatter> </tree><p>
    Finally, the generic elemets rule ensures that
    the variables declared in a context.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1380</anchor>  <taxon>Definition</taxon> <addr>def-001F</addr>  <route>def-001F.xml</route>   <title>Generic Elements</title> </frontmatter> <mainmatter><p>
    The rule for the generic element asserts that 
    any hypothetical element <tex>x:A</tex> in context <tex>\Gamma ,x:A</tex>
    is also an element of <tex>A</tex> in context <tex>\Gamma ,x:A</tex>.
    <tex display="block">
         \frac {
             \Gamma \vdash  A \space \text {type} 
        }{
             \Gamma ,x:A \vdash  x:A
        } \delta 
    </tex>
    This rule is also called the <strong>variable rule</strong>.   
</p></mainmatter> </tree><p>
    The next topic is the dependent function type, a fundamental concept of dependent type theory.
    Simply put, a dependent function type is a function whose type of output may depend on the input.
</p><p>
    Consider a section <tex>b</tex> of a family <tex>B</tex> over <tex>A</tex> in context <tex>\Gamma</tex>:
    <tex display="block">
         \Gamma ,x:A \vdash  b(x):B(x)
    </tex>
    Such a section <tex>b</tex> is an operation or assignment <tex>x \mapsto  b(x)</tex> that assigns to each element <tex>x:A</tex>
    to a term <tex>b(x):B(x)</tex>.
    We may see <tex>b</tex> as a function takes <tex>x:A</tex> to <tex>b(x):B(x)</tex>.
    The function <tex>x \mapsto  b(x)</tex> is called a <strong>dependent function</strong>.
    The type of all dependent functions from <tex>A</tex> to <tex>B</tex> is called the <strong>dependent function type</strong>.
    <tex display="block">
         \Pi _{(x:A)}B(x)  \text { or } (x:A) \to  B(x)
    </tex></p><p>
    To introduce a type we need the following four rules:
    <ul><li>Formation rule</li>
        <li>Introduction rule</li>
        <li>Elimination rule</li>
        <li>Computation rule</li></ul>
    Besides these we also need the <strong>congruence rule</strong> for judgmental equality.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1381</anchor>  <taxon>Definition</taxon> <addr>def-001T</addr>  <route>def-001T.xml</route>   <title>Dependent Function Type</title> </frontmatter> <mainmatter><p><strong>Formation Rule</strong>
    For any type family <tex>B</tex> over <tex>A</tex> in context <tex>\Gamma</tex>:
    <tex display="block">
         \frac {
             \Gamma ,x:A \vdash  B(x) \space \text {type} 
        }{
             \Gamma \vdash   \Pi _{(x:A)}B(x) \space \text {type} 
        } \Pi 
    </tex>
    We also require that the operation of forming dependent function types
    respects judgmental equality.
    <tex display="block">
         \frac {
             \Gamma \vdash  A \equiv  A' \space \text {type} 
             \quad 
             \Gamma ,x:A \vdash  B(x) \equiv  B'(x) \space \text {type} 
        }{
             \Gamma \vdash   \Pi _{(x:A)}B(x) \equiv   \Pi _{(x:A')}B'(x) \space \text {type} 
        } \Pi \text {-eq}
    </tex></p><p><strong>Introduction Rule (<tex>\lambda</tex>-abstraction)</strong>
    In order to construct a dependent function we have to
    construct a term <tex>f(x):B(x)</tex> indexed by <tex>x:A</tex> in context <tex>\Gamma</tex>:
    <tex display="block">
         \frac {
             \Gamma ,x:A \vdash  b(x):B(x)
        }{
             \Gamma \vdash   \lambda  x.b(x): \Pi _{(x:A)}B(x)
        } \lambda 
    </tex>
    And the congruence rule:
    <tex display="block">
         \frac {
             \Gamma ,x:A \vdash  b(x) \equiv  b'(x):B(x)
        }{
             \Gamma \vdash   \lambda  x.b(x) \equiv   \lambda  x.b'(x): \Pi _{(x:A)}B(x)
        } \lambda \text {-eq}
    </tex></p><p><strong>Elimination Rule (Evaluation Rule)</strong>
    In order to use dependent function we need to provide an argument of the domain type.
    <tex display="block">
         \frac {
             \Gamma \vdash  f: \Pi _{(x:A)}B(x)
        }{
             \Gamma ,x:A \vdash  f(x):B(x)
        }ev
    </tex>
    Again we require the judgmental equality to be respected:
    <tex display="block">
         \frac {
             \Gamma \vdash  f \equiv  f': \Pi _{(x:A)}B(x)
        }{
             \Gamma ,x:A \vdash  f(x) \equiv  f'(x):B(x)
        }ev \text {-eq}
    </tex></p><p><strong>Computation Rule (<tex>\beta</tex>-reduction)</strong>
    <tex display="block">
         \frac {
             \Gamma ,x:A \vdash  b(x):B(x)
        }{
             \Gamma ,x:A \vdash  ( \lambda  y.b(y))(x) \equiv  b(x):B(x)
        } \beta 
    </tex>
    We postulate that all elements of a dependent function type are dependent functions.
    <tex display="block">
         \frac {
             \Gamma \vdash  f: \Pi _{(x:A)}B(x)
        }{
             \Gamma \vdash  f \equiv   \lambda  x.f(x): \Pi _{(x:A)}B(x)
        } \eta 
    </tex></p></mainmatter> </tree><p>
    A degenrated case of dependent function type is the ordinary function type.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1382</anchor>  <taxon>Definition</taxon> <addr>def-001U</addr>  <route>def-001U.xml</route>   <title>Ordinary Function Type</title> </frontmatter> <mainmatter><p>
    A special case of <link href="def-001T.xml" type="local" addr="def-001T" title="Dependent Function Type"><tex>\Pi</tex>-type</link> is the <strong>ordinary function type</strong>.
    Using weakening rule we can obtain thee type <tex>A \to  B</tex> of ordinary function from <tex>A</tex> to <tex>B</tex>
    <tex display="block">
         \frac {
             \Gamma \vdash  A \space \text {type} 
             \quad 
             \Gamma \vdash  B \space \text {type} 
        }{ \dfrac {
             \Gamma ,x:A \vdash  B \space \text {type} 
        }{
             \Gamma \vdash   \Pi _{(x:A)}B \space \text {type} 
        } \Pi }W
    </tex>
    A term <tex>f:  \Pi _{(x:A)}B</tex> is an ordinary function. The type <tex>A  \to  B</tex> is defined:
    <tex display="block">
        A \to  B :=  \Pi _{(x:A)}B
    </tex>
    The type <tex>A</tex> is called <strong>domain</strong> of <tex>f</tex>,
    and type <tex>B</tex> is called <strong>codomain</strong> of <tex>f</tex>.
    The notation <tex>:=</tex> here means to make a definition.
</p></mainmatter> </tree></mainmatter> </tree><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1383</anchor>  <taxon>Type Theory</taxon> <addr>tt-0003</addr>  <route>tt-0003.xml</route> <date><year>2024</year> <month>1</month> <day>30</day></date>  <title>Natural Numbers <tex>\mathbb {N}</tex></title> </frontmatter> <mainmatter><p>
    In classical mathematics, the <strong>Peano axioms</strong> are a set of axioms for the natural numbers,
    an important object in mathematics.
</p><p>
    In type theory, the type <tex>\mathbb {N}</tex> of natural number is an <link href="def-001X.xml" type="local" addr="def-001X" title="Inductive Type"><strong>inductive type</strong></link>.
    Just like in dependent function type, we need four inference rules to define the natural numbers.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1384</anchor>  <taxon>Definition</taxon> <addr>def-001Y</addr>  <route>def-001Y.xml</route>   <title>Natural Number</title> </frontmatter> <mainmatter><p>
    In type theory, <strong>natural number</strong> is defined using <strong>peano encoding</strong>.
    The type <tex>\mathbb {N}</tex> is formed by the formation rule:
    <tex display="block">
         \frac {}{ \vdash \mathbb {N} \space \text {type} } \mathbb {N} \text {-form}
    </tex></p><p>
    Peano's first axiom postulates the existence of a natural number <tex>0</tex>.
    The introduction rule for <tex>\mathbb {N}</tex> has a <tex>0</tex> constructor and a <strong>successor</strong> function.
    <tex display="block">
         \frac {}{ \vdash0 : \mathbb {N} } \mathbb {N} \text {-intro-0}
         \quad 
         \frac {}{ \vdash \text {succ} : \mathbb {N} \to \mathbb {N} }  \mathbb {N} \text {-intro-succ}
    </tex></p><p>
    The <strong>elimination rule</strong> is actually the type theoretical <strong>induction principle</strong> of <tex>\mathbb {N}</tex>:
    In order to show that <tex>\forall  n: \mathbb {N} .P(n)</tex> holds, it suffices to show that <tex>P(0)</tex> holds and that <tex>\forall  n: \mathbb {N} .P(n) \to  P( \text {succ} (n))</tex> holds.
    The type theoretical induction principle is therefore formulated using a type family <tex>P</tex> over <tex>\mathbb {N}</tex>:
    <tex display="block">
         \frac {
             \begin {align*}
                 \Gamma &amp;, \, n: \mathbb {N} \vdash  P(n) \space \text {type} 
                 \\ 
                 \Gamma &amp; \vdash  p_0:P(0)
                 \\ 
                 \Gamma &amp; \vdash  p_S: \Pi _{(n: \mathbb {N} )}P(n) \to  P( \text {succ} (n))
             \end {align*}
        }{
             \Gamma \vdash \text {ind}_ \mathbb {N} (p_0,p_S): \Pi _{(n: \mathbb {N} )}P(n)
        }( \mathbb {N}   \text {-ind})
    </tex>
    The induction principle tells us what we need to do in order to construct a dependent function <tex>\text {ind}_ \mathbb {N}</tex> of type <tex>\Pi _{(n: \mathbb {N} )}P(n)</tex>.
    We might alternatively write the induction principle as:
    <tex display="block">
         \frac {
             \Gamma ,n: \mathbb {N} \vdash  P(n) \space \text {type} 
        }{
             \Gamma \vdash \text {ind}_ \mathbb {N} : \left (P(0) \to \left ( \Pi _{(n: \mathbb {N} )}P(n) \to  P( \text {succ} (n)) \right ) \to \Pi _{(n: \mathbb {N} )}P(n) \right )
        }( \mathbb {N}   \text {-ind})
    </tex></p><p>
    The <strong>computation rule</strong> asserts that the dependent function <tex>\text {ind}_ \mathbb {N}</tex> behaves as expected:
    <tex display="block">
         \frac {
             \begin {align*}
                 \Gamma &amp;,n: \mathbb {N} \vdash  P(n) \space \text {type} 
                 \\ 
                 \Gamma &amp; \vdash  p_0:P(0)
                 \\ 
                 \Gamma &amp; \vdash  p_S: \Pi _{(n: \mathbb {N} )}P(n) \to  P( \text {succ} (n))
             \end {align*}
        }{
             \text {ind}_ \mathbb {N} (p_0,p_S,m) \equiv 
             \begin {cases}
                p_0&amp; \text {if }m=0
                 \\ 
                p_S(n, \text {ind}_ \mathbb {N} (p_0,p_S,n))&amp; \text {if }m= \text {succ} (n)
             \end {cases}
        }( \mathbb {N}   \text {-comp})
    </tex></p></mainmatter> </tree><p>
    Now let's use the type theoretical induction principle of <tex>\mathbb {N}</tex> to
    perform some basic construction over <tex>\mathbb {N}</tex>.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1385</anchor>  <taxon>Definition</taxon> <addr>def-001Z</addr>  <route>def-001Z.xml</route>   <title>Addition over <tex>\N</tex></title> </frontmatter> <mainmatter><p>
    We define addition over <tex>\mathbb {N}</tex> using the type theoretical induction principle of <tex>\mathbb {N}</tex>.
    <tex display="block">
         \text {add}_ \mathbb {N}  :  \mathbb {N}   \to  ( \mathbb {N}   \to   \mathbb {N} )
    </tex>
    which satisfies the following specification:
    <tex display="block">
         \begin {align*}
             \text {add}_ \mathbb {N} (n, 0)&amp;: \equiv  n
             \\ 
             \text {add}_ \mathbb {N} (m, \text {succ} (n))&amp;: \equiv \text {succ} ( \text {add}_ \mathbb {N} (m,n))
         \end {align*}
    </tex>
    abbreviated as <tex>m + n</tex> for <tex>\text {add}_ \mathbb {N} (m,n)</tex>.
</p><block open="open"><headline><strong>Construction</strong></headline> 
    
        <p>
            We construct the additon by perform induction over the second variable <tex>n</tex>.
            That is, to construct an element
            <tex display="block">
                m: \mathbb {N}   \vdash   \text {add}_ \mathbb {N} (m): \mathbb {N}   \to   \mathbb {N} 
            </tex>
            The context <tex>\Gamma   \equiv  m: \mathbb {N}</tex> is fixed.
        </p>
        Therefore we need to construct:
        <tex display="block">
             \begin {align*}
                 \Gamma &amp; \vdash   \text {add-zero}_ \mathbb {N} (m): \mathbb {N} 
                 \\ 
                 \Gamma &amp; \vdash   \text {add-succ}_ \mathbb {N} (m): \mathbb {N} \to \mathbb {N}                 
             \end {align*}   
        </tex>
        The <tex>\text {add-zero}_ \mathbb {N}</tex> is defined to be identity function trivially. To see how <tex>\text {add-succ}_ \mathbb {N}</tex> is defined, we need to perform induction:
        <tex display="block">
             \begin {align*}
                 \text {add}_ \mathbb {N} (m,  \text {succ} (n))&amp; \equiv 
                 \text {ind}_ \mathbb {N} ( \text {add-zero}_ \mathbb {N} (m),  \text {add-succ}_ \mathbb {N} (m),  \text {succ} (n))
                 \\ 
                &amp; \equiv   \text {add-succ}_ \mathbb {N} (m,n,  \text {add}_ \mathbb {N} (m,n))
                 \\ 
                &amp; \equiv   \text {succ} ( \text {add}_ \mathbb {N} (m,n))
             \end {align*}
        </tex>
        Hence <tex>\text {add-succ}_ \mathbb {N}</tex> is defined as:
        <tex display="block">
             \text {add-succ}_ \mathbb {N} (m,n,x)  \equiv   \text {succ} (x)
        </tex>
        A formal construction of <tex>\text {add-succ}_ \mathbb {N}</tex> is as follows:
        <tex display="block">
             \dfrac {
                 \dfrac {
                     \dfrac {
                         \dfrac {}{ \vdash \mathbb {N} \space \text {type} }
                         \quad 
                         \dfrac {
                             \dfrac {}{ \vdash   \mathbb {N} \space \text {type} }
                             \quad 
                             \dfrac {}{ \vdash   \text {succ} : \mathbb {N} \to \mathbb {N} }
                        }{
                            n: \mathbb {N} \vdash \text {succ} : \mathbb {N} \to \mathbb {N} 
                        }
                    }{
                        m: \mathbb {N} , n: \mathbb {N} \vdash   \text {succ} : \mathbb {N} \to \mathbb {N} 
                    } 
                }{
                    m: \mathbb {N} \vdash \lambda  n. \text {succ}  :  \mathbb {N} \to ( \mathbb {N} \to \mathbb {N} )
                }
            }{
                m: \mathbb {N} \vdash   \text {add-succ}_ \mathbb {N} (m): \equiv \lambda  m. \text {succ}  :  \mathbb {N} \to ( \mathbb {N} \to \mathbb {N} )
            } \text {Block-1}
        </tex>
        Finally we combine the derivation together:
        <tex display="block">
             \dfrac {
                 \dfrac {
                     \dfrac {
                         \dfrac {
                             \vdash   \mathbb {N} \space \text {type} 
                        }{
                            m: \mathbb {N} \vdash  m: \mathbb {N} 
                        }
                    }{
                        m: \mathbb {N} \vdash \text {add-zero}_ \mathbb {N} (m): \equiv  m: \mathbb {N} 
                    }
                     \quad  
                     \dfrac { \text {Block-1}}{
                        m: \mathbb {N} \vdash \text {add-succ}_ \mathbb {N} (m): \mathbb {N} \to ( \mathbb {N} \to \mathbb {N} )
                    }
                }{
                    m: \mathbb {N} \vdash \text {ind}_ \mathbb {N}  ( \text {add-zero}_ \mathbb {N} (m),  \text {add-succ}_ \mathbb {N} (m)): \mathbb {N} \to \mathbb {N} 
                }
            }{
                m: \mathbb {N} \vdash \text {add}_ \mathbb {N} (m): \equiv \text {ind}_ \mathbb {N}  ( \text {add-zero}_ \mathbb {N} (m),  \text {add-succ}_ \mathbb {N} (m)): \mathbb {N} \to \mathbb {N} 
            }
        </tex>
    
</block></mainmatter> </tree><p>
    Recall the definition of addition function <tex>\text {add}:  \mathbb {N} \to ( \mathbb {N} \to \mathbb {N} )</tex> satisfying the specification:
    <tex display="block">
         \begin {align*}
            m + 0 &amp; : \equiv  m
             \\  
            m +  \text {succ} (n) &amp; : \equiv   \text {succ} (m + n)
         \end {align*}
    </tex>
    Such definition is enough to characterize the addition function.
    Because it postulates te <em>behavior</em> of <tex>\text {add}_ \mathbb {N}</tex> at the constructor of <tex>\mathbb {N}</tex></p><p>
    More generally, we can define a dependent function <tex>f: \Pi  n: \mathbb {N} .P(n)</tex> by induction on <tex>n</tex> using
    <tex display="block">
         \begin {align*}
            p_0 &amp; : P(0)
             \\ 
            p_S &amp; :  \Pi _{(n: \mathbb {N} )}P(n) \to  P( \text {succ} (n))
         \end {align*}
    </tex>
    Just present the definition by writing
    <tex display="block">
         \begin {align*}
            f(0) &amp; : \equiv  p_0
             \\ 
            f( \text {succ} (n)) &amp; : \equiv  p_S(n,f(n))
         \end {align*}
    </tex>
    <tex>f</tex> is said to be defined by <strong>pattern matching</strong> on the variable <tex>n</tex>.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1386</anchor>  <taxon>Example</taxon> <addr>eg-0001</addr>  <route>eg-0001.xml</route>   <title>Fibonacci Function</title> </frontmatter> <mainmatter><p>
    The <strong>Fibonacci function</strong> is a well-known function in mathematics.
    It is defined by pattern matching on the variable <tex>n</tex> as follows:
    <tex display="block">
         \begin {align*}
             \text {fib}(0) &amp; : \equiv  0
             \\ 
             \text {fib}(1) &amp; : \equiv  1
             \\ 
             \text {fib}(n+2) &amp; : \equiv   \text {fib}(n) +  \text {fib}(n+1)
         \end {align*}
    </tex></p></mainmatter> </tree></mainmatter> </tree><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1387</anchor>  <taxon>Type Theory</taxon> <addr>tt-0004</addr>  <route>tt-0004.xml</route> <date><year>2024</year> <month>1</month> <day>30</day></date>  <title>Inductive types</title> </frontmatter> <mainmatter><p>
    In previous post, we have seen the definition of natural numbers as an inductive type.
    In this post, we will see more examples of inductive types, such as 
    unit type, empty type, product type, sum type and etc.
</p><p>
    This section is much more informal than the previous one.
    Without displaying the inference rules, we will just present the <strong>constructor</strong>
    and <strong>induction principle</strong> of each inductive type.
</p><block open="open"><headline><strong>General Inductive Type</strong></headline> 
    <p>
    Just like <tex>\mathbb {N}</tex>, other inductive types can be defined by 
    their constructors and induction principles (and computation rules).
    </p>
    <ul><li>
            The constructors specify the structure of the type equipped.
        </li>
        <li>
            The induction principle specifies the data required to construct 
            a section of an arbitrary type family over the inductive type.
        </li>
        <li>
            The computation rules specify the behavior of the constructors.
        </li></ul>
</block><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1388</anchor>  <taxon>Definition</taxon> <addr>def-0020</addr>  <route>def-0020.xml</route>   <title>Unit Type</title> </frontmatter> <mainmatter><p>
        
        The <strong>unit type</strong> is the simplest inductive type.
        It has only one constructor, denoted by <tex>\star : \textbf {1}</tex>.
    </p><p>
        The induction principle of the unit type is trivial.
        It says that to define a dependent function <tex>f: \Pi  _{(x: \textbf {1} )} P(x)</tex>, 
        it suffices to give a value <tex>p:P( \star )</tex>.
        <tex display="block">
            f ( \star ) : \equiv  p
        </tex></p></mainmatter> </tree><p>
    A degenrate inductive type is the <strong>empty type</strong>.
    It has no constructor, and its induction principle is trivial.
    Empty type is connected to the <strong>exfalso quodlibet principle</strong> in logic.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1389</anchor>  <taxon>Definition</taxon> <addr>def-0021</addr>  <route>def-0021.xml</route>   <title>Empty Type</title> </frontmatter> <mainmatter><p>
    The <strong>empty type</strong> is a degenerate inductive type <tex>\emptyset</tex> satisfying 
    the following induction principle:
    <tex display="block">
         \text {ind}_ \emptyset  :  \Pi _{(x: \emptyset )}P(x)
    </tex>
    And a special case is <strong>exfalso</strong>:
    <tex display="block">
         \text {exfalso} : \equiv   \text {ind}_ \emptyset  :  \empty   \to  A 
    </tex>
    which can draw any conclusion.
</p></mainmatter> </tree><p>
    With the empty type we can define the <strong>negation</strong> of a type
    and the <strong>proof of negation</strong>.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1390</anchor>  <taxon>Definition</taxon> <addr>def-0022</addr>  <route>def-0022.xml</route>   <title>Type Negation</title> </frontmatter> <mainmatter><p>
    The <strong>negation</strong> of type <tex>A</tex> is defined as
    <tex display="block">
         \neg  A : \equiv  A  \to   \emptyset 
    </tex>
    A type <tex>A</tex> is said to be <strong>empty</strong> if and only if <tex>\neg  A</tex> is inhabited.
    <tex display="block">
         \text {empty}(A) : \equiv   \neg  A
    </tex>
    <block open="open"><headline><strong>Proof of negation</strong></headline> 
        <p>
            To prove <tex>\neg  A</tex>, we need to show that <tex>A</tex> implies a contradiction.
            In other words, constructing a function of type <tex>A  \to   \emptyset</tex>.
        </p>
    </block></p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1391</anchor>  <taxon>Definition</taxon> <addr>def-0023</addr>  <route>def-0023.xml</route>   <title>Coproduct Type</title> </frontmatter> <mainmatter><p>
    Let <tex>A</tex> and <tex>B</tex> be types. The <strong>coproduct (disjoint sum)</strong> <tex>A+B</tex> is a typed defined by the following constructors:
    <ul><li><tex>\text {inl} :A \to  A+B</tex></li>
        <li><tex>\text {inr} :B \to  A+B</tex></li></ul>
    For any type family indexed by <tex>x:A+B</tex>, satisfies the following induction principle:
    <tex display="block">
         \text {ind}_ + : ( \Pi _{(x:A)}P( \text {inl} (x))) \to ( \Pi _{(y:B)}P( \text {inr} (y))) \to \Pi _{(z:A+B)}P(z)
    </tex>
    (Note that sometimes we denoted <tex>\text {ind}_ + (f,g)</tex> as <tex>[f,g]</tex>) And the computation rule:
    <tex display="block">
         \begin {align*}
             \text {ind}_ + (f,g, \text {inl} (x))&amp; \equiv  f(x) \\ 
             \text {ind}_ + (f,g, \text {inr} (y))&amp; \equiv  g(y)
         \end {align*}
    </tex>
    where <tex>f</tex> and <tex>g</tex> are defined:
    <tex display="block">
         \begin {align*}
            f&amp;: \Pi _{(x:A)}P( \text {inl} (x)) \\ 
            g&amp;: \Pi _{(y:B)}P( \text {inr} (y))
         \end {align*}
    </tex>
    This can be presented by pattern matching to define a function <tex>h: \Pi  _{(z:A+B)}.P(z)</tex>:
    <tex display="block">
         \begin {align*}
            h( \text {inl} (x))&amp;: \equiv  f(x) \\ 
            h( \text {inr} (y))&amp;: \equiv  g(y)
         \end {align*}
    </tex></p></mainmatter> </tree><block open="open"><headline><strong>Special Case of Coproduct</strong></headline> 
    The special case of coproduct type is also called the <strong>sum type</strong>.
    <tex display="block">
         \text {ind}_ + : (A  \to  X)  \to  (B  \to  X)  \to  (A+B  \to  X)
    </tex>
    which is very similar to the <strong>elimination rule</strong> of disjunction in logic.
    <tex display="block">
        (P \to  Q)  \to  (R \to  Q)  \to  (P \vee  R \to  Q)
    </tex>
</block><p>
    The dependent version of sum type is called the <strong>dependent sum type (dependent coproduct)</strong> traditionally.
    Its terms are ordered pairs.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1392</anchor>  <taxon>Definition</taxon> <addr>def-0024</addr>  <route>def-0024.xml</route>   <title>Dependent Pair Type</title> </frontmatter> <mainmatter><p>
    The <strong>dependent pair type</strong> is a inductive type <tex>\Sigma _{(x:A)}B(x)</tex> (<tex>(x:A)  \times  B(x)</tex>)
</p><block open="open"><headline>
    <strong>Formation Rule</strong>
</headline> 
    <p>
        Given a type family <tex>B</tex> over <tex>A</tex>, we can form the dependent pair type <tex>\Sigma _{(x:A)}B(x)</tex>.
    </p>
    <tex display="block">
         \frac {
             \Gamma \vdash  A \space \text {type} 
             \quad 
             \Gamma ,x:A \vdash  B(x) \space \text {type} 
        }{
             \Gamma \vdash  (x:A)  \times  B(x) \space \text {type} 
        }
    </tex>
</block><block open="open"><headline>
    <strong>Introduction Rule</strong>
</headline> 
    <p>
        Given a term <tex>a:A</tex> and a term <tex>b(a):B(a)</tex>, we can form a term <tex>\text {pair} (a,b):(x:A) \times  B(x)</tex>.
    </p>
    <tex display="block">
         \frac {
             \Gamma \vdash  a:A
             \quad 
             \Gamma \vdash  b(a):B(a)
        }{
             \Gamma \vdash  (a,b):(x:A) \times  B(x)
        }
    </tex>
</block><block open="open"><headline>
    <strong>Elimination Rule</strong>
</headline> 
    <p>
        The elimination rule is formed with two projections.
        <tex display="block">
             \frac {
                 \Gamma \vdash  p:(x:A) \times  B(x)
            }{
                 \begin {align*}
                     \Gamma &amp; \vdash   \text {pr}_ A (p):A \\ 
                     \Gamma &amp; \vdash   \text {pr}_ B (p):B( \text {pr}_ A (p))
                 \end {align*}
            }
        </tex></p>
</block><block open="open"><headline>
    <strong>Computation Rule</strong>
</headline> 
    <p><tex display="block">
             \frac {
                 \Gamma \vdash  x:A 
                 \quad  
                 \Gamma \vdash  y:B(x)
            }{
                 \begin {align*}
                     \Gamma &amp; \vdash   \text {pr}_ A ((x,y))=x:A \\ 
                     \Gamma &amp; \vdash   \text {pr}_ B ((x,y))=y:B(x)
                 \end {align*}
            }
        </tex></p>
</block><block open="open"><headline>
    <strong>Special Case</strong>
</headline> 
    <ul><li>
            In the special case that <tex>B(x) = B</tex> is independent of <tex>A</tex>,
            this reduces to the <strong>product type</strong> <tex>A \times  B</tex>.
        </li>
        <li>
            In the special case that <tex>D  \equiv   \text {Boolean}</tex>,
            this reduces to a <link href="def-0023.xml" type="local" addr="def-0023" title="Coproduct Type">coproduct type</link>.
        </li></ul>
</block></mainmatter> </tree></mainmatter> </tree><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1393</anchor>  <taxon>Type Theory</taxon> <addr>tt-0005</addr>  <route>tt-0005.xml</route> <date><year>2024</year> <month>1</month> <day>31</day></date>  <title>Identity Types</title> </frontmatter> <mainmatter><p>
    How can we think of <strong>equality</strong> in type theory?
    Mentioned before, given a type <tex>A</tex> and two its elements <tex>a,b:A</tex>
    we can define a new type <tex>a=_Ab</tex> which is called the <strong>identity type</strong>.
    In this case, a term of <tex>a=_Ab</tex> is said to be a <em>witness</em> of the equality of <tex>a</tex> and <tex>b</tex>.
    And this witness is itself a type.
    We can then define their equality.
    This gives every type a <strong>groupoid structure</strong>.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1394</anchor>  <taxon>Definition</taxon> <addr>def-002N</addr>  <route>def-002N.xml</route>   <title>Identity Type</title> </frontmatter> <mainmatter><p>
    The <strong>identity type</strong> is an inductive type,
    generated by just a <strong>reflexivity</strong> <em>identification</em>
    that providing an equality of a term with itself.
</p><p>
    Consider a type <tex>A</tex> and let <tex>a:A</tex> be an element.
    The identity type of <tex>A</tex> at <tex>a</tex> is an inductive family of types <tex>a=_Ax</tex> 
    indexed by <tex>x:A</tex>.
    <tex display="block">
         \frac {
             \Gamma \vdash  a:A
        }{
             \Gamma ,x:A \vdash  a=_Ax \space \text {type} 
        }
    </tex>
    The only constructor is the refl:
    <tex display="block">
         \frac {
             \Gamma \vdash  a:A
        }{
             \Gamma \vdash   \text {refl} _a:a=_Ax
        }
    </tex></p><block open="open"><headline><strong>
    Path Induction / Identification Elimination
</strong></headline> 
    The induction principle of the identity type states that
    for any type family <tex>P(x,p)</tex> indexed by <tex>x:A</tex> and <tex>p:a=_Ax</tex>,
    <tex display="block">
         \text {ind-eq}_{ a } :P(a,  \text {refl} _a) \to  (x:A) \to  (p:a=_Ax) \to  P(x,p)
    </tex>
    satisfies <tex>\text {ind-eq}_{ a } (u,a, \text {refl} _a) \equiv  u</tex> where <tex>u:P(a, \text {refl} _a)</tex>.
    Formally we can write:
    <tex display="block">
         \frac {
             \Gamma \vdash  a:A, \quad   \Gamma ,x:A,p:a=_Ax \vdash  P(x,p) \space \text {type} 
        }{
             \Gamma \vdash   \text {ind-eq}_{ a } (a, \text {refl} _a): P(a, \text {refl} _a)  \to  (x:A) \to  (p:a=_Ax) \to  P(x,p)
        } \text {eq-elim}
    </tex>
    <tex display="block">
         \frac {
             \Gamma \vdash  a:A, \quad   \Gamma ,x:A,p:a=_Ax \vdash  P(x,p) \space \text {type} 
        }{
             \Gamma ,u:P(a, \text {refl} _a) \vdash   \text {ind-eq}_{ a } (u,a, \text {refl} _a) \equiv  u:P(a, \text {refl} _a)
        } \text {eq-comp}
    </tex>

</block><p>
    A term of <tex>a=_Ax</tex> is called a <strong>identification</strong> of <tex>a</tex> and <tex>x</tex>,
    or the <strong>path</strong> from <tex>a</tex> to <tex>x</tex>.
</p><block open="open"><headline><strong>Variable Version</strong></headline> 
    We can form an identity type with variables of <tex>A</tex>.
    <tex display="block">
         \Gamma ,x:A,y:A \vdash  x=_Ay \space \text {type} 
    </tex>
    with the following introduction rule:
    <tex display="block">
         \frac {
             \Gamma ,x:A \vdash  x:A
        }{
             \Gamma ,x:A \vdash   \text {refl} _x:x=_Ax
        }
    </tex>
    and similarly the elimination rule and computation rule.
</block></mainmatter> </tree><p>
    The identifications can be <strong>concatenated</strong> and <strong>inverted</strong>,
    which implies the <em>transitivity</em> and <em>symmetry</em> of the identity type.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1395</anchor>  <taxon>Definition</taxon> <addr>def-002O</addr>  <route>def-002O.xml</route>   <title>Concatenation Operation</title> </frontmatter> <mainmatter><p>
    Let <tex>A</tex> be a type.
    The <strong>Concatenation</strong> operation is defined:
    <tex display="block">
         \text {concat} : (x:A) \to  (y:A) \to  (z:A) \to  (x=_Ay) \to  (y=_Az) \to  (x=_Az)
    </tex></p><block open="open"><headline><strong>Construction</strong></headline> 
    We can first construct:
    <tex display="block">
        f(x):(y:A) \to  (x=_Ay) \to (z:A) \to  (y=_Az) \to (x=_Az)
    </tex>
    For any <tex>x:A</tex>, it suffices to construct
    <tex display="block">
        f(x,x, \text {refl} _x) : (z:A)  \to  (x=_Az) \to (x=_Az)
    </tex>
    That is the identity function <tex>\lambda  z.  \text {id} _{x=_Az}</tex>.
    Then we can define by induction:
    <tex display="block">
        f(x) : \equiv   \text {ind-eq}_{ x } ( \lambda  z. \text {id} )
    </tex>
    Finally:
    <tex display="block">
         \text {concat} _{x,y,z}(p,q) : \equiv  f(x,y,p,z,q)
    </tex>
    Or simply we denote <tex>\text {concat} (p,q)</tex> as <tex>p  \cdot  q</tex>
</block></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1396</anchor>  <taxon>Definition</taxon> <addr>def-002P</addr>  <route>def-002P.xml</route>   <title>Inverse Operation</title> </frontmatter> <mainmatter><p>
    Let <tex>A</tex> be a type. The <strong>inverse oepration</strong> is defined:
    <tex display="block">
         \text {inv} : (x:A) \to  (y:A) \to  (x=_Ay) \to  (y=_Ax)
    </tex></p><block open="open"><headline>
    <strong>Construction</strong>
</headline> 
    By induction, it suffices to construct:
    <tex display="block">
         \text {inv} (x,x, \text {refl} _x) : (x=_Ax)
    </tex>
    for any <tex>x:A</tex>. And trivially we have <tex>\text {inv} (x,x, \text {refl} _x) \equiv \text {refl} _x</tex>.
</block></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1397</anchor>  <taxon>Definition</taxon> <addr>def-002W</addr>  <route>def-002W.xml</route>   <title>Associator</title> </frontmatter> <mainmatter><p>
    Let <tex>A</tex> be a type.
    These are 3 consecutive identifications
    <tex display="block">
        p:x=_Ay,q:y=_Az, r:z=_Aw
    </tex>
    we define the <strong>associator</strong>:
    <tex display="block">
         \text {assoc} : (p,q,r): (p \cdot  q) \cdot  r = p \cdot  (q \cdot  r)
    </tex></p><block open="open"><headline>
    <strong>Construction</strong>
</headline> 
    By definition it suffices to show that
    <tex display="block">
         \Pi _{(z:A)} \Pi _{(q:x=_Az)}
         \Pi _{(w:A)} \Pi _{(r:z=_Aw)}
        ( \text {refl} _x \cdot  q) \cdot  r =  \text {refl} _x \cdot  (q \cdot  r)
    </tex>
    Let <tex>q:x=_Az</tex> and <tex>r:z=_Aw</tex>. By computation rule of identity types
    <tex display="block">
         \text {refl} _x \cdot  q  \equiv  q
    </tex>
    Then we can conclude that
    <tex display="block">
        ( \text {refl} _x \cdot  q) \cdot  r  \equiv  q \cdot  r
    </tex>
    similarly <tex>\text {refl} _x \cdot  (q \cdot  r)  \equiv  q \cdot  r</tex>.
    Hence we have the left and right side
    <tex display="block">
        ( \text {refl} _x \cdot  q) \cdot  r =  \text {refl} _x \cdot  (q \cdot  r)
    </tex>
    are judgementally equal, 
    so we can simply define the associator as
    <tex display="block">
         \text {assoc} ( \text {refl} _x,q,r) : \equiv   \text {refl} _{q \cdot  r}
    </tex>
</block></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1398</anchor>  <taxon>Definition</taxon> <addr>def-002X</addr>  <route>def-002X.xml</route>   <title>Unit Law Operations</title> </frontmatter> <mainmatter><p>
    Let <tex>A</tex> be a type.
    We defined the <strong>unit law</strong> operations for <tex>x=_Ay</tex>:
    <tex display="block">
         \begin {align*}
             \text {left-unit} &amp; : (x=_Ay) \to  ( \text {refl} _x \cdot  x = x)  \\ 
             \text {right-unit} &amp; : (x=_Ay) \to  (x \cdot \text {refl} _y = x)
         \end {align*}
    </tex></p><block open="open"><headline>
    <strong>Construction</strong>
</headline> 
    By elimination it suffices to construct:
    <tex display="block">
         \begin {align*}
             \text {left-unit}( \text {refl} _x) &amp;:  \text {refl} _x  \cdot   \text {refl} _x =  \text {refl} _x  \\ 
             \text {right-unit}( \text {refl} _x) &amp;:  \text {refl} _x  \cdot   \text {refl} _x =  \text {refl} _x
         \end {align*}
    </tex>
    In both cases we need only to construct <tex>\text {refl} _{ \text {refl} _x}</tex>.
</block></mainmatter> </tree></mainmatter> </tree><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1399</anchor>  <taxon>String Theory</taxon> <addr>phy-0001</addr>  <route>phy-0001.xml</route> <date><year>2024</year> <month>1</month> <day>29</day></date> <authors><author>CAIMEO</author> </authors> <title>Special Relativity and extra dimensions</title> </frontmatter> <mainmatter><p>
    Speical relativity is based on the exprimental fact that the speed of light is the same for all inertial observers.
    In comparing the coordinates of events, two inertial observers (<strong>Lorentz observers</strong>) find that the
    appropriate coordinate transformations mix space and time.   
</p><p>
    In special relativity events are characterized by their coordinates in space <tex>(x,y,z)</tex> and time (<tex>t</tex>).
    It's convenient to combine these into a four-vector where the <tex>t</tex> coordinate is multiplied by <tex>c</tex> (<strong>Speed of light</strong>): so 
    that all four coordinates have the same units (length).
    <tex display="block">x^ \mu  = (x^0,x^1,x^2,x^3) = (ct,x,y,z)</tex>
    The superscript <tex>\mu</tex> is called a <strong>Lorentz index</strong> and runs from 0 to 3.
</p><p>
    Consider a Lorentz frame <tex>S</tex> where two events are represented by the coordinates 
    <tex>x^ \mu</tex> and <tex>x^ \mu  +  \Delta  x^ \mu</tex>.
    Let <tex>S'</tex> be another Lorentz frame where the same two events are represented by the coordinates
    <tex>x'^ \mu</tex> and <tex>x'^ \mu  +  \Delta  x'^ \mu</tex>.
    The value of <tex>x'^ \mu</tex> is different from <tex>x^ \mu</tex> and so as <tex>\Delta  x'^ \mu</tex> from <tex>\Delta  x^ \mu</tex>.
    However there is an invariant <strong>interval</strong> <tex>\Delta  s^2</tex> defined by
    <tex display="block">
        - \Delta  s^2 = -( \Delta  x^0)^2 + ( \Delta  x^1)^2 + ( \Delta  x^2)^2 + ( \Delta  x^3)^2
    </tex>
    The minus sign in front of <tex>(x^0)^2</tex> encodes the fundamental difference between space and time coordinates.
</p><p>
    The invariant interval implies the following equation:
    <tex display="block">
         \Delta  s ^2 =  \Delta  s'^2
    </tex>
    The minus sign on the left of <tex>\Delta  s^2</tex> implies that <tex>\Delta  s^2 &gt;0</tex> for events that are <strong>timelike separated</strong>.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1400</anchor>  <taxon>Definition</taxon> <addr>def-001H</addr>  <route>def-001H.xml</route>   <title>Timelike Separated Events</title> </frontmatter> <mainmatter><p>
    An event <tex>S</tex> is said to be timelike separated if
    <tex display="block">
        ( \Delta  x^0)^2 &gt; ( \Delta  x^1)^2 + ( \Delta  x^2)^2 + ( \Delta  x^3)^2
    </tex>
    or briefly <tex>\Delta  s^2 &gt; 0</tex>. The spatial separation is less than the distance light travels.
</p></mainmatter> </tree><p>
    The history of a particle is represented in spacetime as a curve called a <strong>world-line</strong>.
    Any two events on the world-line are timelike separated, because no particle can 
    move faster than light.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1401</anchor>  <taxon>Definition</taxon> <addr>def-001I</addr>  <route>def-001I.xml</route>   <title>Lightlike Separated Events</title> </frontmatter> <mainmatter><p>
    Events connected by the world-line of a <strong>photon</strong> are said to be <strong>lightlike separated</strong>.
    For which <tex>\Delta  s^2 = 0</tex>.
</p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1402</anchor>  <taxon>Definition</taxon> <addr>def-001J</addr>  <route>def-001J.xml</route>   <title>Spacelike Separated Events</title> </frontmatter> <mainmatter><p>
    Two events for which <tex>\Delta  s^2 &lt; 0</tex> are said to be <strong>spacelike separated</strong>.
    Events that are simultaneous in a Lorentz frame but in different position are spacelike separated.
</p></mainmatter> </tree><p>
    For timelike event we can define
    <tex display="block">
         \Delta  s  \equiv   \sqrt { \Delta  s^2}
    </tex></p><p>
    It is useful to consider events that are <em>infinitesimally close</em> to each other.
    Small coordinate difference are needed to define velocity.
    Infinitesimal coordinate differences are written as <tex>dx^ \mu</tex>.
    <tex display="block">
        -ds^2 = -(dx^0)^2 + (dx^1)^2 + (dx^2)^2 + (dx^3)^2
    </tex>
    The equality of intervals is the statement
    <tex display="block">
        ds^2 = ds'^2
    </tex>
    Let's define a better notation:
    <tex display="block">
        dx_0  \equiv  -dx^0,
        dx_1  \equiv  dx^1,
        dx_2  \equiv  dx^2,
        dx_3  \equiv  dx^3
        
    </tex>
    Notice that the inclusion of the minus sign in the definition of <tex>dx_0</tex> is a matter of convention.
    <tex display="block">
        dx_ \mu  = (dx_0,dx_1,dx_2,dx_3)
    </tex>
    Now rewrite <tex>ds^2</tex> in terms of <tex>dx_ \mu</tex> and <tex>dx^ \mu</tex>:
    <tex display="block">
        -ds^2 = dx_0dx^0 + dx_1dx^1 + dx_2dx^2 + dx_3dx^3 =  \sum _{ \mu =0}^3 dx_ \mu  dx^ \mu 
    </tex>
    Using <link href="def-001K.xml" type="local" addr="def-001K" title="Einstein's Summation Convention">Einstein's Summation Convention</link> we can rewrite
    <tex display="block">
        ds^2 = dx_ \mu  dx^ \mu 
    </tex>
    And for Infinitesimal timelike intervals we can define
    <tex display="block">
        ds  \equiv   \sqrt {ds^2} 
    </tex>
    We can also express the interval <tex>ds^2</tex> using the <strong>Minkowski Metric</strong>:
    <tex display="block">
        -ds^2=  \eta _{ \mu \nu } dx^ \mu  dx^ \nu 
    </tex>
    and the metric is defined by
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1403</anchor>  <taxon>Definition</taxon> <addr>def-001L</addr>  <route>def-001L.xml</route>   <title>Minkowski Metric</title> </frontmatter> <mainmatter><p>
    The <strong>Minkowski Metric</strong>, aka <strong>Minkowski Tensor</strong>, is a tensor <tex>\eta _{ \mu \nu }</tex> whose elements are defined by the matrix
    <tex display="block">
         \eta _{ \mu \nu } =  \begin {pmatrix}
            -1 &amp; 0 &amp; 0 &amp; 0  \\ 
            0 &amp; 1 &amp; 0 &amp; 0  \\ 
            0 &amp; 0 &amp; 1 &amp; 0  \\ 
            0 &amp; 0 &amp; 0 &amp; 1 
         \end {pmatrix}
    </tex>
    where <tex>\mu</tex> and <tex>\nu</tex> are Lorentz indices run over <tex>0,1,2,3</tex>.
</p></mainmatter> </tree><p>
    How can we derive the Minkowski Metric? First we require <tex>\eta _{ \mu \nu }</tex> to be a symmetric matrix,
    because any antisymmetric part would not contribute to the interval.
    <tex display="block">
         \eta _{ \mu \nu } =  \eta _{ \nu \mu }
    </tex>
    And for any two-indexed object <tex>M_{ \mu \nu }</tex> can be decomposed into symmetric and antisymmetric parts:
    <tex display="block">
        M_{ \mu \nu } =  \frac {1}{2}(M_{ \mu \nu } + M_{ \nu \mu }) +  \frac {1}{2}(M_{ \mu \nu } - M_{ \nu \mu })
    </tex>
    With the antisymmetric part (denoted <tex>\delta</tex>) which we can see
    <tex display="block">
         \delta _{ \mu \nu }dx^ \mu  dx^ \nu  = (- \delta _{ \nu \mu }) dx^ \mu  dx^ \nu  = - \delta _{ \mu \nu }dx^ \nu  dx^ \mu  = - \delta _{ \mu \nu }dx^ \mu  dx^ \nu 
    </tex>
    Note that the second step relabeled the dummy indices <tex>\mu</tex> and <tex>\nu</tex>.
    The third step we swapped the order of the two terms. Hence the antisymmetric part is zero.
</p><p>
    The equation <tex>dx_ \mu  = (dx_0,dx_1,dx_2,dx_3)</tex> can be rewritten as
    <tex display="block">
        dx_ \mu  =  \eta _{ \mu \nu } dx^ \nu 
    </tex>
    For more general case:
    <tex display="block">
        b_ \mu   \equiv   \eta _{ \mu \nu } b^ \nu 
    </tex>
    Given <tex>a^ \mu</tex> and <tex>b^ \mu</tex> we can define the dot scalar product as 
    <tex display="block">
        a  \cdot  b  \equiv  a^ \mu  b_ \mu  = a^ \mu   \eta _{ \mu \nu } b^ \nu  = -a^0 b^0 + a^1 b^1 + a^2 b^2 + a^3 b^3
    </tex>
    Note that <tex>a^ \mu  b_ \mu  = a_ \mu  b^ \mu</tex> because <tex>\eta _{ \mu \nu }</tex> is symmetric.
</p><p>
    It's convenient to introduce the inverse matrix of <tex>\eta _{ \mu \nu }</tex>:
    <tex display="block">
         \eta ^{ \mu \nu } = 
         \begin {pmatrix}
            -1 &amp; 0 &amp; 0 &amp; 0  \\ 
            0 &amp; 1 &amp; 0 &amp; 0  \\ 
            0 &amp; 0 &amp; 1 &amp; 0  \\ 
            0 &amp; 0 &amp; 0 &amp; 1
         \end {pmatrix}
    </tex>
    And the inverse property is
    <tex display="block">
         \eta ^{ \mu \rho }  \eta _{ \rho \nu } =  \delta ^ \mu _ \nu 
    </tex>
    where <tex>\delta ^ \mu _ \nu</tex> is the <link href="def-001P.xml" type="local" addr="def-001P" title="Kronecker Delta">Kronecker Delta</link>.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1404</anchor>  <taxon>Trick</taxon> <addr>thm-0004</addr>  <route>thm-0004.xml</route>   <title>Raise Indices</title> </frontmatter> <mainmatter><p><tex display="block">
         \eta ^{ \rho \mu }b_ \mu 
        =  \eta ^{ \rho \mu } ( \eta _{ \mu \nu } b^ \nu )
        =  \eta ^{ \rho \mu }  \eta _{ \mu \nu } b^ \nu 
        =  \delta ^ \rho _ \nu  b^ \nu 
        = b^ \rho 
    </tex>
    The lower index of <tex>b_ \mu</tex> is raised to <tex>b^ \rho</tex> by <tex>\eta ^{ \rho \mu }</tex>.
</p></mainmatter> </tree><p><strong>Lorentz transformations</strong> are the relations between coordinates in two different
    inertial frames.
    
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1405</anchor>  <taxon>Definition</taxon> <addr>def-001Q</addr>  <route>def-001Q.xml</route>   <title>Lorentz Transformations</title> </frontmatter> <mainmatter><p>
    Consider a frame <tex>S</tex> and <tex>S'</tex> which is moving along the <tex>+x</tex> direction of the <tex>S</tex> frame
    with a velocity <tex>v</tex>.
    Assume that the origins of the two frames coincide at <tex>t=t'=0</tex> and coordinate axes are parallel.

    We say that <tex>S'</tex> is boosted along the <tex>x</tex> direction with velocity parameter <tex>\beta \equiv \frac {v}{c}</tex>.
    The <strong>Lorentz transformations</strong> are defined by a set of equations that relate the coordinates of an event in the two frames.
    <tex display="block">
         \begin {align*}
            x' &amp;=  \gamma (x- \beta  ct)  \\ 
            y' &amp;= y  \\ 
            z' &amp;= z  \\ 
            ct' &amp;=  \gamma (ct- \beta  x)
         \end {align*}
    </tex>
    where <tex>\gamma \equiv \dfrac {1}{ \sqrt {1- \beta ^2}} =  \dfrac {1}{ \sqrt {1- \frac {v^2}{c^2}}}</tex> is the <strong>Lorentz factor</strong>.
    The coordinates orthogonal to the <tex>x</tex> direction remains unchanged.
</p><p>
    Lorentz transformations are the linear transformations of coordinates that remains the <tex>\Delta  s^2</tex> unchanged. 
    We can write the Lorentz transformations in matrix form:
    <tex display="block">
         \begin {pmatrix}
            ct'  \\ 
            x'  \\ 
            y'  \\ 
            z' 
         \end {pmatrix}
        =
         \begin {pmatrix}
             \gamma  &amp; - \beta \gamma  &amp; 0 &amp; 0  \\ 
            - \beta \gamma  &amp;  \gamma  &amp; 0 &amp; 0  \\ 
            0 &amp; 0 &amp; 1 &amp; 0  \\ 
            0 &amp; 0 &amp; 0 &amp; 1 
         \end {pmatrix}
         \begin {pmatrix}
            ct  \\ 
            x  \\ 
            y  \\ 
            z 
         \end {pmatrix}
    </tex>
    Or in a more compact form:
    <tex display="block">
        x'^ \mu  = L^ \mu _ \nu  x^ \nu 
    </tex>
    where <tex>L^ \mu _ \nu</tex> is the <strong>Lorentz transformation matrix</strong> presented above.
</p></mainmatter> </tree><p>
    We now introduce a coordinate system that will be extremely useful in string theory,
    the <strong>light-cone coordinates</strong>.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1406</anchor>  <taxon>Definition</taxon> <addr>def-001R</addr>  <route>def-001R.xml</route>   <title>Light-cone Coordinates</title> </frontmatter> <mainmatter><p>
    The <strong>light-cone coordinates</strong> can be defined as
    two independent <link href="def-000L.xml" type="local" addr="def-000L" title="Linear Combination">linear combinations</link> of the time 
    and a chosen spatial coordinate (conventionally <tex>x^1</tex>):
    <tex display="block">
         \begin {align*}
            x^+  \equiv   \frac {1}{ \sqrt {2}} (X^0 + X^1)  \\ 
            x^-  \equiv   \frac {1}{ \sqrt {2}} (X^0 - X^1)
         \end {align*}
    </tex>
    while other spatial coordinates remain unchanged. Thus the complete set of 
    light-cone coordinates is <tex>(x^+,x^-,x^2,x^3)</tex>.
</p></mainmatter> </tree><p>
    The name <strong>light-cone coordinates</strong> comes from the fact that the associated coordinates axes
    are the world-lines of beams of light emitted form the origin along the <tex>x^1</tex> axis.
    <ul><li>
            For a beam of light moving in the positive <tex>x^1</tex> direction,
            we have <tex>x^1=ct=x^0</tex> and thus <tex>x^-=0</tex>. By definition <tex>x^-=0</tex>
            is actually the <tex>x^+</tex> axis. 
        </li>
        <li>
            For a beam of light moving in the negative <tex>x^1</tex> direction,
            we have <tex>x^1=-ct=-x^0</tex> and thus <tex>x^+=0</tex>. By definition <tex>x^+=0</tex>
            is actually the <tex>x^-</tex> axis. 
        </li></ul>
    The <tex>x^+</tex> and <tex>x^-</tex> axes are perpendicular to each other and at <tex>45^ \circ</tex> to the <tex>x^0</tex> and <tex>x^1</tex> axis.
</p><p>
    Both <tex>x^+</tex> and <tex>x^-</tex> can be a time coordinate although neither is a time coordinate
    in the standard sense of the world (Not ordinary time).
    For definiteness we will take <tex>x^+</tex> as the light-cone time coordinate and <tex>x^-</tex> as the spatial coordinate.
</p><p>
    Take differentials and multiply of the light-cone coordinates:
    <tex display="block">
        2dx^+ dx^- = (dx^0 + dx^1)(dx^0 - dx^1) = (dx^0)^2 - (dx^1)^2
    </tex>
    which follows the invariant interval
    <tex display="block">
        -ds^2 = -2dx^+ dx^- + (dx^2)^2 + (dx^3)^2
    </tex>
    As we did before, we can represent this with index notation:
    <tex display="block">
        -ds^2 =  \hat { \eta }_{ \mu \nu } dx^ \mu  dx^ \nu 
    </tex>
    where the <strong>light-cone metric</strong> is
    <tex display="block">
         \hat { \eta }_{ \mu \nu } = 
         \begin {pmatrix}
            0 &amp; -1 &amp; 0 &amp; 0  \\ 
            -1 &amp; 0 &amp; 0 &amp; 0  \\ 
            0 &amp; 0 &amp; 1 &amp; 0  \\ 
            0 &amp; 0 &amp; 0 &amp; 1
         \end {pmatrix}
    </tex>
    This is easy to derive from the symmetric.
</p><p>
    The light-cone coordinates looks unusual but if you see some
    calculations you will find the results very surprising.
</p><p>
    Consider a particle moving in the <tex>x^1</tex> direction with velocity <tex>v</tex>.
    At the initial time the positions are all <tex>0</tex>.
    (The velocity parameter is denote <tex>\beta</tex>)
    <tex display="block">
         \begin {align*}
            x^1 = vt =  \beta  x^0
             \\ 
            x^2(t) = x^3(t) = 0
         \end {align*}
    </tex>
    Now compute the light-cone coordinates:
    <tex display="block">
         \begin {align*}
            x^+ =  \frac {1}{ \sqrt {2}}(x^0 + x^1) =  \frac {1}{ \sqrt {2}}(x^0 +  \beta  x^0) =  \frac {1+ \beta }{ \sqrt {2}}x^0
             \\ 
            x^- =  \frac {1}{ \sqrt {2}}(x^0 - x^1) =  \frac {1}{ \sqrt {2}}(x^0 -  \beta  x^0) =  \frac {1- \beta }{ \sqrt {2}}x^0
         \end {align*}
    </tex>
    And we identify the ratio 
    <tex display="block">
         \frac {dx^+}{dx^-} =  \frac {1+ \beta }{1- \beta }
    </tex>
    as the light-cone velocity of the particle. This looks strange:
    <ul><li>
            For <tex>\beta =-1</tex> the light-cone velocity is <tex>\infty</tex>.
        </li>
        <li>
            For a particle moving at the speed of light (<tex>\beta =1</tex>), the light-cone velocity is <tex>0</tex>.
        </li>
        <li>
            More interestingly, a static particle (<tex>\beta =0</tex>) is moving quite fast in the light-cone coordinates.
        </li></ul>
    Note that the light-cone coordinates can't be acquired by Lorentz transformation.
</p><p>
    Just get the idea of the light-cone coordinates, now let's dive into the relativistic energy and momentum.
    In special relativity there is relationship between energy and momentum.
    <tex display="block">
         \frac {E^2}{c^2}- \vec {p}^2 = m^2c^2
    </tex>
    where <tex>m</tex> is the rest mass of the particle, and <tex>c</tex> is the speed of light.
    <tex display="block">
        E =  \gamma  mc^2,
         \quad  
         \vec {p} =  \gamma  m \vec {m}
    </tex>
    The energy and momentum can be used to define a momentum four-vector
    <tex display="block">
        p^ \mu  = ( \frac {E}{c},p_x, p_y, p_z)
    </tex>
    or shortly
    <tex display="block">
        p^ \mu  = ( \frac {E}{c}, \vec {p}) = m \gamma  (c,  \vec {v})
    </tex>
    Using operator <tex>\eta _{ \mu \nu }</tex> to lower the index:
    <tex display="block">
        p_ \mu  = (p_0, p_1, p_2, p_3) =  \eta _{ \mu \nu } p^ \nu  = (- \frac {E}{c},p_x, p_y, p_z)
    </tex>
    And make use of the relationship above.
    <tex display="block">
        p_ \mu  p^ \mu  =  \eta _{ \mu \nu } p^ \mu  p^ \nu  = - \frac {E^2}{c^2} + p_x^2 + p_y^2 + p_z^2 = -m^2c^2
    </tex>
    Using the relativistic scalar product notation:
    <tex display="block">
        p ^2  \equiv  p  \cdot  p = p_ \mu  p^ \mu  = -m^2c^2
    </tex></p><p>
    A central concept in special relativity is <strong>proper time</strong>,
    which is a Lorentz invariant measure of time.
    Consider a world-line and two events <tex>A</tex> and <tex>B</tex> on the world-line.
    Different Lorentz observers will measure different time intervals between the two events.
    But imagine a clock that moves along the world-line.
    The time measured by the clock is called the <strong>proper time</strong> between the two events.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1407</anchor>  <taxon>Definition</taxon> <addr>def-001S</addr>  <route>def-001S.xml</route>   <title>Proper Time</title> </frontmatter> <mainmatter><p>
    The <strong>proper time</strong> along a timelike world-line is defined as the
    time as measured by a clock following that line. 
</p></mainmatter> </tree><p>
    By this definition, proper time is a invariant. Consider an invariant interval 
    for the motion of a particle along <tex>x</tex> axis:
    <tex display="block">
        -ds^2 = -c^2 dt^2 + dx^2 = -c^2 dt^2 (1 -  \beta ^2)
    </tex>
    Now attach a Lorentz frame to the particle does not move
    and the time is recorded by the clock that is moving with the particle.
    Hence <tex>dx=0</tex> and <tex>dt=dt_p</tex> is the proper time.
    <tex display="block">
        -ds^2 = -c^2 dt_p^2 
    </tex>
    Cancel the minus sign and the square root
    <tex display="block">
        dt_p = c dt_p
    </tex>
    This shows that for timelike intervals,
    the <strong>proper time interval</strong> is <tex>\frac {ds}{c}</tex>.
    Similarly, 
    <tex display="block">
        ds = cdt  \sqrt {1- \beta ^2}  \implies   \frac {dt}{ds} =  \frac { \gamma }{c}
    </tex>
    The invariant <tex>ds</tex> can be used to construct nre Lorentz vectors.
    For instance, we can construct velocity four-vector:
    <tex display="block">
        u^ \mu  = c  \frac {dx^ \mu }{ds} = c ( \frac {d(ct)}{ds},  \frac {dx}{ds},  \frac {dy}{ds},  \frac {dz}{ds})
    </tex>
    This can be simplified by using the definition of proper time:
    <tex display="block">
         \frac {dx}{ds} =  \frac {dx}{dt}  \frac {dt}{ds} =  \frac {v_x \gamma }{c}
    </tex>
    Hence we find
    <tex display="block">
        u^ \mu  = ( \gamma  c,  \gamma  v_x,  \gamma  v_y,  \gamma  v_z) =  \gamma  (c,  \vec {v})
    </tex>
    We see that the momentum four-vector is just the velocity four-vector multiplied by the rest mass.
    <tex display="block">
        p^ \mu  = m u^ \mu 
    </tex></p><p>
    The light-cone components <tex>p^+</tex> and <tex>p^-</tex> of the momentum Lorentz vector are obtained:
    <tex display="block">
         \begin {align*}
            p^+ =  \frac {1}{ \sqrt {2}}(p^0+p^1) = -p_-
             \\ 
            p^- =  \frac {1}{ \sqrt {2}}(p^0-p^1) = -p_+
         \end {align*}
    </tex>
    Note that light-cone coordinates do not transform as Lorentz ones do. 
    Both <tex>p^ \pm</tex> are energy-like since both are positive for physical particles.
    <tex display="block">
        p^0 =  \frac {E}{c}  \sqrt { \vec {p}^2 + m^2c^2} &gt; | \vec {p}| \geq  |p^1|
    </tex>
    Hence <tex>p^0 \pm  p^1 &gt; 0</tex> and <tex>p^ \pm &gt;0</tex>, which both are possible candidates for energy.
    We finally choose <tex>p^-</tex> as the component, we explain this later. 
</p></mainmatter> </tree></mainmatter> <backmatter><contributions/> <context><tree expanded="false" show-heading="true" show-metadata="true" toc="false" numbered="false" root="true"><frontmatter><anchor>1408</anchor>   <addr>index</addr>  <route>index.xml</route>  <authors><author>CAIMEO</author> </authors> <title>The Rabbit Hole</title> </frontmatter> <mainmatter>
    
    <tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="false" root="false"><frontmatter><anchor>1409</anchor>  <taxon>Person</taxon> <addr>caimeo</addr>  <route>caimeo.xml</route>   <title>CAIMEO</title> <meta name="position">Student</meta><meta name="external">https://github.com/CAIMEOX</meta></frontmatter> <mainmatter><p>
    A student interested in math, physics and computer science.
</p><tex display="block">
    i \hbar   \frac { \partial }{ \partial  t}  \Psi (x, t) =  \hat {H}  \Psi (x, t)
</tex><ul><li>Learning Programming Language Theory and Type Theory</li>
    <li>Attend in writing articles about String Theory</li>
    <li>Reading Type Theory and Formal Proof and Homotopy Type Theory</li>
    <li>Working on CommandLisp</li></ul></mainmatter> </tree>
    <tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="false" root="false"><frontmatter><anchor>1410</anchor>   <addr>about</addr>  <route>about.xml</route>   <title>About this website</title> </frontmatter> <mainmatter><p>
    The choice of the name &quot;<strong>Rabbit Hole</strong>&quot; carries a metaphorical significance inspired by Lewis Carroll's <em>Alice's Adventures 
    in Wonderland</em>, meaning to delve into a topic or pursue a line of thought that leads to unexpected or complex places.
    It can refer to getting deeply involved in researching a subject, exploring a particular interest, or going through 
    a series of trees in the forest that leads to a chain of related topics.
</p>
    <strong>How to navigate?</strong>
    <p>This website is a <em>forest</em> created using the <strong>Forester</strong> tool.
    To navigate my forest, press <code>Ctrl-K</code>.
    Here are some standards of this blog.</p>
    <ul><li>All posts starts with a prefix and appends with a hex number</li>
        <li>Available post prefixes:
            <ul><li><code>cs</code> Computer Science</li>
                <li><code>math</code> Mathematics</li>
                <li><code>phy</code> Physics</li>
                <li><code>plt</code> Programming language theory</li>
                <li><code>tt</code> Type Theory</li>
                <li><code>def</code> Definitions (For any topic above)</li>
                <li><code>thm</code> Theorems and propositions (For any topic above)</li>
                <li><code>eg</code> Examples</li>
                <li><code>proj</code> My Project</li></ul></li></ul>
</mainmatter> </tree>

    
    
    <tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1411</anchor>   <addr>notes</addr>  <route>notes.xml</route>   <title>Notes</title> </frontmatter> <mainmatter><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1412</anchor>  <taxon>Type Theory</taxon> <addr>tt-0001</addr>  <route>tt-0001.xml</route> <date><year>2024</year> <month>1</month> <day>26</day></date>  <title>Untyped Lambda Calculus</title> </frontmatter> <mainmatter><p>
In dealing with functions there are two <strong>construction principles</strong> and one <strong>evalutaion rule</strong>
<ul><li>Construction Principles</li>
<ul><li>Function Abstraction: <tex>\lambda  x.M</tex></li>
<li>Function Application: <tex>M N</tex></li></ul>
<li>Evaluation Rule</li>
<ul><li>Beta Reduction: <tex>( \lambda  x.M)N \to  M[N/x]</tex></li></ul></ul>
The beta reduction makes use of the <strong>substitution</strong> <tex>M[N/x]</tex> which represents the result of replacing all free occurences of <tex>x</tex> in <tex>M</tex> with <tex>N</tex>.
</p>
<p>Expressions in the lambda calculus is called <strong>terms</strong>. The set of terms is denoted <tex>\Lambda</tex>.</p>
<tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1413</anchor>  <taxon>Definition</taxon> <addr>def-000F</addr>  <route>def-000F.xml</route>   <title>Set of Lambda Terms</title> </frontmatter> <mainmatter><p>
Let <tex>\Lambda</tex> be the set of lambda terms. Then <tex>\Lambda</tex> is defined inductively as follows:
(<tex>V</tex> is the set of variables)
<ul><li>Variable: <tex>\forall  x \in  V, x \in   \Lambda</tex></li>
<li>Abstraction: <tex>\forall  x \in  V, M \in   \Lambda ,  \lambda  x.M \in   \Lambda</tex></li>
<li>Application: <tex>\forall  M,N \in   \Lambda , (MN) \in   \Lambda</tex></li></ul></p><p>
Another way to define <tex>\Lambda</tex> is to use the following grammar (The 3 possibilities are separated by <code>|</code>):
<tex display="block">\Lambda  = V |  \lambda  V. \Lambda  |  \Lambda \Lambda</tex></p></mainmatter> </tree> 
</mainmatter> </tree><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1414</anchor>  <taxon>Set Theory</taxon> <addr>math-0003</addr>  <route>math-0003.xml</route> <date><year>2024</year> <month>1</month> <day>26</day></date>  <title>Set Theory</title> </frontmatter> <mainmatter><p><strong>Set</strong> is a common concept in mathematics.
    This post is a brief introduction to set theory aimed at 
    complete all basic knowledge of set theory.
    The following topics will be covered
    <ul><li><strong>Zermelo-Fraenkel Axioms</strong> and <strong>Axiom of Choice</strong></li>
        <li>Cardinality</li>
        <li>Set theory constructions</li></ul></p><p>
    In this post, we use the Zermelo-Fraenkel set theory with the Axiom of Choice (<strong>ZFC</strong>).
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1415</anchor>  <taxon>Definition</taxon> <addr>def-000S</addr>  <route>def-000S.xml</route>   <title>ZFC Set</title> </frontmatter> <mainmatter><p><strong>ZFC</strong> is the abbreviation of Zermelo-Fraenkel set theory with the Axiom of Choice.
    The axioms of ZFC are listed below.
    <ul><li><strong>Axiom of Extensionality</strong>:
            Two sets are equal if and only if they have the same elements.
        </li>
        <li><strong>Axiom of Pairing</strong>:
            For any two sets <tex>a</tex> and <tex>b</tex>,
            there exists a set <tex>\{   a,b   \}</tex> whose elements are exactly <tex>a</tex> and <tex>b</tex>.
        </li>
        <li><strong>Axiom schema of Separation</strong>:
            Let <tex>P</tex> is a property of sets.
            <tex>P(u)</tex> means <tex>u</tex> satisfies the property <tex>P</tex>.
            then for any set <tex>X</tex> exists <tex>Y =  \{   u  \in  X | P(u)   \}</tex>.
        </li>
        <li><strong>Axiom of Union</strong>:
            For any set <tex>X</tex> (a family of sets), exists union set <tex>\bigcup  X : \equiv   \{   
                u: \exists  v \in  X  \text { such that } u \in  v
               \}</tex>.
        </li>
        <li><strong>Axiom of Power Set</strong>:
            For any set <tex>X</tex>, exists the power <tex>P(X) : \equiv   \{   Y:Y \subseteq  X   \}</tex>.
        </li>
        <li><strong>Axiom of Infinity</strong>:
            There exists a set <tex>\omega</tex> such that <tex>\emptyset \in \omega</tex> and for any <tex>x \in \omega</tex>, <tex>x \cup \{   x   \} \in \omega</tex>.
        </li>
        <li><strong>Axiom of Regularity</strong>:
            For any non-empty set there is a minimal element with respect to the membership relation.
        </li>
        <li><strong>Axiom schema of Replacement</strong>:
            Let <tex>F</tex> be a function where <tex>\text {dom }  f = X</tex>, then for any set <tex>X</tex> exists a set <tex>Y =  \{   F(x):x \in  X   \}</tex>.
            <p>
                This function is not the normal function but some logical stuff.
            </p></li>
        <li><strong>Axiom of Choice</strong>:
            For any family of non-empty sets <tex>X</tex>, there exists a function <tex>f:X \to \bigcup  X</tex> such that for any <tex>x \in  X</tex>, <tex>f(x) \in  x</tex>.
        </li></ul></p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1416</anchor>  <taxon>Definition</taxon> <addr>def-002V</addr>  <route>def-002V.xml</route>   <title>Set Operations</title> </frontmatter> <mainmatter><p>
    Let <tex>(X_i)_{i \in  I}</tex> be a family of sets.
</p><block open="open"><headline><strong>Union</strong></headline> 
    <tex display="block">
         \bigcup _{i \in  I}X_i =  \set {x: \exists  i \in  I  \text { such that } x \in  X_i}
    </tex>
</block><block open="open"><headline><strong>Intersection</strong></headline> 
    <tex display="block">
         \bigcap _{i \in  I}X_i =  \set {x: \forall  i \in  I, x \in  X_i}
    </tex>
    Note that <tex>I  \neq   \emptyset</tex> here.
</block><block open="open"><headline><strong>Disjoint Union</strong></headline> 
    <tex display="block">
         \bigsqcup _{i \in  I}X_i =  \set {(x,i):x \in  X_i, i \in  I}
    </tex>
</block><block open="open"><headline><strong>Product</strong></headline> 
    <tex display="block">
         \prod _{i \in  I}X_i =  \set {(x_i)_{i \in  I}: \forall  i \in  I, x_i \in  X_i}
    </tex>
</block></mainmatter> </tree><p>
    And principles of set theory
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1417</anchor>  <taxon>Definition</taxon> <addr>def-000T</addr>  <route>def-000T.xml</route>   <title>Principle of Extensionality</title> </frontmatter> <mainmatter><p>
    Two sets are equal if and only if they have the same elements.
</p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1418</anchor>  <taxon>Definition</taxon> <addr>def-000U</addr>  <route>def-000U.xml</route>   <title>Principle of Comprehension</title> </frontmatter> <mainmatter><p>
    Given a set <tex>A</tex> and a property <tex>P(x)</tex>, there exists a set <tex>B</tex> such that
    <tex>x \in  B  \iff  x \in  A  \land  P(x)</tex>.
</p></mainmatter> </tree><p>
    We then define the Cartesian product of two sets
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1419</anchor>  <taxon>Definition</taxon> <addr>def-000V</addr>  <route>def-000V.xml</route>   <title>Cartesian product</title> </frontmatter> <mainmatter><p>
    Given two sets <tex>A</tex> and <tex>B</tex>, the Cartesian product <tex>A \times  B</tex> is the set
    of all ordered pairs <tex>(a,b)</tex> where <tex>a \in  A</tex> and <tex>b \in  B</tex>.
</p></mainmatter> </tree><p>
    With the Cartesian product, we can define the relation
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1420</anchor>  <taxon>Definition</taxon> <addr>def-000W</addr>  <route>def-000W.xml</route>   <title>Relation</title> </frontmatter> <mainmatter><p>
    A <strong>relation</strong> <tex>R</tex> is a subset of the Cartesian product of two sets <tex>A</tex> and
    <tex>B</tex>, i.e. <tex>R \subseteq  A \times  B</tex>.
    If <tex>(a,b) \in  R</tex>, we write <tex>aRb</tex>.

    A relation that between <tex>X</tex> and itself is called <strong>homogeneous relation</strong>.
</p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1421</anchor>  <taxon>Definition</taxon> <addr>def-000X</addr>  <route>def-000X.xml</route>   <title>Equivalence Relation</title> </frontmatter> <mainmatter><p>
    An equivalence relation <tex>R</tex> on a set <tex>A</tex> is a <link href="def-000W.xml" type="local" addr="def-000W" title="Relation">relation</link> that is reflexive,
    symmetric, and transitive.
    <ul><li>Reflexive:
            <tex>\forall  x \in  A, xRx</tex></li>
        <li>Symmetric:
            <tex>\forall  x,y \in  A, xRy \implies  yRx</tex></li>
        <li>Transitive:
            <tex>\forall  x,y,z \in  A, xRy \land  yRz \implies  xRz</tex></li></ul>
    We often denote the equivalence relation by <tex>\sim</tex>.
</p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1422</anchor>  <taxon>Definition</taxon> <addr>def-002U</addr>  <route>def-002U.xml</route>   <title>Equivalence Class</title> </frontmatter> <mainmatter><p>
    Let <tex>\sim</tex> be an <link href="def-000X.xml" type="local" addr="def-000X" title="Equivalence Relation">equivalence relation</link> on a set <tex>A</tex>.
    For any element <tex>a \in  A</tex>, the <strong>equivalence class</strong> of <tex>a</tex> is the set
    <tex>[a] =  \set {b \in  A:b \sim  a}</tex>.
    The set of all equivalence classes is denoted by <tex>A/ \sim</tex>,
    which is called the <strong>quotient set</strong> of <tex>A</tex> by <tex>\sim</tex>.
    <p>
        The equivalence class of <tex>a</tex> is also denoted by <tex>\overline {a}</tex>.
    </p></p></mainmatter> </tree><p>
    One of the most important relations is the order relation.
    The basic order relation is the preorder.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1423</anchor>  <taxon>Definition</taxon> <addr>def-000Z</addr>  <route>def-000Z.xml</route>   <title>Preorder</title> </frontmatter> <mainmatter><p>
    A <strong>preorder</strong> is a relation <tex>\leq</tex> that is reflexive and transitive.
    <ul><li>Reflexive: <tex>a \leq  a</tex></li>
        <li>Transitive: <tex>a \leq  b</tex> and <tex>b \leq  c</tex> implies <tex>a \leq  c</tex></li></ul></p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1424</anchor>  <taxon>Definition</taxon> <addr>def-000Y</addr>  <route>def-000Y.xml</route>   <title>Partial Order</title> </frontmatter> <mainmatter><p>
    A <strong>(non-strict) partial order</strong> is a relation <tex>\leq</tex> that is reflexive, antisymmetric and transitive.
    <ul><li>Reflexive: <tex>a \leq  a</tex></li>
        <li>Antisymmetric: <tex>a \leq  b</tex> and <tex>b \leq  a</tex> implies <tex>a=b</tex></li>
        <li>Transitive: <tex>a \leq  b</tex> and <tex>b \leq  c</tex> implies <tex>a \leq  c</tex></li></ul>
    A non-strict partial order is also known as an antisymmetric <link href="def-000Z.xml" type="local" addr="def-000Z" title="Preorder">preorder</link>.
</p></mainmatter> </tree><p>
    And the strict partial order (notice the difference between asymmetric and antisymmetric)
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1425</anchor>  <taxon>Definition</taxon> <addr>def-0010</addr>  <route>def-0010.xml</route>   <title>Strict partial orders</title> </frontmatter> <mainmatter><p>
    A strict partial order is a relation <tex>&lt;</tex> that is irreflexive, asymmetric and transitive.
    <ul><li>Irreflexive: <tex>\neg (a&lt;a)</tex></li>
        <li>Asymmetric: <tex>a&lt;b</tex> implies <tex>\neg (b&lt;a)</tex></li>
        <li>Transitive: <tex>a&lt;b</tex> and <tex>b&lt;c</tex> implies <tex>a&lt;c</tex></li></ul></p></mainmatter> </tree><p>
    With the definition of order, we can define the upper bound and lower bound
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1426</anchor>  <taxon>Definition</taxon> <addr>def-0011</addr>  <route>def-0011.xml</route>   <title>Upper Bound and Lower Bound</title> </frontmatter> <mainmatter><p>
    Let a subset <tex>S</tex> of a <link href="def-000Y.xml" type="local" addr="def-000Y" title="Partial Order">partially ordered</link> set <tex>(P,  \leq )</tex>,
    <tex>S</tex> is bounded above if there exists <tex>x  \in  P</tex> such that <tex>\forall  y  \in  S, y  \leq  x</tex>. And <tex>x</tex> is called an <strong>upper bound</strong> of <tex>S</tex>.
    Dually, <tex>S</tex> is bounded below if there exists <tex>x  \in  P</tex> such that <tex>\forall  y  \in  S, x  \leq  y</tex>. And <tex>x</tex> is called a <strong>lower bound</strong> of <tex>S</tex>.
</p>
    <p><strong>Supremum (least upper bound)</strong></p>
    <p>
    An element <tex>x \in  P</tex> is a supremum of <tex>S</tex>,
    if for all upper bounds <tex>z  \in  P</tex> of <tex>S</tex>, <tex>x  \leq  z</tex>.
    Denoted as <tex>x =  \sup  S</tex>.
    </p>
    <p><strong>Infimum (greatest lower bound)</strong></p>
    <p>
    An element <tex>x \in  P</tex> is a infimum of <tex>S</tex>,
    if for all lower bounds <tex>z  \in  P</tex> of <tex>S</tex>, <tex>z  \leq  x</tex>.
    Denoted as <tex>x =  \inf  S</tex>.
    </p>
</mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1427</anchor>  <taxon>Definition</taxon> <addr>def-002G</addr>  <route>def-002G.xml</route>   <title>Function</title> </frontmatter> <mainmatter><p>
    Let <tex>X</tex> and <tex>Y</tex> be sets then a <strong>function</strong> <tex>f:X  \to  Y</tex>
    is a mapping that sends each element of <tex>X</tex> to a unique element of <tex>Y</tex>,
    denoted by <tex>f(x) = y</tex>.
    Function is a special case of <link href="def-000W.xml" type="local" addr="def-000W" title="Relation">relation</link>, and it is a relation that is left-total and right-unique.
    <tex display="block">
        f  \in  X  \times  Y  \text { and }  \forall  x  \in  X,  \exists ! y  \in  Y, (x,y)  \in  f
    </tex>
    <tex>X</tex> is said to be the <strong>domain</strong> of <tex>f</tex> and <tex>Y</tex> is said to be the <strong>codomain</strong> of <tex>f</tex>,
    where we denote <tex>X =  \text {dom }  f</tex> and <tex>Y =  \text {cod }  f</tex>.
</p><p>
    Two functions <tex>f:X \to  Y</tex> and <tex>g:Y \to  Z</tex> can be <strong>composed</strong> to form a new function <tex>g  \circ  f : X  \to  Z</tex>,
    where the composition is defined by
    <tex display="block">
        (g  \circ  f)(x) = g(f(x)) 
    </tex></p><p>
    The set of all functions from <tex>X</tex> to <tex>Y</tex> is denoted by <tex>\hom _ \text {set} (X, Y)</tex>.
</p></mainmatter> </tree><p>
    The isomorphism function is defined as follows
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1428</anchor>  <taxon>Definition</taxon> <addr>def-002H</addr>  <route>def-002H.xml</route>   <title>Set Isomorphism</title> </frontmatter> <mainmatter><p>
    Let <tex>X</tex> and <tex>Y</tex> be sets and <tex>f: X  \to  Y</tex> be a function.
    The function <tex>f</tex> is called an <strong>isomorphism</strong> if it is both <link href="def-002D.xml" type="local" addr="def-002D" title="Injective">injective</link> and <link href="def-002F.xml" type="local" addr="def-002F" title="Surjective">surjective</link>.
    In other words, there exists a function <tex>g: Y  \to  X</tex> such that
    <tex display="block">
        g  \circ  f =  \text {id} _X  \text { and } f  \circ  g =  \text {id} _Y
    </tex>
    where <tex>\text {id} _X</tex> and <tex>\text {id} _Y</tex> are the <strong>identity functions</strong> on <tex>X</tex> and <tex>Y</tex> respectively.
    And we say <tex>f</tex> is <strong>invertible</strong> and <tex>g</tex> is the <strong>inverse</strong> of <tex>f</tex>.
    If there is a isomorphism between <tex>X</tex> and <tex>Y</tex>, we say <tex>X</tex> and <tex>Y</tex> are <strong>isomorphic</strong>,
    denoted by <tex>X  \cong  Y</tex>.
    Isomorphism is an <link href="def-000X.xml" type="local" addr="def-000X" title="Equivalence Relation">equivalence relation</link>.
</p></mainmatter> </tree><p>
    With isomorphism, we can define the cardinality of a set.
    Two isomorphic sets have the same cardinality.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1429</anchor>  <taxon>Definition</taxon> <addr>def-002I</addr>  <route>def-002I.xml</route>   <title>Cardinality</title> </frontmatter> <mainmatter><p>
    Let <tex>X</tex> be a set and <tex>n  \in   \mathbb {N}</tex>. 
    <tex>A</tex> si said to have <strong>cardinality</strong> <tex>n</tex>, denoted by <tex> |A|= n</tex>,
    if there exists an isomorphism between <tex>A</tex> and <tex>S_n =  \{   1,2, \cdots ,n   \}</tex>.
    If <tex>A</tex> has finite cardinality, we say <tex>A</tex> is <strong>finite</strong>, otherwise
    we say <tex>A</tex> is <strong>infinite</strong>, denoted by <tex>|A|  \geq   \infty</tex>.
</p></mainmatter> </tree><p>
    The next topic is the product of sets
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1430</anchor>  <taxon>Definition</taxon> <addr>def-002J</addr>  <route>def-002J.xml</route>   <title>Product of Sets</title> </frontmatter> <mainmatter><p>
    Let <tex>X</tex> and <tex>Y</tex> be sets, then the <strong>Cartesian product</strong> of <tex>X</tex> and <tex>Y</tex> is the set
    <tex display="block">
        X  \times  Y =  \set {(x,y)  \mid  x  \in  X  \text { and } y  \in  Y}
    </tex>
    There are two natural projections from the Cartesian product to the original sets, namely
    <tex display="block">
         \pi _1 : X  \times  Y  \to  X  \text { and }  \pi _2 : X  \times  Y  \to  Y
    </tex></p></mainmatter> </tree><p>
    This leads to an improtant concept named <strong>universal property</strong>.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1431</anchor>  <taxon>Lemma</taxon> <addr>thm-000J</addr>  <route>thm-000J.xml</route>   <title>Universal Property for Product of Sets</title> </frontmatter> <mainmatter><p>
    Let <tex>X</tex> and <tex>Y</tex> be sets.
    For any set <tex>A</tex> and function
    <tex>f: A  \to  X</tex> and <tex>g: A  \to  Y</tex>,
    there exists a <em>unique</em> function <tex>h: A  \to  X  \times  Y</tex> such that
    the following diagram commutes:
    
    <center><img src="resources/4157eb89f51117c585cb94d00d036a56.svg"/></center>

    We might denote the unique function by <tex>\langle  f,g  \rangle : A  \to  X  \times  Y</tex>.
    It is sufficient to define <tex>\langle  f,g  \rangle (a) = (f(a),g(a))</tex> for all <tex>a \in  A</tex> as the unique function.
</p></mainmatter> </tree><p>
    Dual to the product of sets, we have the coproduct of sets
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1432</anchor>  <taxon>Definition</taxon> <addr>def-002K</addr>  <route>def-002K.xml</route>   <title>Coproduct of Sets</title> </frontmatter> <mainmatter><p>
    Let <tex>X</tex> and <tex>Y</tex> be sets, then the <strong>coproduct</strong> of <tex>X</tex> and <tex>Y</tex> is 
    defined as the <strong>disjoint union</strong> of <tex>X</tex> and <tex>Y</tex>, denoted by <tex>X  \sqcup  Y</tex>.
    There are two natural injections from the original sets to the coproduct, namely
    <tex display="block">
        i_1 : X  \to  X  \sqcup  Y  \text { and } i_2 : Y  \to  X  \sqcup  Y
    </tex></p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1433</anchor>  <taxon>Lemma</taxon> <addr>thm-000K</addr>  <route>thm-000K.xml</route>   <title>Universal Property for Coproduct of Sets</title> </frontmatter> <mainmatter><p>
    Let <tex>X</tex> and <tex>Y</tex> be sets. For any set <tex>A</tex> and function
    <tex>f : X  \to  A</tex> and <tex>g : Y  \to  A</tex>, there exists a <em>unique</em> function
    <tex>h : X  \sqcup  Y  \to  A</tex> such that the following diagram commutes:
    
    <center><img src="resources/31473672edfba5d6215d76dd08a01a24.svg"/></center>

    We might denote the unique as <tex>f \sqcup  g: X  \sqcup  Y  \to  A</tex>.
</p></mainmatter> </tree><p>
    In this section we discuss the <em>limits</em> of variously-shaped diagrams of sets.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1434</anchor>  <taxon>Definition</taxon> <addr>def-002L</addr>  <route>def-002L.xml</route>   <title>Pullback of Sets</title> </frontmatter> <mainmatter><p>
    Suppose we have sets <tex>X</tex>, <tex>Y</tex>, and <tex>Z</tex> and functions
    <tex>f : X  \to  Z</tex> and <tex>g : Y  \to  Z</tex>.
    
    <center><img src="resources/aea0109fc5888410344cbfd1c2bfad2d.svg"/></center>

    Its <strong>fiber product</strong> is the set
    <tex display="block">
        X  \times _Z Y =  \{   (x,w,y)  \mid  f(x) = w = g(y)   \} 
    </tex>
    There are obvious projections 
    <tex>
         \pi _1 : X  \times _Z Y  \to  X  \text { and }  \pi _2 : X  \times _Z Y  \to  Y
    </tex>
    such that the following diagram commutes (<tex>W = X  \times _Z Y</tex>):
    
    <center><img src="resources/9194755931a1b27a68b010256252d579.svg"/></center>

    The <strong>pullback</strong> is defined to be any set <tex>W  \cong  X \times _Z Y</tex>
    The corner symbol indicates <tex>W</tex> is a <em>pullback</em></p></mainmatter> </tree><p>
    The pullback also satisfies the universal property.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1435</anchor>  <taxon>Lemma</taxon> <addr>thm-000L</addr>  <route>thm-000L.xml</route>   <title>Universal Property for Pullback</title> </frontmatter> <mainmatter><p>
    Suppose the given diagram:
    
    <center><img src="resources/5e6f2da5af96f3d2b5aa340ada59f646.svg"/></center>

    For any set <tex>A</tex> and commutative solid arrow diagram as below
    (functions <tex>f:A \to  X</tex> and <tex>g:A \to  Y</tex> such that <tex>t \circ  f = u \circ  g</tex>):
    
    <center><img src="resources/2d3ff6bcdbabe8193a5f90642c805f62.svg"/></center>

    there exists a <em>unique</em> arrow <tex>\langle  f,g  \rangle _Z: A \to  X \times _Z Y</tex> such that
    <tex display="block">
         \pi _1 \circ \langle  f,g  \rangle _Z = f  \text { and }  \pi _2 \circ \langle  f,g  \rangle _Z = g
    </tex></p></mainmatter> </tree></mainmatter> </tree><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1436</anchor>  <taxon>Math Analysis</taxon> <addr>math-0004</addr>  <route>math-0004.xml</route> <date><year>2024</year> <month>1</month> <day>27</day></date>  <title>The construction of <tex>\mathbb {R}</tex></title> </frontmatter> <mainmatter><p>
    We start constructing <tex>\mathbb {R}</tex> from <tex>\mathbb {Q}</tex> by a way that it satisfies the existence theorem,
    the core of construction.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1437</anchor>  <taxon>Theorem</taxon> <addr>thm-0003</addr>  <route>thm-0003.xml</route>   <title>Existence theorem</title> </frontmatter> <mainmatter><p>
    There exists an ordered field <tex>\mathbb {R}</tex> that satisfies the <link href="def-0012.xml" type="local" addr="def-0012" title="Least upper bound property">least upper bound property</link>.
    Moreover <tex>\mathbb {R}</tex> contains <tex>\mathbb {Q}</tex> as a subfield.
</p></mainmatter> </tree><p>
    The least-upper-bound property mentioned above is defined:
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1438</anchor>  <taxon>Definition</taxon> <addr>def-0012</addr>  <route>def-0012.xml</route>   <title>Least upper bound property</title> </frontmatter> <mainmatter><p>
    A set <tex>S</tex> has the least upper bound property if every non-empty subset <tex>T</tex> of <tex>S</tex> that is bounded above has a least upper bound <tex>\sup  T</tex>.
</p></mainmatter> </tree><p>
    Why do we need the least-upper-bound property?
    Consider the set <tex>S =  \{ x  \in   \mathbb {Q} | x^2 &lt; 2 \}</tex>.
    <tex>S</tex> is bounded above by <tex>2</tex>, but it does not have a least upper bound in <tex>\mathbb {Q}</tex>.
    Therefore we can't express <tex>\sqrt {2}</tex> in field <tex>\mathbb {Q}</tex> since some &quot;gaps&quot; exist.
    This fact motivates us to construct a more complete field <tex>\mathbb {R}</tex>.
    We have constructed <tex>\mathbb {Q}</tex> from <tex>\mathbb {Z}</tex>, and now we construct <tex>\mathbb {R}</tex> from <tex>\mathbb {Q}</tex>.
</p><p>
    Then we should find a way to express &quot;<tex>\sqrt {2}</tex>&quot; using <tex>\mathbb {Q}</tex>.
    A crucial idea is <strong>approximating</strong> <tex>\sqrt {2}</tex> by a sequence of rational numbers.
    <tex display="block">
         \sqrt {2} :=  \{  p^2&lt;2  \lor  p&lt;0, p \in \mathbb {Q}  \} 
    </tex>
    We can cut the number axis into two pieces by <tex>\sqrt {2}</tex>, such cut is called a <strong>Dedekind cut</strong>. 
    A cut should be well-defined rather than just an intuitive concept.
</p><p>
    As we use set theory to construct <tex>\mathbb {R}</tex>, it motivates us to define Dedekind cut as a set.
    It should satisfies some properties:
    <ul><li>Can't be empty or the whole <tex>\mathbb {Q}</tex></li>
        <li>Closed downward</li>
        <li>Contains not the largest number</li></ul>
    A formal definition is given below:
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1439</anchor>  <taxon>Definition</taxon> <addr>def-0013</addr>  <route>def-0013.xml</route>   <title>Dedekind cuts</title> </frontmatter> <mainmatter><p>
    A Dedekind cut is a partition of the rationals <tex>\mathbb {Q}</tex> into two non-empty sets <tex>L</tex> and <tex>R</tex> such that:
    <ul><li><tex>L \neq \emptyset</tex></li>
        <li><tex>R \neq \emptyset</tex></li>
        <li>if <tex>x,y \in \mathbb {Q}, x&lt;y</tex> and <tex>y \in  L</tex> then <tex>x \in  L</tex></li>
        <li>if <tex>p \in  L</tex> then exists <tex>q \in  L</tex> such that <tex>p&lt;q</tex></li></ul></p></mainmatter> </tree><p>
    Now we can defined the real number as a set of Dedekind cuts.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1440</anchor>  <taxon>Definition</taxon> <addr>def-0014</addr>  <route>def-0014.xml</route>   <title>Real Number System</title> </frontmatter> <mainmatter><p>
    The element of <tex>\mathbb {R}</tex> is a <link href="def-0013.xml" type="local" addr="def-0013" title="Dedekind cuts">Dedekind Cut</link> in <tex>\mathbb {Q}</tex>.
    <tex display="block">
         \mathbb {R} :=  \{  L | (L,R)  \text { is a Dedekind Cut}  \} 
    </tex></p></mainmatter> </tree><p>
    Now define the order relation on <tex>\mathbb {R}</tex>.
    We have defined <tex>\mathbb {R}</tex> as the set of Dedekind cuts, so we can define the strict partial order relation <tex>&lt;</tex> on <tex>\mathbb {R}</tex> by the set operation <tex>\subset</tex>.
    The irreflexive, asymmetric and transitive properties are trivial. 
</p></mainmatter> </tree><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1441</anchor>  <taxon>Linear Algebra</taxon> <addr>math-0001</addr>  <route>math-0001.xml</route> <date><year>2024</year> <month>1</month> <day>26</day></date>  <title>Vector Space</title> </frontmatter> <mainmatter><p>
    The motivation for the definition of a vector space comes from the properties
    of vectors in Euclidean space <tex>\mathbb {R}^n</tex> and <tex>\mathbb {C}^n</tex>.
    The definition abstracts and generalizes these properties.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1442</anchor>  <taxon>Definition</taxon> <addr>def-000H</addr>  <route>def-000H.xml</route>   <title>Vector Space</title> </frontmatter> <mainmatter><p>
    A vector space over a <link href="def-0006.xml" type="local" addr="def-0006" title="Field">field</link> <tex>F</tex> is a non-empty set <tex>V</tex> together with a binary operation and a binary function that satisfy the axioms listed below. 
    In this context, the elements of <tex>F</tex> are commonly called <strong>vectors</strong>, and the elements of <tex>F</tex> are called <strong>scalars</strong>.
    <ul><li>Commutativity: <tex>
             \forall  x, y  \in  V, x + y = y + x
        </tex></li>
        <li>Associativity: <tex>
             \forall  x, y, z  \in  V, (x + y) + z = x + (y + z)
        </tex></li>
        <li>Additive Identity: <tex>
             \exists  0  \in  V  \text { such that }  \forall  x  \in  V, x + 0 = x
        </tex></li>
        <li>Multiplicative Identity: <tex>
             \forall  x  \in  V, 1x = x
        </tex></li>
        <li>Additive Inverse: <tex>
             \forall  x  \in  V,  \exists  y  \in  V  \text { such that } x + y = 0
        </tex></li>
        <li>Distributivity: <tex>
             \forall  x, y  \in  V,  \forall  c, d  \in  F, c(x + y) = cx + cy, (c + d)x = cx + dx
        </tex></li></ul></p><p>
    Elements of a vector space are called <strong>vectors</strong> or <strong>points</strong>.
</p></mainmatter> </tree><p>
    When dealing with vector spaces, we usually interested only in subspaces.
    And the union of subspaces is rarely a subspace, thus
    we are more interested with sums of subspaces.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1443</anchor>  <taxon>Definition</taxon> <addr>def-000I</addr>  <route>def-000I.xml</route>   <title>Linear Subspace</title> </frontmatter> <mainmatter><p>
    A subset <tex>U</tex> of a vector space <tex>V</tex> over a field <tex>F</tex> is called a <strong>subspace</strong> of <tex>V</tex> if <tex>U</tex> is itself a <strong>vector space</strong> over <tex>F</tex> with the operations of addition and scalar multiplication on <tex>V</tex>.
    The subset also satisfies the following axioms (vice versa):
    <ul><li>Additive identity: <tex>0 \in  U</tex></li>
        <li>Closure: <tex>\forall  u,v \in  U, u+v \in  U</tex></li>
        <li>Closed Scalar multiplication: <tex>\forall  u \in  U,  \forall  c \in  F, cu \in  U</tex></li></ul></p></mainmatter> </tree><p>
    After that we can define the sum of subsets.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1444</anchor>  <taxon>Definition</taxon> <addr>def-000J</addr>  <route>def-000J.xml</route>   <title>Sum of subsets</title> </frontmatter> <mainmatter><p>
    Let <tex>U_1,  \dots , U_n</tex> be subsets of a vector space <tex>V</tex>.
    The <strong>sum</strong> of <tex>U_1,  \dots , U_n</tex> is defined as
    <tex display="block">U_1 +  \dots  + U_n =  \{ u_1 +  \dots  + u_n  \mid  u_i  \in  U_i \}</tex>.
</p></mainmatter> </tree><p>
    The sum of subspaces is the smallest subspace that contains all the subspaces.
</p><p>
    Every element in <tex>U_1 +  \dots  + U_n</tex> can be written as a sum of elements <tex>u_i</tex> in <tex>U_i</tex>:
    <tex display="block">
        u_1+ \cdots +u_n
    </tex>
    We will interested in cases where each vector in <tex>U_1 +  \dots  + U_n</tex> can be represented in the form above
    in only one way. This leads to the definition of direct sum.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1445</anchor>  <taxon>Definition</taxon> <addr>def-000K</addr>  <route>def-000K.xml</route>   <title>Direct Sum</title> </frontmatter> <mainmatter><p>
    Let <tex>U_1,  \dots , U_n</tex> be subspaces of a vector space <tex>V</tex>.
    The <strong>direct sum</strong> of <tex>U_1,  \dots , U_n</tex> is defined as
    <tex display="block">
        U_1  \oplus   \dots   \oplus  U_n =  \{ u_1 +  \dots  + u_n  \mid  u_i  \in  U_i \} 
    </tex>
    if every element in <tex>U_1  \oplus   \dots   \oplus  U_n</tex> can be written as <tex>u_1 +  \dots  + u_n </tex> in only one way.
    This definition requires every vector in the sum have a unique representation.
</p></mainmatter> </tree></mainmatter> </tree><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1446</anchor>  <taxon>Linear Algebra</taxon> <addr>math-0002</addr>  <route>math-0002.xml</route> <date><year>2024</year> <month>1</month> <day>26</day></date>  <title>Finite Dimensional Vector Space</title> </frontmatter> <mainmatter><p>
    Adding up scalar mulitples of vectors in a list gives a linear combination.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1447</anchor>  <taxon>Definition</taxon> <addr>def-000L</addr>  <route>def-000L.xml</route>   <title>Linear Combination</title> </frontmatter> <mainmatter><p>
    Let <tex>V</tex> be a <link href="def-000H.xml" type="local" addr="def-000H" title="Vector Space">vector space</link> over a field <tex>F</tex>.
    Let <tex>v_1,  \dots , v_n</tex> be vectors in <tex>V</tex>.
    A <strong>linear combination</strong> of <tex>v_1,  \dots , v_n</tex> is an expression of the form
    <tex display="block">
        a_1 v_1 +  \dots  + a_n v_n
    </tex>
    where <tex>a_1,  \dots , a_n  \in  F</tex>.
</p></mainmatter> </tree><p>
    To talk about a structure, we usually define a collection of this structure.
    Hence we have span for linear combinations.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1448</anchor>  <taxon>Definition</taxon> <addr>def-000M</addr>  <route>def-000M.xml</route>   <title>Linear Span</title> </frontmatter> <mainmatter><p>
    Let <tex>V</tex> be a vector space over a field <tex>F</tex>.
    Let <tex>v_1,  \dots , v_n</tex> be vectors in <tex>V</tex>.
    The <strong>span</strong> of <tex>v_1,  \dots , v_n</tex> is defined as
    <tex display="block">
         \text {span} (v_1,  \dots , v_n) =  \{ a_1 v_1 +  \dots  + a_n v_n  \mid  a_i  \in  F \} 
    </tex>
    The span of empty set is defined to be <tex>\{ 0 \}</tex>.    
</p><p>
    If <tex>\text {span} (v_1,  \dots , v_n) = V</tex>, we say that <tex>v_1,  \dots , v_n</tex> <strong>spans</strong> <tex>V</tex>.
</p></mainmatter> </tree><p>
    Suppose we have span <tex>S= \text {span} (v_1,  \dots , v_n)</tex>. (Span is trivially a subspace.)
    Obviously for all <tex>v_j (1  \leq  j  \leq  n)</tex>, <tex>v_j  \in  S</tex>.
    Because subspaces are closed under scalar multiplication and addition, every
    subspace of <tex>V</tex> containing <tex>v_1,  \dots , v_n</tex> must contain <tex>S</tex>.
    Thus we conclude that <tex>S</tex> is the smallest subspace containing <tex>v_1,  \dots , v_n</tex>.
</p><p>
    The discussion about <strong>spans</strong> leads to a key definition in linear algebra.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1449</anchor>  <taxon>Definition</taxon> <addr>def-000N</addr>  <route>def-000N.xml</route>   <title>Finite-Dimensional Vector Space</title> </frontmatter> <mainmatter><p>
    A <link href="def-000H.xml" type="local" addr="def-000H" title="Vector Space">vector space</link> <tex>V</tex> is called <strong>finite-dimensional</strong> if some <link href="def-000G.xml" type="local" addr="def-000G" title="List">list</link> of vectors <tex>v_1,  \dots , v_n</tex> <link href="def-000M.xml" type="local" addr="def-000M" title="Linear Span">spans</link> <tex>V</tex>.
</p></mainmatter> </tree><p>
    The opposite of finite-dimensional is infinite-dimensional.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1450</anchor>  <taxon>Definition</taxon> <addr>def-000O</addr>  <route>def-000O.xml</route>   <title>Infinite-dimensional vector space</title> </frontmatter> <mainmatter><p>
    A vector space <tex>V</tex> is called <strong>infinite-dimensional</strong> if it is not <link href="def-000N.xml" type="local" addr="def-000N" title="Finite-Dimensional Vector Space">finite-dimensional</link>.
</p></mainmatter> </tree><p>
    Consider the situation that there is only one way to
    express a vector <tex>v</tex> as a linear combination of vectors in a list <tex>v_1,  \dots , v_n</tex>.
    What property of the list <tex>v_1,  \dots , v_n</tex> does this situation imply? The answer is
    linear independence.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1451</anchor>  <taxon>Definition</taxon> <addr>def-000P</addr>  <route>def-000P.xml</route>   <title>Linearly independent</title> </frontmatter> <mainmatter><p>
    A set of vectors <tex>\{ v_1,  \dots , v_n \}</tex> is called <strong>linearly independent</strong> if
    <tex display="block">a_1 v_1 +  \dots  + a_n v_n = 0</tex>
    implies that <tex>a_1 =  \dots  = a_n = 0</tex>.
    The trivial case of <tex>\{ 0 \}</tex> is also considered linearly independent.
</p></mainmatter> </tree><p>
    If some vectors are not linearly independent, then there are more than one way to
    express a vector as a linear combination of vectors in the list. This leads to 
    the following definition.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1452</anchor>  <taxon>Definition</taxon> <addr>def-000Q</addr>  <route>def-000Q.xml</route>   <title>Linearly dependent</title> </frontmatter> <mainmatter><p>
    A set of vectors <tex>\{ v_1,  \dots , v_n \}</tex> is called <strong>linearly dependent</strong> if
    <tex display="block">a_1 v_1 +  \dots  + a_n v_n = 0</tex>
    for some <tex>a_1,  \dots , a_n  \in   \mathbb {F}</tex> with at least one <tex>a_i  \neq  0</tex> (not all <tex>0</tex>).
</p></mainmatter> </tree><p>
    The following lemma is a direct consequence of the definition of linear independence.
    It states that for a given linearly dependent list, we can always remove a vector
    without changing the span.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1453</anchor>  <taxon>Lemma</taxon> <addr>thm-0001</addr>  <route>thm-0001.xml</route>   <title>Linear Dependence Lemma</title> </frontmatter> <mainmatter><p>
    Let <tex>v_1,  \dots , v_n</tex> be vectors in a vector space <tex>V</tex> over a field <tex>\mathbb {F}</tex>.
    If <tex>v_1,  \dots , v_n</tex> are linearly dependent, then there exists <tex>1  \leq  i  \leq  n</tex> such that
    <ul><li><tex>v_i  \in   \text {span} (v_1,  \dots , v_{i-1})</tex></li>
        <li>Remove <tex>v_i</tex> from the list <tex>v_1,  \dots , v_n</tex> and the span does not change</li></ul></p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1454</anchor>  <taxon>Lemma</taxon> <addr>thm-0002</addr>  <route>thm-0002.xml</route>   <title>Length of linearly independent list <tex>\leq</tex> length of spanning list</title> </frontmatter> <mainmatter><p>
    In a finite dimensional vector space, the length of a linearly independent list is less than or equal to the length of a spanning list.
</p></mainmatter> </tree><p>
    We have discussed linear independent lists and spanning lists.
    Now we are ready to define a basis.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1455</anchor>  <taxon>Definition</taxon> <addr>def-000R</addr>  <route>def-000R.xml</route>   <title>Basis</title> </frontmatter> <mainmatter><p>
    A basis of <tex>V</tex> is a list of vectors in <tex>V</tex>
    that is linearly independent and spans <tex>V</tex>. 
</p><p><strong>Criterion for basis</strong>
    A list of vectors <tex>\{ v_1,  \dots , v_n \}</tex> is a basis of <tex>V</tex> if and only if
    every <tex>v  \in  V</tex> can be written <strong>uniquely</strong> as a linear combination of <tex>v_1,  \dots , v_n</tex>.
</p></mainmatter> </tree><p>
    For instance, we have standard basis <tex>\{ e_1,  \dots , e_n \}</tex> for <tex>\mathbb {F}^n</tex>,
    where <tex>e_i</tex> is the vector with <tex>1</tex> at <tex>i</tex>-th position and <tex>0</tex> elsewhere.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1456</anchor>  <taxon>Theorem</taxon> <addr>thm-0005</addr>  <route>thm-0005.xml</route>   <title>Spanning List contains a basis</title> </frontmatter> <mainmatter><p>
    Every spanning list in a vector space can be reduced to a basis.
</p></mainmatter> </tree><p>
    From the <link href="thm-0005.xml" type="local" addr="thm-0005" title="Spanning List contains a basis">theorem</link> we can infer a corollary.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1457</anchor>  <taxon>Corollary</taxon> <addr>thm-0006</addr>  <route>thm-0006.xml</route>   <title>Basis of finite-dimensional vector space</title> </frontmatter> <mainmatter><p>
    Every finite-dimensional vector space has a basis.
</p></mainmatter> </tree><p>
    The next result states for a spanning list can be reduced to a basis.
    We can adjoin one or more vectors to a linearly independent list to form a basis.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1458</anchor>  <taxon>Theorem</taxon> <addr>thm-0007</addr>  <route>thm-0007.xml</route>   <title>Linearly dependent list extends to a basis</title> </frontmatter> <mainmatter><p>
    Every linearly independent list of vectors in  a finite-dimensional vector space can be extended to a basis.
</p></mainmatter> </tree><p>
    Remind the definition of <link href="der-000K" type="external">direct sum</link>, we can now show that
    every subspace of a finite-dimensional vecrtor space can be paired
    with another subspace to form a direct sum of the whole space.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1459</anchor>  <taxon>Theorem</taxon> <addr>thm-0008</addr>  <route>thm-0008.xml</route>   <title>Direct Sum of Subspaces of <tex>V</tex></title> </frontmatter> <mainmatter><p>
    Suppose <tex>V</tex> is a finite dimensional vector space,
    and <tex>U</tex> is a subspace of <tex>V</tex>.
    Then there exists a subspace <tex>W</tex> of <tex>V</tex> such that
    <tex>V = U  \oplus  W</tex>.
</p></mainmatter> </tree><p>
    This post discusses about <em>finite-dimensional vector space</em>.
    But we have not yet defined what is dimension.
    We tempted to define the dimension as the length of basis intuitively.
    With this definition we should prove its well-definedness.
    That is, every basis has the same length.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1460</anchor>  <taxon>Theorem</taxon> <addr>thm-0009</addr>  <route>thm-0009.xml</route>   <title>Basis length is invariant</title> </frontmatter> <mainmatter><p>
    Let <tex>V</tex> be a finite-dimensional vector space.
    Then every basis of <tex>V</tex> has the same length.
</p></mainmatter> </tree><p>
    This can be proved by <link href="thm-0002.xml" type="local" addr="thm-0002" title="Length of linearly independent list  length of spanning list">Lemma 8</link>.
    Now we can formally define the dimension of such spaces.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1461</anchor>  <taxon>Definition</taxon> <addr>def-001V</addr>  <route>def-001V.xml</route>   <title>Dimension</title> </frontmatter> <mainmatter><p>
    The <strong>dimension</strong> of a finite-dimensional vector space <tex>V</tex> is the length of any basis of the vector space.
    Denoted by <tex>\dim  V</tex>.
</p></mainmatter> </tree><p>
    Every subspace of a finite-dimensional vector space is also finite-dimensional.
    Hence we can talk about the dimension of a subspace.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1462</anchor>  <taxon>Theorem</taxon> <addr>thm-000A</addr>  <route>thm-000A.xml</route>   <title>Dimension of a subspace</title> </frontmatter> <mainmatter><p>
    Let <tex>V</tex> be a finite-dimensional vector space,
    and <tex>U</tex> be a subspace of <tex>V</tex>.
    Then <tex>\dim  U  \leq   \dim  V</tex>.
</p></mainmatter> </tree><p>
    According to the definition of <link href="def-000P.xml" type="local" addr="def-000P" title="Linearly independent">linearly independent</link>,
    to show a list of vectors is a basis, we only need to show it is linearly independent,
    and it spans the whole space.
    The next theorems simplifies the task:
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1463</anchor>  <taxon>Theorem</taxon> <addr>thm-000B</addr>  <route>thm-000B.xml</route>   <title>Linearly independent list of the right length is a basis</title> </frontmatter> <mainmatter><p>
    Let <tex>V</tex> be a finite-dimensional vector space.
    Then every <link href="def-000P.xml" type="local" addr="def-000P" title="Linearly independent">linearly independent</link> list of vectors in <tex>V</tex> with length equal to <tex>\dim  V</tex> is a basis of <tex>V</tex>.
</p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1464</anchor>  <taxon>Theorem</taxon> <addr>thm-000C</addr>  <route>thm-000C.xml</route>   <title>Spanning list of the right length is a basis</title> </frontmatter> <mainmatter><p>
    Let <tex>V</tex> be a finite-dimensional vector space.
    Then every <link href="def-000M.xml" type="local" addr="def-000M" title="Linear Span">spanning</link> list of vectors in <tex>V</tex> with length equal to <tex>\dim  V</tex> is a basis of <tex>V</tex>.
</p></mainmatter> </tree><p>
    Now we move to the discussion of the dimension of the sum of two subspaces.
    This is analogous to the <link href="thm-000E.xml" type="local" addr="thm-000E" title="Inclusion-Exclusion Principle">inclusion-exclusion principle</link>.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1465</anchor>  <taxon>Theorem</taxon> <addr>thm-000D</addr>  <route>thm-000D.xml</route>   <title>Dimension of a sum</title> </frontmatter> <mainmatter><p>
    Let <tex>V</tex> be a finite-dimensional vector space,
    and <tex>U</tex> and <tex>W</tex> be subspaces of <tex>V</tex>.
    Then
    <tex display="block">
         \dim (U + W) =  \dim  U +  \dim  W -  \dim (U  \cap  W).
    </tex></p></mainmatter> </tree></mainmatter> </tree><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1466</anchor>  <taxon>Linear Algebra</taxon> <addr>math-0005</addr>  <route>math-0005.xml</route> <date><year>2024</year> <month>1</month> <day>31</day></date>  <title>Linear Maps</title> </frontmatter> <mainmatter><p>
    Now we arrive at the main topic of this chapter: linear maps. 
    In classic mathematics, to understand the properties of the structure or space,
    we often study the maps between them.
    For vector spaces we study the <strong>linear maps</strong>.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1467</anchor>  <taxon>Definition</taxon> <addr>def-0025</addr>  <route>def-0025.xml</route>   <title>Linear Map</title> </frontmatter> <mainmatter><p>
    A <strong>linear map</strong> is a function between two vector spaces that preserves the operations of addition and scalar multiplication.
    In other words, a function <tex>T: V  \to  W</tex> where <tex>V,W</tex> are vector spaces if the following conditions are satisfied:
    <ul><li>Additivity: <tex>T(u+v) = T(u) + T(v)</tex> for all <tex>u,v  \in  V</tex></li>
        <li>Homogeneity: <tex>T( \alpha  v) =  \alpha  T(v)</tex> for all <tex>\alpha   \in   \mathbb {F}</tex> and <tex>v  \in  V</tex></li></ul>
    Sometimes we ignore the brackets and write <tex>T v</tex> instead of <tex>T(v)</tex>.
</p></mainmatter> </tree><p>
    Now we can talk about the set of all linear maps between two vector spaces.
    <tex display="block">
         \mathcal {L} (V,W) =  \{    T: V  \to  W | T  \text { is a linear map}   \} 
    </tex></p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1468</anchor>  <taxon>Example</taxon> <addr>eg-0002</addr>  <route>eg-0002.xml</route>   <title>Differentiation is linear map</title> </frontmatter> <mainmatter><p>
    Define <tex>D \in \mathcal {L} ( \mathcal {P}( \mathbb {R} ), \mathcal {P}( \mathbb {R} ))</tex> (recall that <tex>\mathcal {P}</tex> means <link href="def-0027.xml" type="local" addr="def-0027" title="Polynomial">set of polynomials</link>) by
    <tex display="block">
        D(f) = f'
    </tex>
    We can see that <tex>D</tex> a linear map.
    <ul><li>Additivity: <tex>D(f+g) = (f+g)' = f' + g' = D(f) + D(g)</tex></li>
        <li>Homogeneity: <tex>D( \alpha  f) = ( \alpha  f)' =  \alpha  f' =  \alpha  D(f)</tex></li></ul></p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1469</anchor>  <taxon>Definition</taxon> <addr>eg-0003</addr>  <route>eg-0003.xml</route>   <title>Integration is linear map</title> </frontmatter> <mainmatter><p>
    Let <tex>V</tex> be the vector space of all continuous functions on the interval <tex>[a,b]</tex>.
    The map <tex>I: V  \to  V</tex> defined by
    <tex display="block">
        I(f) =  \int _a^x f(t) dt
    </tex>
    is a <strong>linear map</strong>.
    In other words, <tex>I</tex> preserves the operations of addition and scalar multiplication:
    For all <tex>f,g  \in  V</tex> and all <tex>\alpha   \in   \mathbb {R}</tex>,
    <tex display="block">
        I(f+g) = I(f) + I(g)  \quad   \text {and}  \quad  I( \alpha  f) =  \alpha  I(f)
    </tex></p></mainmatter> </tree><p>
    We can find a linear map that takes on <em>whatever values we wish</em> on the 
    vectors in a basis by the following theorem.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1470</anchor>  <taxon>Theorem</taxon> <addr>thm-000F</addr>  <route>thm-000F.xml</route>   <title>Linear maps and basis of domain</title> </frontmatter> <mainmatter><p>
    Let <tex>v_1, v_2,  \ldots , v_n</tex> be a basis of vector space <tex>V</tex>.
    Then for any vector space <tex>W</tex> and any vectors <tex>w_1, w_2,  \ldots , w_n</tex> in <tex>W</tex>,
    there exists a unique linear map <tex>T: V  \to  W</tex> such that
    <tex display="block">
        T(v_i) = w_i  \quad   \text {for all}  \quad  i = 1,2, \ldots ,n
    </tex></p></mainmatter> </tree><p>
    Now let's turn to the algebraic operations over the set of linear maps <tex>\mathcal {L} (V,W)</tex>.
    We begin by defining the addition and scalar multiplication of linear maps.
    This leads to a surprising result: the set of linear maps is actually a vector space.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1471</anchor>  <taxon>Definition</taxon> <addr>def-0029</addr>  <route>def-0029.xml</route>   <title>Addition and scalar multiplication over <tex>\mathcal {L} (V,W)</tex></title> </frontmatter> <mainmatter><p>
    Let <tex>T_1, T_2  \in   \mathcal {L} (V,W)</tex>.
    We define the <strong>addition</strong> of <tex>T_1</tex> and <tex>T_2</tex> as the linear map <tex>T_1 + T_2: V  \to  W</tex> such that
    <tex display="block">
        (T_1 + T_2)(v) = T_1(v) + T_2(v)  \quad   \text {for all}  \quad  v  \in  V
    </tex>
    The scalar multiplication of a linear map <tex>T  \in   \mathcal {L} (V,W)</tex> by a scalar <tex>c  \in   \mathbb {F}</tex> is the linear map <tex>cT: V  \to  W</tex> such that
    <tex display="block">
        (cT)(v) = cT(v)  \quad   \text {for all}  \quad  v  \in  V
    </tex>
    With these operations, <tex>\mathcal {L} (V,W)</tex> is a <link href="def-000H.xml" type="local" addr="def-000H" title="Vector Space"><strong>vector space</strong></link> over the field <tex>\mathbb {F}</tex>.
    Note that the additive identity of <tex>\mathcal {L} (V,W)</tex> is the <strong>zero map</strong> <tex>0: V  \to  W</tex> such that
    <tex display="block">
        0(v) = 0  \quad   \text {for all}  \quad  v  \in  V
    </tex></p></mainmatter> </tree><p>
    Usually it makes no sense to multiply two linear maps. But we can define
    an operation called the <strong>product</strong> of linear maps, which is just the composition of the two functions.
    This can form a <strong>monoid</strong> or even a <strong>group</strong> under certain conditions.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1472</anchor>  <taxon>Definition</taxon> <addr>def-002A</addr>  <route>def-002A.xml</route>   <title>Product of Linear Maps</title> </frontmatter> <mainmatter><p>
    Let <tex>T_1: V  \to  W</tex> and <tex>T_2: W  \to  U</tex> be linear maps.
    We define the <strong>product</strong> of <tex>T_1</tex> and <tex>T_2</tex> as the linear map <tex>T_2  \circ  T_1: V  \to  U</tex> such that
    <tex display="block">
        (T_2  \circ  T_1)(v) = T_2(T_1(v))  \quad   \text {for all}  \quad  v  \in  V
    </tex>
    Note that this is just the composition of the two functions <tex>T_1</tex> and <tex>T_2</tex>. 
    And we usually denote <tex>T_2  \circ  T_1</tex> by <tex>T_2T_1</tex>.
    The product of linear maps is associative, that is,
    <tex display="block">
        (T_3  \circ  T_2)  \circ  T_1 = T_3  \circ  (T_2  \circ  T_1)
    </tex>
    for any linear maps <tex>T_1: V  \to  W</tex>, <tex>T_2: W  \to  U</tex>, and <tex>T_3: U  \to  X</tex>.
    The identity map <tex>I_V: V  \to  V</tex> is the identity element of the set of linear maps <tex>\mathcal {L} (V,V)</tex> under the product operation.
    That is, for any linear map <tex>T: V  \to  V</tex>,
    <tex display="block">
        I_V  \circ  T = T  \circ  I_V = T
    </tex>
    where <tex>I_V</tex> is the identity map on <tex>V</tex>.
    The set of all linear maps from a vector space to itself, <tex>\mathcal {L} (V,V)</tex>, forms a <link href="def-0007.xml" type="local" addr="def-0007" title="Monoid"><strong>monoid</strong></link> under the product operation.
    The set of all invertible linear maps from a vector space to itself, <tex>\mathcal {L} (V,V)^*</tex>, forms a group under the product operation.
    The identity map is the identity element of the <link href="def-0001.xml" type="local" addr="def-0001" title="Group"><strong>group</strong></link> <tex>\mathcal {L} (V,V)^*</tex>.
</p><p>
    With addition we also have the distributive law for the product of linear maps.
    That is, for any linear maps <tex>S,S_1,S_2: V  \to  W</tex> and <tex>T,T_1,T_2: U \to  V</tex>:
    <tex display="block">
        (S_1 + S_2)T = S_1T + S_2T  \quad   \text {and}  \quad  T(S_1 + S_2) = TS_1 + TS_2
    </tex></p></mainmatter> </tree><p>
    In algebra, we have a structure named <strong>kernel</strong>, which is the set of all elements that are mapped to the zero element.
    For linear maps, the kernel is the <strong>null space</strong></p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1473</anchor>  <taxon>Definition</taxon> <addr>def-002C</addr>  <route>def-002C.xml</route>   <title>Null Space</title> </frontmatter> <mainmatter><p>
    For <tex>T: V  \to  W</tex>, the <strong>null space</strong> of <tex>T</tex> is the set of all vectors in <tex>V</tex> that are mapped to <tex>0</tex> in <tex>W</tex>.
    <tex display="block">
         \text {null }  T =  \{   v  \in  V | T(v) = 0   \} 
    </tex>
    The null space of <tex>T</tex> is a subspace of <tex>V</tex>.
</p></mainmatter> </tree><p>
    The injective linear map is defined like normal <link href="def-002D.xml" type="local" addr="def-002D" title="Injective">injective</link> functions.
    To check whether a linear map is injective, we can just check whether the null space is trivial.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1474</anchor>  <taxon>Theorem</taxon> <addr>thm-000G</addr>  <route>thm-000G.xml</route>   <title>Injectivity equivalent to Kernel Triviality</title> </frontmatter> <mainmatter><p>
    Let <tex>T: V  \to  W</tex> be a linear map. Then <tex>T</tex> is injective if and only if <tex>\text {null }  T =  \{   0   \}</tex>.
</p></mainmatter> </tree><p>
    The image of a linear map is the set of all elements that are mapped to by some element in the domain.
    This is called the <strong>range</strong> of the linear map, just like <link href="def-002E.xml" type="local" addr="def-002E" title="Range">range</link> of normal function.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1475</anchor>  <taxon>Theorem</taxon> <addr>thm-000H</addr>  <route>thm-000H.xml</route>   <title>Range is a subspace</title> </frontmatter> <mainmatter><p>
    If <tex>T: V  \to  W</tex> is a linear map, then the range of <tex>T</tex> is a subspace of <tex>W</tex>.
</p></mainmatter> </tree><p>
    The next theorem plays a crucial role in the study of linear maps.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1476</anchor>  <taxon>Theorem</taxon> <addr>thm-000I</addr>  <route>thm-000I.xml</route>   <title>Fundamental Theorems of Linear Maps</title> </frontmatter> <mainmatter><p>
    Let <tex>V</tex> be finite-dimensional vector space and <tex>T : V  \to  W</tex> be a linear map. 
    Then <tex>\text {range }  T</tex> is finite-dimensional and 
    <tex display="block">
         \dim  V =  \dim   \text {range }  T +  \dim   \text {null }  T
    </tex></p></mainmatter> </tree><p>
    Now we can show that no linear map from a finite-dimensional vector space
    to a <em>smaller</em> (In dimension) vector space can be <link href="def-002D.xml" type="local" addr="def-002D" title="Injective">injective</link>.
    This can be easily proved by the fundamental theorem of linear maps.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1477</anchor>  <taxon>Lemma</taxon> <addr>thm-000M</addr>  <route>thm-000M.xml</route>   <title>Map to smaller dimension is not injective</title> </frontmatter> <mainmatter><p>
    Let <tex>V</tex> and <tex>W</tex> be finite-dimensional vector spaces, 
    and <tex>\dim  V &gt;  \dim  W</tex>.
    Then no linear map <tex>T:V \to  W</tex> is injective.
</p></mainmatter> </tree><p>
    Similarly, we can show that no linear map from a finite-dimensional vector space
    to a <em>larger</em> (In dimension) vector space can be <link href="def-002F.xml" type="local" addr="def-002F" title="Surjective">surjective</link>.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1478</anchor>  <taxon>Lemma</taxon> <addr>thm-000N</addr>  <route>thm-000N.xml</route>   <title>Map to bigger dimension is not surjective</title> </frontmatter> <mainmatter><p>
    Let <tex>V</tex> and <tex>W</tex> be finite-dimensional vector spaces, 
    and <tex>\dim  V &lt;  \dim  W</tex>.
    Then no linear map <tex>T:V \to  W</tex> is surjective.
</p></mainmatter> </tree><p>
    These two lemmas are very important in the study of linear equations.
    The idea here is to express linear equations system in terms of linear maps.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1479</anchor>  <taxon>Example</taxon> <addr>eg-0004</addr>  <route>eg-0004.xml</route>   <title>Homogeneous Linear Equations System</title> </frontmatter> <mainmatter><block open="open"><headline>
    <strong>Reprase in terms of a linear map the question of whether a <link href="def-002Q.xml" type="local" addr="def-002Q" title="Homogeneous Linear Equations">homogeneous system linear equations</link> has a nonzero solution.</strong>
</headline> 
    Let <tex>A</tex> be the coefficient matrix of a homogeneous linear system.
    <tex display="block">
        A =  \begin {bmatrix}
            a_{11} &amp; a_{12} &amp;  \cdots  &amp; a_{1n}  \\ 
            a_{21} &amp; a_{22} &amp;  \cdots  &amp; a_{2n}  \\ 
             \vdots  &amp;  \vdots  &amp;  \ddots  &amp;  \vdots   \\ 
            a_{m1} &amp; a_{m2} &amp;  \cdots  &amp; a_{mn}
         \end {bmatrix}
    </tex>
    The equation <tex>A \vec {x} =  \vec {0}</tex> has a trivial solution <tex>\vec {x} =  \vec {0}</tex>.
    The question here is whether there is a nontrivial solution.
    <p>
        Define <tex>T:  \mathbb {F} ^n  \to   \mathbb {F} ^m</tex> by
        <tex display="block">
            T( \vec {x}) = A \vec {x}
        </tex>
        Then the question of whether the homogeneous linear system has a nontrivial solution is equivalent to 
        asking <tex>\text {null }  T</tex> is nontrivial.
        That is, <tex>T</tex> is <link href="thm-000G.xml" type="local" addr="thm-000G" title="Injectivity equivalent to Kernel Triviality">not injective</link>.
    </p>
</block></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1480</anchor>  <taxon>Theorem</taxon> <addr>thm-000O</addr>  <route>thm-000O.xml</route>   <title>Homogeneous system of linear equations</title> </frontmatter> <mainmatter><p> 
    A homogeneous system of linear equations
    with more variables than equations has 
    a nontrivial solution.
</p></mainmatter> </tree><p>
    We have seen that <link href="thm-000M.xml" type="local" addr="thm-000M" title="Map to smaller dimension is not injective">map to smaller dimension is not injective</link>.
    <tex>T</tex> is not injective if <tex>n &gt; m</tex>. This results the theorem above.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1481</anchor>  <taxon>Example</taxon> <addr>eg-0005</addr>  <route>eg-0005.xml</route>   <title>Inhomogeneous Linear Equations System</title> </frontmatter> <mainmatter><block open="open"><headline>
    <strong>Rephrase in terms of a linear map the question of whether a inhomogeneous system linear equations has no solutions
    for some choice of constant terms.</strong>
</headline> 
    Let <tex>A</tex> be the coefficient matrix of a inhomogeneous linear system.
    <tex display="block">
        A =  \begin {bmatrix}
            a_{11} &amp; a_{12} &amp;  \cdots  &amp; a_{1n}  \\ 
            a_{21} &amp; a_{22} &amp;  \cdots  &amp; a_{2n}  \\ 
             \vdots  &amp;  \vdots  &amp;  \ddots  &amp;  \vdots   \\ 
            a_{m1} &amp; a_{m2} &amp;  \cdots  &amp; a_{mn}
         \end {bmatrix}
    </tex>
    The equation <tex>A \vec {x} =  \vec {b}</tex> has a solution <tex>\vec {x} = A^{-1} \vec {b}</tex>.
    <p>
        Define <tex>T:  \mathbb {F} ^n  \to   \mathbb {F} ^m</tex> by
        <tex display="block">
            T( \vec {x}) = A \vec {x}
        </tex>
        Then the statement that inhomogeneous linear system has no solutions is equivalent to 
        <tex>\vec {b}  \not \in   \text {range }  T</tex>.
        Thus the question is rephrased as not having a solution for some choice of <tex>\vec {b}</tex>.
        What condition ensures <tex>T</tex> is not surjective.
    </p>
</block></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1482</anchor>  <taxon>Theorem</taxon> <addr>thm-000P</addr>  <route>thm-000P.xml</route>   <title>Inhomogeneous system of linear equations</title> </frontmatter> <mainmatter><p>
    An inhomogeneous system of linear equations
    with more equations than variables has 
    no solution for some choice of the constant term.
</p></mainmatter> </tree><p>
    Let <tex>v_1, v_2,  \cdots , v_n</tex> be a basis of <tex>V</tex>.
    We know that for any value of a linear map <tex>T:V \to  W</tex>,
    can be determined by values <tex>\{   T(v_1), T(v_2),  \cdots , T(v_n)   \}</tex>.
    This leads to the definition of the matrix representation of a linear map.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1483</anchor>  <taxon>Definition</taxon> <addr>def-002R</addr>  <route>def-002R.xml</route>   <title>Matrix</title> </frontmatter> <mainmatter><p>
    Let <tex>m,n \in   \mathbb {Z} ^+</tex>.
    A <tex>m \times  n</tex> matrix is a rectangular array of elements of a field <tex>\mathbb {F}</tex>
    with <tex>m</tex> <strong>rows</strong> and <tex>n</tex> <strong>columns</strong>.
    <tex display="block">
        A =  \begin {bmatrix}
            a_{11} &amp; a_{12} &amp;  \cdots  &amp; a_{1n}  \\ 
            a_{21} &amp; a_{22} &amp;  \cdots  &amp; a_{2n}  \\ 
             \vdots  &amp;  \vdots  &amp;  \ddots  &amp;  \vdots   \\ 
            a_{m1} &amp; a_{m2} &amp;  \cdots  &amp; a_{mn}
         \end {bmatrix}
    </tex>
    The notation <tex>A_{jk}</tex> refers to the element in the <tex>j</tex>-th row and <tex>k</tex>-th column.
</p></mainmatter> </tree><p>
    Now we can define the matrix representation of a linear map.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1484</anchor>  <taxon>Definition</taxon> <addr>def-002S</addr>  <route>def-002S.xml</route>   <title>Matrix of Linear Maps</title> </frontmatter> <mainmatter><p>
    Let <tex>T \in   \mathcal {L} (V,W)</tex>,
    <tex>\{   v_1, \ldots ,v_n   \} \subset  V</tex> be a basis of <tex>V</tex>,
    and <tex>\{   w_1, \ldots ,w_m   \} \subset  W</tex> be a basis of <tex>W</tex>.
    The <strong>matrix of <tex>T</tex></strong> with respect to these bases is
    the <tex>m \times  n</tex> matrix <tex>\mathcal {M} (T)</tex> such that
    <tex display="block">
        T(v_j) =  \sum _{i=1}^m  \mathcal {M} (T)_{ij}w_i
    </tex>
    Or we denote <tex>\mathcal {M} (T)</tex> as <tex>\mathcal {M} (T, (v_1, \ldots ,v_n), (w_1, \ldots ,w_m))</tex>.
</p><p>
    If <tex>T</tex> maps <tex>n</tex>-dimensional vector space to <tex>m</tex>-dimensional vector space,
    then <tex>\mathcal {M} (T)</tex> is a <tex>m \times  n</tex> matrix.
</p><block open="open"><headline>
    <strong>Addition</strong>
</headline> 
    For two same-size matrix <tex>A,B</tex>,
    the sum of <tex>A</tex> and <tex>B</tex> is the matrix <tex>C</tex> such that
    <tex display="block">
        C_{ij} = A_{ij} + B_{ij}
    </tex>
    In the language of linear maps <tex>S,T \in   \mathcal {L} (V,W)</tex>,
    <tex display="block">
         \mathcal {M} (T+S) =  \mathcal {M} (T) +  \mathcal {M} (S)
    </tex>
</block><block open="open"><headline>
    <strong>Scalar Multiplication</strong>
</headline> 
    For a scalar <tex>c</tex> and a matrix <tex>A</tex>,
    the product of <tex>c</tex> and <tex>A</tex> is the matrix <tex>B</tex> such that
    <tex display="block">
        B_{ij} = cA_{ij}
    </tex>
    In the language of linear maps <tex>T \in   \mathcal {L} (V,W)</tex>,
    <tex display="block">
         \mathcal {M} (cT) = c \mathcal {M} (T)
    </tex>
</block><block open="open"><headline>
    <strong>Set of Matrices</strong>
</headline> 
    The set of all <tex>m \times  n</tex> matrices with elements in <tex>\mathbb {F}</tex> is denoted as <tex>\mathcal {M} _{m \times  n}( \mathbb {F} )</tex>
    or <tex>\mathbb {F} ^{m \times  n}</tex>.
</block></mainmatter> </tree><p>
    We can see that <tex>\mathbb {F} ^{m \times  n}</tex> is itself a vector space.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1485</anchor>  <taxon>Theorem</taxon> <addr>thm-000Q</addr>  <route>thm-000Q.xml</route>   <title><tex>\dim \mathbb {F} ^{m \times  n} = mn</tex></title> </frontmatter> <mainmatter><p><tex>\mathbb {F} ^{m \times  n}</tex> is a vector space with dimension <tex>mn</tex>.
</p></mainmatter> </tree><p>
    Consider linear maps <tex>T:U \to  V</tex> and <tex>S:V \to  W</tex>.
    The composition of linear maps is <tex>ST</tex>.
    Does the composition of linear maps have a matrix representation?
    <tex display="block">
         \mathcal {M} (ST) =  \mathcal {M} (S) \mathcal {M} (T)
    </tex>
    This makes no sense now but indicates the definition of <strong>matrix multiplication</strong>.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1486</anchor>  <taxon>Definition</taxon> <addr>def-002T</addr>  <route>def-002T.xml</route>   <title>Matrix Multiplication</title> </frontmatter> <mainmatter><p>
    Let <tex>A</tex> be a <tex>m \times  n</tex> matrix and <tex>B</tex> be a <tex>n \times  p</tex> matrix.
    Then <tex>AC</tex> is defined as the <tex>m \times  p</tex> matrix <tex>C</tex> such that
    <tex display="block">
        C_{ij} =  \sum _{k=1}^n A_{ik}B_{kj}
    </tex></p><block open="open"><headline>
    <strong>Derivation</strong>
</headline> 
    Let <tex>T:U \to  V</tex> and <tex>S:V \to  W</tex> be linear maps.
    Denote <tex>A =  \mathcal {M} (S)</tex> and <tex>C =  \mathcal {M} (T)</tex>.
    Then the composition of linear maps <tex>ST</tex> is computed
    <tex display="block">
         \begin {align*}
            (ST)(u)_k &amp;= S( \sum _{r=1}^n C_{rk}v_r)  \\ 
            &amp;=  \sum _{r=1}^n C_{rk}S(v_r)  \\ 
            &amp;=  \sum _{r=1}^n C_{rk} \sum _{s=1}^m A _{sr}w_s  \\ 
            &amp;=  \sum _{s=1}^m \left ( \sum _{r=1}^n C_{rk}A_{sr} \right )w_s  \\ 
         \end {align*}
    </tex>
    Thus <tex>\mathcal {M} (ST)</tex> is the <tex>m \times  p</tex> whose entries are
    <tex display="block">
         \mathcal {M} (ST)_{sk} =  \sum _{r=1}^n A_{sr}C_{rk}
    </tex>
</block></mainmatter> </tree><p>
    Now we see that the desired matrix multiplication holds.
    Matrix multiplication is not commutative in general.
    However, it satisfies the associative law and the distributive law.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1487</anchor>  <taxon>Definition</taxon> <addr>def-002Y</addr>  <route>def-002Y.xml</route>   <title><tex>A_{j \cdot }</tex> and <tex>A_{ \cdot  j}</tex></title> </frontmatter> <mainmatter><p>
    Let <tex>A</tex> be a <tex>m \times  n</tex> matrix.
    <ul><li>
            If <tex>1 \leq  j \leq  m</tex> then <tex>A_{j \cdot }</tex> is the <tex>j</tex>-th row of <tex>A</tex>,
            defined as a <tex>1 \times  n</tex> matrix. (A row vector)
        </li>
        <li>
            If <tex>1 \leq  j \leq  n</tex> then <tex>A_{ \cdot  j}</tex> is the <tex>j</tex>-th column of <tex>A</tex>,
            defined as a <tex>m \times  1</tex> matrix. (A column vector)
        </li></ul></p></mainmatter> </tree><p>
    With the notation we can think of matrix multiplication in another perspective.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1488</anchor>  <taxon>Lemma</taxon> <addr>thm-000R</addr>  <route>thm-000R.xml</route>   <title>Entry pf matrix product</title> </frontmatter> <mainmatter><p>
    Suppose <tex>A</tex> is an <tex>m \times  n</tex> matrix and <tex>B</tex> is an <tex>n \times  p</tex> matrix.
    Then the entry of the product <tex>AB</tex> is:
    <tex display="block">
        (AB)_{ij} = A_{i \cdot }B_{ \cdot  j}
    </tex>
    for <tex>1 \leq  i \leq  m</tex> and <tex>1 \leq  j \leq  p</tex>.
</p></mainmatter> </tree><p>
    We have an interesting observation.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1489</anchor>  <taxon>Lemma</taxon> <addr>thm-000S</addr>  <route>thm-000S.xml</route>   <title>Linear Combination of columns</title> </frontmatter> <mainmatter><p>
    Let <tex>A</tex> be an <tex>m \times  n</tex> matrix,
    and <tex>c</tex> is a <tex>1 \times  1</tex> matrix.
    <tex display="block">
        c =  \begin {pmatrix} c_1  \\  c_2  \\   \vdots   \\  c_n  \end {pmatrix}
    </tex>
    Then <tex>Ac = c_1A_{ \cdot  1} + c_2A_{ \cdot  2} +  \cdots  + c_nA_{ \cdot  n}</tex>.
    In other words, <tex>Ac</tex> is a linear Combination of the columns of <tex>A</tex>,
    with the scalars that multiply the columns coming from <tex>c</tex>.
</p></mainmatter> </tree><p>
    Now we begin the study the invertibility of linear maps.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1490</anchor>  <taxon>Definition</taxon> <addr>def-002Z</addr>  <route>def-002Z.xml</route>   <title>Inverse</title> </frontmatter> <mainmatter><p>
    A linear map <tex>T \in \mathcal {L} (V,W)</tex> is said to be <tex>invertible</tex> if 
    there exists a linear map <tex>S \in \mathcal {L} (W,V)</tex> such that:
    <tex display="block">
         \begin {align*}
            T \cdot  S &amp;=  \text {id} _V  \\ 
            S \cdot  T &amp;=  \text {id} _W
         \end {align*}
    </tex>
    where <tex>\text {id}</tex> is the identity map.
    If a linear map <tex>T</tex> is invertible, 
    then the map <tex>S</tex> is <strong>unique</strong> and is called the <strong>inverse</strong> of <tex>T</tex>, denoted <tex>T^{-1}</tex>.
</p><p>
    An <strong>isomorphism</strong> is a linear map that is invertible.
    Two vector spaces are said to be <strong>isomorphic</strong> if there exists an isomorphism between them.
</p></mainmatter> </tree><p>
    A linear map is invertible if and only if
    it is <strong>bijective</strong>.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1491</anchor>  <taxon>Theorem</taxon> <addr>thm-000T</addr>  <route>thm-000T.xml</route>   <title>Isomorphism of equal dimensions</title> </frontmatter> <mainmatter><p>
    Two finite-dimensional vector spaces over <tex>\mathbb {F}</tex>
    are isomorphic iff they have the same <link href="def-001V.xml" type="local" addr="def-001V" title="Dimension">dimension</link>.
</p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1492</anchor>  <taxon>Theorem</taxon> <addr>thm-000U</addr>  <route>thm-000U.xml</route>   <title><tex>\mathcal {L} (V,W)</tex> is isomorphic to <tex>\mathbb {F} ^{m \times  n}</tex></title> </frontmatter> <mainmatter><p>
    Let <tex>v_1, v_2,  \ldots , v_n</tex> be a basis for <tex>V</tex>,
    and <tex>w_1, w_2,  \ldots , w_m</tex> be a basis for <tex>W</tex>.
    Then <tex>\mathcal {M}</tex> is an isomorphism between <tex>\mathcal {L} (V,W)</tex> and <tex>\mathbb {F} ^{m \times  n}</tex>.
</p></mainmatter> </tree><p>
    This has a trivial corollary.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1493</anchor>  <taxon>Corollary</taxon> <addr>thm-000V</addr>  <route>thm-000V.xml</route>   <title>Dimension product</title> </frontmatter> <mainmatter><p>
    Let <tex>V</tex> and <tex>W</tex> be finite-dimensional vector spaces.
    Then <tex>\mathcal {L} (V,W)</tex> is finite-dimensional and
    <tex display="block">
         \dim ( \mathcal {L} (V,W)) =  \dim (V) \dim (W).
    </tex></p></mainmatter> </tree></mainmatter> </tree><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1494</anchor>  <taxon>Compute Science</taxon> <addr>cs-0001</addr>  <route>cs-0001.xml</route> <date><year>2024</year> <month>1</month> <day>29</day></date>  <title>Is JavaScript an untyped language?</title> </frontmatter> <mainmatter><p>
    This is a note about the the argument that JavaScript is an untyped language.
    Most opinions came from the References.
</p><p>
    The first thing I want to classify is the word <strong>strong typing</strong> and <strong>weak typing</strong> are meaningless.
    In a limit case we can compare two languages that have similar type system, and talk about which one is <em>stronger</em>.
    But for the common case, it's totally nonsense.
</p><p>
    Static and dynamic typing is a meaningful classsification. But the discussion about dynamic and static languages is mostly wrong on the Internet.
    Dynamic language is a popular concept, however, it is rather a <strong>marketing</strong> than a well-defined terminology.
    It's designed to confuse rather than inform.
</p><p>
    In fact, dynamic typing is just a special case of static typing.
    It limits more than contributes.The root of the problem is the confusion 
    between type and class. It's very useful to have multiple classes of values
    of a same type.
    They are interchangeable because they represent values of the same type.
    Only the form of presentation differs.
</p><p>
    The distinction between two classes of the same type is dynamic.
    But this does not conflict with the fact that only one static type.
    In type theory this is what we called <strong>Sum Type</strong>.
    Being a sum type we can dispatch on the class of the value of the type,
    and decide what to do at runtime.
</p><p>
    This characteristics is same to dynamic language where values can be classified into
    various forms that can be distinguished at runtime.
    The answer is now clear: dynamic language classifies all values in this way.
    What they do just merge all values of the language into a single type.
    The so-called <strong>untyped</strong> language is just <strong>unityped</strong>.
</p><p>
    Therefore, JavaScript is definitely untyped.
</p>
    <p><strong>References</strong></p>
    <ul><li><link href="https://existentialtype.wordpress.com/2011/03/19/dynamic-languages-are-static-languages/" type="external">Dynamic and static language</link></li>
        <li><link href="https://stackoverflow.com/questions/964910/is-javascript-an-untyped-language" type="external">stackoverflow</link></li>
        <li><link href="https://blogs.perl.org/users/ovid/2010/08/what-to-know-before-debating-type-systems.html" type="external">What to know before debating type systems</link></li>
        <li><em>Practical Foundations for Programming Languages</em>, Robert Harper</li></ul>
</mainmatter> </tree></mainmatter> </tree>
    <tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1495</anchor>   <addr>projects</addr>  <route>projects.xml</route>   <title>Projects</title> </frontmatter> <mainmatter><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1496</anchor>  <taxon>Project</taxon> <addr>proj-0001</addr>  <route>proj-0001.xml</route>   <title>Command Lisp</title> </frontmatter> <mainmatter><p><link href="https://github.com/CAIMEOX/CommandLisp" type="external">Command Lisp</link> is a simplified language designed for Minecraft Bedrock Command System, characterized by a very high level of abstraction, which is also a dialect of Lisp.
</p></mainmatter> </tree><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1497</anchor>  <taxon>Project</taxon> <addr>proj-0002</addr>  <route>proj-0002.xml</route>   <title>Pure Eval</title> </frontmatter> <mainmatter><p><link href="https://github.com/PureEval/PureEval" type="external">Pure Eval</link> was created for the <link href="proj-0003.xml" type="local" addr="proj-0003" title="Voxel Geometry">VoxelGeometry</link> project, aiming to build a compact yet powerful JavaScript functional utility toolkit.
</p></mainmatter> </tree><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1498</anchor>  <taxon>Project</taxon> <addr>proj-0003</addr>  <route>proj-0003.xml</route>   <title>Voxel Geometry</title> </frontmatter> <mainmatter><p><link href="https://github.com/CAIMEOX/VoxelGeometry" type="external">Voxel Geometry</link> is voxel geometry library which is used to construct Space (A collection of 3-dimension Vectors) and perform transformation between Spaces.
</p></mainmatter> </tree><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1499</anchor>  <taxon>Project</taxon> <addr>proj-0004</addr>  <route>proj-0004.xml</route>   <title>Minecraft ScriptAPI wrapper</title> </frontmatter> <mainmatter><p>These projects create Minecraft Script API wrapper for foreign language that compiles to JavaScript.</p><ul><li><link href="https://github.com/CAIMEOX/rescript-bedrock" type="external">ReScript</link></li>
    <li><link href="https://github.com/CAIMEOX/pure_bedrock" type="external">PureScript</link></li>
    <li><link href="https://github.com/CAIMEOX/BedrockFP" type="external">Idris2</link></li></ul></mainmatter> </tree></mainmatter> </tree>
</mainmatter> </tree></context> <related/> <backlinks/> <references/></backmatter></tree>