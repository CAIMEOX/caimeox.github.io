<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="forest.xsl"?>
<tree expanded="true" show-heading="true" show-metadata="true" toc="false" numbered="false" root="false"><frontmatter><anchor>1244</anchor> <taxon>Compute Science</taxon> <addr>cs-0001</addr><route>cs-0001.xml</route>  <date><year>2024</year> <month>1</month> <day>29</day></date>  <title>Is JavaScript an untyped language?</title>   </frontmatter> <mainmatter><p>
    This is a note about the the argument that JavaScript is an untyped language.
    Most opinions came from the References.
</p><p>
    The first thing I want to classify is the word <strong>strong typing</strong> and <strong>weak typing</strong> are meaningless.
    In a limit case we can compare two languages that have similar type system, and talk about which one is <em>stronger</em>.
    But for the common case, it's totally nonsense.
</p><p>
    Static and dynamic typing is a meaningful classsification. But the discussion about dynamic and static languages is mostly wrong on the Internet.
    Dynamic language is a popular concept, however, it is rather a <strong>marketing</strong> than a well-defined terminology.
    It's designed to confuse rather than inform.
</p><p>
    In fact, dynamic typing is just a special case of static typing.
    It limits more than contributes.The root of the problem is the confusion 
    between type and class. It's very useful to have multiple classes of values
    of a same type.
    They are interchangeable because they represent values of the same type.
    Only the form of presentation differs.
</p><p>
    The distinction between two classes of the same type is dynamic.
    But this does not conflict with the fact that only one static type.
    In type theory this is what we called <strong>Sum Type</strong>.
    Being a sum type we can dispatch on the class of the value of the type,
    and decide what to do at runtime.
</p><p>
    This characteristics is same to dynamic language where values can be classified into
    various forms that can be distinguished at runtime.
    The answer is now clear: dynamic language classifies all values in this way.
    What they do just merge all values of the language into a single type.
    The so-called <strong>untyped</strong> language is just <strong>unityped</strong>.
</p><p>
    Therefore, JavaScript is definitely untyped.
</p>
    <p><strong>References</strong></p>
    <ul><li><link href="https://existentialtype.wordpress.com/2011/03/19/dynamic-languages-are-static-languages/" type="external">Dynamic and static language</link></li>
        <li><link href="https://stackoverflow.com/questions/964910/is-javascript-an-untyped-language" type="external">stackoverflow</link></li>
        <li><link href="https://blogs.perl.org/users/ovid/2010/08/what-to-know-before-debating-type-systems.html" type="external">What to know before debating type systems</link></li>
        <li><em>Practical Foundations for Programming Languages</em>, Robert Harper</li></ul>
</mainmatter> <backmatter><contributions/> <context><tree expanded="false" show-heading="true" show-metadata="true" toc="false" numbered="false" root="false"><frontmatter><anchor>1245</anchor>  <addr>notes</addr><route>notes.xml</route>    <title>Notes</title>   </frontmatter> <mainmatter><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1246</anchor> <taxon>Type Theory</taxon> <addr>tt-0001</addr><route>tt-0001.xml</route>  <date><year>2024</year> <month>1</month> <day>26</day></date>  <title>Untyped Lambda Calculus</title>   </frontmatter> <mainmatter><p>
    Refer to <link href="ttafp-2014.xml" type="local" addr="ttafp-2014" title="Type Theory and Formal Proofs">Type Theory and Formal Proof</link>.
</p><p>
    The idea to generalize the behavior of functions in mathematics and logic led to the development of the lambda calculus.
    The lambda calculus is a formal system for expressing computation based on function abstraction and application using <em>variable binding</em> and <em>substitution</em>. 
    In dealing with functions there are two <strong>construction principles</strong> and one <strong>evalutaion rule</strong>.
    <ul><li><strong>Construction Principles</strong>: note that expressions do not force to be meaningful.</li>
        <ul><li>Function Abstraction: <tex>\lambda  x.M</tex></li>
            <li>Function Application: <tex>M N</tex>, this only produces a new expression,
            in which the function has not yet been executed.</li></ul>
    <li><strong>Evaluation Rule</strong></li>
        <ul><li>Beta Reduction: <tex>( \lambda  x.M)N \to  M[x:=N]</tex></li></ul></ul>
    The beta reduction makes use of the <strong>substitution</strong> <tex>M[x:=N]</tex> which represents the result of replacing all free occurrences of <tex>x</tex> in <tex>M</tex> with <tex>N</tex>.
    Note that the application is <strong>left associative</strong>, that is, <tex>MNP</tex> means <tex>(MN)P</tex>.
    And application has the highest precedence, that is, <tex>\lambda  x.MN</tex> means <tex>\lambda  x.(MN)</tex>.
</p><p>
    The multi-argument function <tex>\lambda  x_1 \ldots  x_n.M</tex> is defined as <tex>\lambda  x_1.( \lambda  x_2.( \ldots ( \lambda  x_n.M) \ldots ))</tex> (right associative),
    that is, simulated by a sequence of single-argument functions. The later function is called <strong>curried function</strong> and the
    process of transforming a multi-argument function into a sequence of single-argument functions is called <strong>currying</strong>.
</p>
    <strong>Lambda Terms</strong>
    <p>Expressions in the lambda calculus is called <strong>terms</strong>. The set of terms is denoted <tex>\Lambda</tex>.</p>
    <tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1247</anchor> <taxon>Definition</taxon> <addr>def-000F</addr><route>def-000F.xml</route>    <title>Set of Lambda Terms</title>   </frontmatter> <mainmatter><p>
Let <tex>\Lambda</tex> be the set of lambda terms. Then <tex>\Lambda</tex> is defined inductively as follows:
(<tex>V</tex> is the set of variables)
<ul><li>Variable: <tex>\forall  x \in  V, x \in   \Lambda</tex></li>
<li>Abstraction: <tex>\forall  x \in  V, M \in   \Lambda ,  \lambda  x.M \in   \Lambda</tex></li>
<li>Application: <tex>\forall  M,N \in   \Lambda , (MN) \in   \Lambda</tex></li></ul></p><p>
Another way to define <tex>\Lambda</tex> is to use the following grammar (The 3 possibilities are separated by <code>|</code>):
<tex display="block">\Lambda  = V |  \lambda  V. \Lambda  |  \Lambda \Lambda</tex></p></mainmatter> </tree>
<p>
    With the following recursive definition we can determine 
    what the <strong>subterms</strong> of a give <tex>\lambda \text {-term}</tex> are. Here we use 
    a concept named <link href="def-0035.xml" type="local" addr="def-0035" title="Multiset">multiset</link>.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1248</anchor> <taxon>Definition</taxon> <addr>def-0036</addr><route>def-0036.xml</route>    <title>Multiset of Subterms</title>   </frontmatter> <mainmatter><p>
    We define a map <tex>\text {Sub}</tex>:
    <ul><li><strong>Basis</strong>: <tex>\forall  x \in  V, \text {Sub} (x) = \{   x   \}</tex></li>
        <li><strong>Application</strong>: <tex>\forall  M,N \in \Lambda , \text {Sub} (MN) =  \text {Sub} (M) \cup \text {Sub} (N) \cup \{   MN   \}</tex></li>
        <li><strong>Abstraction</strong>: <tex>\forall  x \in  V,M \in \Lambda , \text {Sub} ( \lambda  x.M) =  \text {Sub} (M) \cup \{   \lambda  x.M   \}</tex></li></ul>
    <tex>L</tex> is a subterm of <tex>M</tex> if <tex>L \in \text {Sub} (M)</tex>.
    If <tex>L \not \equiv  M</tex> then we say <tex>L</tex> is a <strong>proper subterm</strong> of <tex>M</tex>.
</p></mainmatter> </tree><p>
    The definition above uses a notation <tex>\equiv</tex> which means <em>syntactic equality</em> here.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1249</anchor> <taxon>Example</taxon> <addr>eg-0006</addr><route>eg-0006.xml</route>    <title>Subterms</title>   </frontmatter> <mainmatter><ul><li><tex>\text {Sub} ( ( x \space z ) )</tex> = <tex>\{   x,z, ( x \space z )   \}</tex></li>
    <li><tex>              \text {Sub} ( \lambda  x. ( x \space x ) )              =  \{   \lambda  x. ( x \space x ) , ( x \space x ) ,x,x   \}          </tex></li></ul></mainmatter> </tree><p>
    The substerm mapping satisfies the following lemma.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1250</anchor> <taxon>Lemma</taxon> <addr>thm-000W</addr><route>thm-000W.xml</route>    <title>Lemma of subterms</title>   </frontmatter> <mainmatter><p><ul><li><strong>Reflexivity</strong>: <tex>\forall  M \in \Lambda ,M \in \text {Sub} (M)</tex></li>
        <li><strong>Transitivity</strong>: <tex>\forall  L,M,N \in \Lambda ,L \in \text {Sub} (M) \land  M \in \text {Sub} (N) \implies  L \in \text {Sub} (N)</tex></li></ul></p></mainmatter> </tree><p>
    Variable occurrences in a <tex>\lambda \text {-term}</tex> can be divided into 3 categories:
    <ul><li>Bound Occurrences: <tex>x</tex> is bound in <tex>M</tex> if <tex>x</tex> is the argument of an <strong>abstraction</strong> in <tex>M</tex>.</li>
        <li>Free Occurrences: <tex>x</tex> is free in <tex>M</tex> if <tex>x</tex> is not bound by any <strong>abstraction</strong> in <tex>M</tex>.</li>
        <li>Binding Occurrences: something after a lambda notation <tex>\lambda</tex></li></ul>
    We mainly focus on the <strong>free variables</strong> of a term.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1251</anchor> <taxon>Definition</taxon> <addr>def-0037</addr><route>def-0037.xml</route>    <title>Set of Free Variables</title>   </frontmatter> <mainmatter><p>
    Let <tex>FV(L)</tex> be the set of free variables in a term <tex>L</tex>.
    <ul><li><strong>Variable</strong>: <tex>\forall  x \in  V, \text {FV} (x) =  \{   x   \}</tex></li>
        <li><strong>Application</strong>: <tex>\forall  M,N \in \Lambda , \text {FV} (MN) =  \text {FV} (M) \cup \text {FV} (N)</tex></li>
        <li><strong>Abstraction</strong>: <tex>\forall  x \in  V,M \in \Lambda , \text {FV} ( \lambda  x.M) =  \text {FV} (M) \setminus \{   x   \}</tex></li></ul></p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1252</anchor> <taxon>Definition</taxon> <addr>def-0038</addr><route>def-0038.xml</route>    <title>Closed Lambda Terms</title>   </frontmatter> <mainmatter><p>
    The <tex>\lambda \text {-term}</tex> <tex>M</tex> is <strong>closed</strong> if <tex>\text {FV} (M) =  \emptyset</tex>.
    A closed <tex>\lambda \text {-term}</tex> is also called a <strong>combinator</strong>.
    The set of all combinators is denoted by <tex>\Lambda ^0</tex>.
</p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1253</anchor>    <date><year>2024</year> <month>1</month> <day>26</day></date>  <title>Alpha conversion</title>   </frontmatter> <mainmatter><p>Functions in <tex>\lambda \text {-calculus}</tex> have the property that the name of 
    the binding variables is irrelevant.
    In order to describe this equality we need to define a relation
    called <tex>\alpha \text {-conversion}</tex> or <tex>\alpha \text {-equivalence}</tex>,
    which is based on the process of renaming binding variables.</p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1254</anchor> <taxon>Definition</taxon> <addr>def-0039</addr><route>def-0039.xml</route>    <title>Renaming</title>   </frontmatter> <mainmatter><p>
    Let <tex>M ^{ x \to y }</tex> be the result of replacing all free occurrences of <tex>x</tex> in <tex>M</tex> with <tex>y</tex>.
    The relation <strong>renaming</strong> is expression with the symbol <tex>=_ \alpha</tex>:
    <tex>\lambda  x.M =_ \alpha \lambda  y. M ^{ x \to y }</tex> if <tex>y \not \in \text {FV} (M)</tex> and <tex>y</tex> is not a binding variable in <tex>M</tex>.
</p></mainmatter> </tree><p>
    The definition of <strong>renaming</strong> should be extended to more general terms.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1255</anchor> <taxon>Definition</taxon> <addr>def-003A</addr><route>def-003A.xml</route>    <title>Alpha Equivalence</title>   </frontmatter> <mainmatter><p>
    The <tex>\alpha</tex> equivalence is a relation between <tex>\lambda \text {-term}</tex>, defined as follows:
    <ul><li><strong>Renaming</strong>: <tex>\lambda  x. M  =_ \alpha   \lambda  y.  M ^{ x \to y }</tex> if <tex>y \not \in \text {FV} (M)</tex> and <tex>y</tex> is not a binding variable in <tex>M</tex>.
        </li>
        <li><strong>Compatibility</strong>: If <tex>M =_ \alpha  N</tex> then <tex>\lambda  x.M =_ \alpha \lambda  x.N</tex> and <tex>ML  =_ \alpha  NL, LM  =_ \alpha  LN</tex>.
        </li>
        <li><strong>Reflexivity</strong>: <tex>M =_ \alpha  M</tex>.
        </li>
        <li><strong>Symmetry</strong>: If <tex>M =_ \alpha  N</tex> then <tex>N =_ \alpha  M</tex>.
        </li>
        <li><strong>Transitivity</strong>: If <tex>M =_ \alpha  N</tex> and <tex>N =_ \alpha  L</tex> then <tex>M =_ \alpha  L</tex>.
        </li></ul>
    The first principle is the basis of alpha equivalence, which is the same as <link href="def-0039.xml" type="local" addr="def-0039" title="Renaming">renaming</link>.
    The last 3 properties ensures that <tex>=_ \alpha</tex> is an <link href="def-000X.xml" type="local" addr="def-000X" title="Equivalence Relation">equivalence relation</link>.
    
</p><p>
    If <tex>M =_ \alpha  N</tex> then we say <tex>M</tex> and <tex>N</tex> are <tex>\alpha \text {-equivalent}</tex> or <tex>\alpha \text {-convertible}</tex>.
    <tex>M</tex> is an <tex>\alpha \text {-variant}</tex> of <tex>N</tex> and vice versa.

</p></mainmatter> </tree><p>
    In previous sections we informally mentioned the concept of <strong>substitution</strong>.
    Now we give a precise formulation
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1256</anchor> <taxon>Definition</taxon> <addr>def-003B</addr><route>def-003B.xml</route>    <title>Substitution</title>   </frontmatter> <mainmatter><p>
    The <strong>substitution</strong> is defined by the following rules:
    <ul><li><tex>M [ x := N ] : \equiv  N</tex></li>
        <li><tex>y [ x := N ] : \equiv  y</tex> if <tex>y \not \equiv  x</tex></li>
        <li><tex>(PQ) [ x := N ] : \equiv ( P [ x := N ] )( Q [ x := N ] )</tex></li>
        <li><tex>( \lambda  y.P) [ x := N ] : \equiv \lambda  z. P ^{ y \to z } [ x := N ]</tex> 
            if <tex>\lambda  z. P ^{ y \to z }   =_ \alpha   \lambda  y.P</tex> and <tex>z \not \in \text {FV} (N)</tex></li></ul>
    The terms with form <tex>P [ x := N ]</tex> are not <tex>\lambda \text {-term}</tex>,
    but we can regard them as a <em>meta notation</em> that appears
    in the substitution process and the result contains no such terms.
</p></mainmatter> </tree><p>
    Renaming can be considered as a special case of substitution.
    We can show that <tex>M ^{ x \to u } =_ \alpha M [ x := u ]</tex> if the conditions of renaming are satisfied.
</p><p>
    We may do <strong>sequential substitution</strong> in a term,
    that is, doing a number of substitutions consecutively.
    And we have the following lemma, which states that the order of substitution is important.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1257</anchor> <taxon>Lemma</taxon> <addr>thm-000X</addr><route>thm-000X.xml</route>    <title>Substitution is not commutative</title>   </frontmatter> <mainmatter><p>
    Let <tex>x \not \equiv  y</tex> and assume <tex>x \not \in \text {FV} (L)</tex>.
    Then <tex>L[y:=N][x:=M]  \equiv  L[x:=M][y:=N[x:=M]]</tex>.
</p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1258</anchor>    <date><year>2024</year> <month>1</month> <day>26</day></date>  <title><tex>\lambda \text {-term}</tex> modulo <tex>\alpha \text {-equivalence}</tex></title>   </frontmatter> <mainmatter><p>As we have seen, the relation <tex>=_ \alpha</tex> is an equivalence relation.
    Hence we can define the set of equivalence classes of terms with respect to <tex>=_ \alpha</tex>.
    Now we can identify a term with its equivalence class.
    We still use <tex>\equiv</tex> for syntactic equality modulo <tex>\alpha \text {-equivalence}</tex>.
    </p></mainmatter> </tree><p><tex>\alpha \text {-equivalence}</tex> is a congruence relation, which means that it is conserved by elementary process of term construction.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1259</anchor> <taxon>Lemma</taxon> <addr>thm-000Y</addr><route>thm-000Y.xml</route>    <title>Congruence Property of Substitution</title>   </frontmatter> <mainmatter><p>
    Let <tex>L =_ \alpha  M</tex> and <tex>N =_ \alpha  P</tex>.
    <ul><li><tex>                 LN  =_ \alpha  MP             </tex></li>
        <li><tex>                  \lambda  x.L  =_ \alpha   \lambda  x.M             </tex></li>
        <li><tex>                  L [ x := N ]   =_ \alpha   M [ x := P ]              </tex></li></ul></p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1260</anchor>    <date><year>2024</year> <month>1</month> <day>26</day></date>  <title>Barendregt Convention</title>   </frontmatter> <mainmatter><p>The Barendregt Convention states that we should avoid using the same variable name in different abstractions.
    This is to avoid the confusion of free variables. It states that 
    we choose the names for the binding variables in a <tex>\lambda \text {-term}</tex> in such a manner
    that they are all different, and each of them differs from 
    all free variables occurring in the term.</p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1261</anchor>    <date><year>2024</year> <month>1</month> <day>26</day></date>  <title>Beta Reduction</title>   </frontmatter> <mainmatter><p>
        Since we have formally defined the <strong>substitution</strong>,
        we can rephrase the reduction as a relation on <tex>\lambda \text {-term}</tex>, namely <tex>\beta \text {-reduction}</tex>.
    </p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1262</anchor> <taxon>Definition</taxon> <addr>def-003C</addr><route>def-003C.xml</route>    <title>One Step Beta Reduction</title>   </frontmatter> <mainmatter><p><strong>One step beta reduction</strong> (<tex>\to _ \beta</tex>) is defined as follows:
    <ul><li><strong>Basis</strong>:
            <tex>                 ( \lambda  x.M)N \to _ \beta M [ x := N ]              </tex></li>
        <li><strong>Compatibility</strong>:
            If <tex>M \to _ \beta  N</tex> then <tex>\lambda  x.M \to _ \beta \lambda  x.N</tex>,
            <tex>ML \to _ \beta  NL</tex> and <tex>LM \to _ \beta  LN</tex>.
        </li></ul></p><p>
    The term of the form <tex>( \lambda  x.M)N</tex> is called a <strong>redex (reducible expression)</strong>.
    The term of the form <tex>M [ x := N ]</tex> is called the <strong>contractum</strong> (of the redex).
</p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1263</anchor> <taxon>Example</taxon> <addr>eg-0007</addr><route>eg-0007.xml</route>    <title>Divergent Combinator</title>   </frontmatter> <mainmatter><p>
    An interesting example named <strong>omega combinator</strong> of beta reduction is the following:
    <tex display="block">          ( ( \lambda  x. ( x \space x ) ) \space ( \lambda  x. ( x \space x ) ) )           \to _ \beta           ( ( \lambda  x. ( x \space x ) ) \space ( \lambda  x. ( x \space x ) ) )      </tex>
    The result of the beta reduction is the same term as the original term,
    and never terminates.
</p></mainmatter> </tree><p>
    We can often perform a sequence of beta reductions. This leads to the definition.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1264</anchor> <taxon>Definition</taxon> <addr>def-003D</addr><route>def-003D.xml</route>    <title>Beta Reduction</title>   </frontmatter> <mainmatter><p>
    The <strong>(zero-or-more-step) beta reduction</strong> (<tex>\twoheadrightarrow _{ \beta }</tex>) is a
    generalized version of the <link href="def-003C.xml" type="local" addr="def-003C" title="One Step Beta Reduction">one step beta reduction</link>.
    <tex>M \twoheadrightarrow _{ \beta }  N</tex> if there exists <tex>n \geq  0</tex> and there are terms <tex>M_0,M_1, \ldots ,M_n</tex>
    such that <tex>M_0=M</tex>, <tex>M_n=N</tex> and <tex>M_i \to _ \beta  M_{i+1}</tex> for <tex>0 \leq  i&lt;n</tex>.
    In other words there exists a chain of one-step beta reductions from <tex>M</tex> to <tex>N</tex>.
    <tex display="block">         M \equiv  M_0 \to _ \beta  M_1 \to _ \beta \cdots \to _ \beta  M_n \equiv  N     </tex></p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1265</anchor> <taxon>Lemma</taxon> <addr>thm-000Z</addr><route>thm-000Z.xml</route>    <title>Properties of Beta Reduction</title>   </frontmatter> <mainmatter><p><ul><li><strong>Compatibility</strong>:
            <tex>\twoheadrightarrow _{ \beta }</tex> extends <tex>\to _ \beta</tex>, i.e. if <tex>M \to _ \beta  N</tex> then <tex>M \twoheadrightarrow _{ \beta }  N</tex>.
        </li>
        <li><strong>Reflixivity</strong>:
            <tex>                 M \twoheadrightarrow _{ \beta }  M             </tex></li>
        <li><strong>Transitivity</strong>:
            <tex>                 M \twoheadrightarrow _{ \beta }  N  \land  N \twoheadrightarrow _{ \beta }  P  \implies  M \twoheadrightarrow _{ \beta }  P             </tex></li></ul></p></mainmatter> </tree></mainmatter> </tree><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1266</anchor> <taxon>Set Theory</taxon> <addr>math-0003</addr><route>math-0003.xml</route>  <date><year>2024</year> <month>1</month> <day>26</day></date>  <title>Set Theory</title>   </frontmatter> <mainmatter><p>
    Refer to <link href="cat-sci-2013.xml" type="local" addr="cat-sci-2013" title="Category theory for scientists">Category Theory for Scientists</link>.
</p><p><strong>Set</strong> is a common concept in mathematics.
    This post is a brief introduction to set theory aimed at 
    complete all basic knowledge of set theory.
    The following topics will be covered
    <ul><li><strong>Zermelo-Fraenkel Axioms</strong> and <strong>Axiom of Choice</strong></li>
        <li>Cardinality</li>
        <li>Set theory constructions</li></ul></p><p>
    In this post, we use the Zermelo-Fraenkel set theory with the Axiom of Choice (<strong>ZFC</strong>).
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1267</anchor> <taxon>Definition</taxon> <addr>def-000S</addr><route>def-000S.xml</route>    <title>ZFC Set</title>   </frontmatter> <mainmatter><p><strong>ZFC</strong> is the abbreviation of Zermelo-Fraenkel set theory with the Axiom of Choice.
    The axioms of ZFC are listed below.
    <ul><li><strong>Axiom of Extensionality</strong>:
            Two sets are equal if and only if they have the same elements.
        </li>
        <li><strong>Axiom of Pairing</strong>:
            For any two sets <tex>a</tex> and <tex>b</tex>,
            there exists a set <tex>\{   a,b   \}</tex> whose elements are exactly <tex>a</tex> and <tex>b</tex>.
        </li>
        <li><strong>Axiom schema of Separation</strong>:
            Let <tex>P</tex> is a property of sets.
            <tex>P(u)</tex> means <tex>u</tex> satisfies the property <tex>P</tex>.
            then for any set <tex>X</tex> exists <tex>Y =  \{   u  \in  X | P(u)   \}</tex>.
        </li>
        <li><strong>Axiom of Union</strong>:
            For any set <tex>X</tex> (a family of sets), exists union set <tex>\bigcup  X : \equiv   \{                    u: \exists  v \in  X  \text { such that } u \in  v                \}</tex>.
        </li>
        <li><strong>Axiom of Power Set</strong>:
            For any set <tex>X</tex>, exists the power <tex>P(X) : \equiv   \{   Y:Y \subseteq  X   \}</tex>.
        </li>
        <li><strong>Axiom of Infinity</strong>:
            There exists a set <tex>\omega</tex> such that <tex>\emptyset \in \omega</tex> and for any <tex>x \in \omega</tex>, <tex>x \cup \{   x   \} \in \omega</tex>.
        </li>
        <li><strong>Axiom of Regularity</strong>:
            For any non-empty set there is a minimal element with respect to the membership relation.
        </li>
        <li><strong>Axiom schema of Replacement</strong>:
            Let <tex>F</tex> be a function where <tex>\text {dom }  f = X</tex>, then for any set <tex>X</tex> exists a set <tex>Y =  \{   F(x):x \in  X   \}</tex>.
            <p>
                This function is not the normal function but some logical stuff.
            </p></li>
        <li><strong>Axiom of Choice</strong>:
            For any family of non-empty sets <tex>X</tex>, there exists a function <tex>f:X \to \bigcup  X</tex> such that for any <tex>x \in  X</tex>, <tex>f(x) \in  x</tex>.
        </li></ul></p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1268</anchor> <taxon>Definition</taxon> <addr>def-002V</addr><route>def-002V.xml</route>    <title>Set Operations</title>   </frontmatter> <mainmatter><p>
    Let <tex>(X_i)_{i \in  I}</tex> be a family of sets.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1269</anchor>      <title>Union</title>   </frontmatter> <mainmatter><p><tex display="block">          \bigcup _{i \in  I}X_i =  \set {x: \exists  i \in  I  \text { such that } x \in  X_i}     </tex></p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1270</anchor>      <title>Intersection</title>   </frontmatter> <mainmatter><p><tex display="block">          \bigcap _{i \in  I}X_i =  \set {x: \forall  i \in  I, x \in  X_i}     </tex>
    Note that <tex>I  \neq   \emptyset</tex> here.
</p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1271</anchor>      <title>Disjoint Union</title>   </frontmatter> <mainmatter><p><tex display="block">          \bigsqcup _{i \in  I}X_i =  \set {(x,i):x \in  X_i, i \in  I}     </tex></p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1272</anchor>      <title>Product</title>   </frontmatter> <mainmatter><p><tex display="block">          \prod _{i \in  I}X_i =  \set {(x_i)_{i \in  I}: \forall  i \in  I, x_i \in  X_i}     </tex></p></mainmatter> </tree></mainmatter> </tree><p>
    And principles of set theory
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1273</anchor> <taxon>Definition</taxon> <addr>def-000T</addr><route>def-000T.xml</route>    <title>Principle of Extensionality</title>   </frontmatter> <mainmatter><p>
    Two sets are equal if and only if they have the same elements.
</p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1274</anchor> <taxon>Definition</taxon> <addr>def-000U</addr><route>def-000U.xml</route>    <title>Principle of Comprehension</title>   </frontmatter> <mainmatter><p>
    Given a set <tex>A</tex> and a property <tex>P(x)</tex>, there exists a set <tex>B</tex> such that
    <tex>x \in  B  \iff  x \in  A  \land  P(x)</tex>.
</p></mainmatter> </tree><p>
    We then define the Cartesian product of two sets
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1275</anchor> <taxon>Definition</taxon> <addr>def-000V</addr><route>def-000V.xml</route>    <title>Cartesian product</title>   </frontmatter> <mainmatter><p>
    Given two sets <tex>A</tex> and <tex>B</tex>, the Cartesian product <tex>A \times  B</tex> is the set
    of all ordered pairs <tex>(a,b)</tex> where <tex>a \in  A</tex> and <tex>b \in  B</tex>.
</p></mainmatter> </tree><p>
    With the Cartesian product, we can define the relation
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1276</anchor> <taxon>Definition</taxon> <addr>def-000W</addr><route>def-000W.xml</route>    <title>Relation</title>   </frontmatter> <mainmatter><p>
    A <strong>relation</strong> <tex>R</tex> is a subset of the Cartesian product of two sets <tex>A</tex> and
    <tex>B</tex>, i.e. <tex>R \subseteq  A \times  B</tex>.
    If <tex>(a,b) \in  R</tex>, we write <tex>aRb</tex>.

    A relation that between <tex>X</tex> and itself is called <strong>homogeneous relation</strong>.
</p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1277</anchor> <taxon>Definition</taxon> <addr>def-000X</addr><route>def-000X.xml</route>    <title>Equivalence Relation</title>   </frontmatter> <mainmatter><p>
    An equivalence relation <tex>R</tex> on a set <tex>A</tex> is a <link href="def-000W.xml" type="local" addr="def-000W" title="Relation">relation</link> that is reflexive,
    symmetric, and transitive.
    <ul><li>Reflexive:
            <tex>\forall  x \in  A, xRx</tex></li>
        <li>Symmetric:
            <tex>\forall  x,y \in  A, xRy \implies  yRx</tex></li>
        <li>Transitive:
            <tex>\forall  x,y,z \in  A, xRy \land  yRz \implies  xRz</tex></li></ul>
    We often denote the equivalence relation by <tex>\sim</tex>.
</p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1278</anchor> <taxon>Definition</taxon> <addr>def-002U</addr><route>def-002U.xml</route>    <title>Equivalence Class</title>   </frontmatter> <mainmatter><p>
    Let <tex>\sim</tex> be an <link href="def-000X.xml" type="local" addr="def-000X" title="Equivalence Relation">equivalence relation</link> on a set <tex>A</tex>.
    For any element <tex>a \in  A</tex>, the <strong>equivalence class</strong> of <tex>a</tex> is the set
    <tex>[a] =  \set {b \in  A:b \sim  a}</tex>.
    The set of all equivalence classes is denoted by <tex>A/ \sim</tex>,
    which is called the <strong>quotient set</strong> of <tex>A</tex> by <tex>\sim</tex>.
    <p>
        The equivalence class of <tex>a</tex> is also denoted by <tex>\overline {a}</tex>.
    </p></p></mainmatter> </tree><p>
    One of the most important relations is the order relation.
    The basic order relation is the preorder.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1279</anchor> <taxon>Definition</taxon> <addr>def-000Z</addr><route>def-000Z.xml</route>    <title>Preorder</title>   </frontmatter> <mainmatter><p>
    A <strong>preorder</strong> is a relation <tex>\leq</tex> that is reflexive and transitive.
    <ul><li>Reflexive: <tex>a \leq  a</tex></li>
        <li>Transitive: <tex>a \leq  b</tex> and <tex>b \leq  c</tex> implies <tex>a \leq  c</tex></li></ul></p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1280</anchor> <taxon>Definition</taxon> <addr>def-000Y</addr><route>def-000Y.xml</route>    <title>Partial Order</title>   </frontmatter> <mainmatter><p>
    A <strong>(non-strict) partial order</strong> is a relation <tex>\leq</tex> that is reflexive, antisymmetric and transitive.
    <ul><li>Reflexive: <tex>a \leq  a</tex></li>
        <li>Antisymmetric: <tex>a \leq  b</tex> and <tex>b \leq  a</tex> implies <tex>a=b</tex></li>
        <li>Transitive: <tex>a \leq  b</tex> and <tex>b \leq  c</tex> implies <tex>a \leq  c</tex></li></ul>
    A non-strict partial order is also known as an antisymmetric <link href="def-000Z.xml" type="local" addr="def-000Z" title="Preorder">preorder</link>.
</p></mainmatter> </tree><p>
    And the strict partial order (notice the difference between asymmetric and antisymmetric)
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1281</anchor> <taxon>Definition</taxon> <addr>def-0010</addr><route>def-0010.xml</route>    <title>Strict partial orders</title>   </frontmatter> <mainmatter><p>
    A strict partial order is a relation <tex>&lt;</tex> that is irreflexive, asymmetric and transitive.
    <ul><li>Irreflexive: <tex>\neg (a&lt;a)</tex></li>
        <li>Asymmetric: <tex>a&lt;b</tex> implies <tex>\neg (b&lt;a)</tex></li>
        <li>Transitive: <tex>a&lt;b</tex> and <tex>b&lt;c</tex> implies <tex>a&lt;c</tex></li></ul></p></mainmatter> </tree><p>
    With the definition of order, we can define the upper bound and lower bound
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1282</anchor> <taxon>Definition</taxon> <addr>def-0011</addr><route>def-0011.xml</route>    <title>Upper Bound and Lower Bound</title>   </frontmatter> <mainmatter><p>
    Let a subset <tex>S</tex> of a <link href="def-000Y.xml" type="local" addr="def-000Y" title="Partial Order">partially ordered</link> set <tex>(P,  \leq )</tex>,
    <tex>S</tex> is bounded above if there exists <tex>x  \in  P</tex> such that <tex>\forall  y  \in  S, y  \leq  x</tex>. And <tex>x</tex> is called an <strong>upper bound</strong> of <tex>S</tex>.
    Dually, <tex>S</tex> is bounded below if there exists <tex>x  \in  P</tex> such that <tex>\forall  y  \in  S, x  \leq  y</tex>. And <tex>x</tex> is called a <strong>lower bound</strong> of <tex>S</tex>.
</p>
    <p><strong>Supremum (least upper bound)</strong></p>
    <p>
    An element <tex>x \in  P</tex> is a supremum of <tex>S</tex>,
    if for all upper bounds <tex>z  \in  P</tex> of <tex>S</tex>, <tex>x  \leq  z</tex>.
    Denoted as <tex>x =  \sup  S</tex>.
    </p>
    <p><strong>Infimum (greatest lower bound)</strong></p>
    <p>
    An element <tex>x \in  P</tex> is a infimum of <tex>S</tex>,
    if for all lower bounds <tex>z  \in  P</tex> of <tex>S</tex>, <tex>z  \leq  x</tex>.
    Denoted as <tex>x =  \inf  S</tex>.
    </p>
</mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1283</anchor> <taxon>Definition</taxon> <addr>def-002G</addr><route>def-002G.xml</route>    <title>Function</title>   </frontmatter> <mainmatter><p>
    Let <tex>X</tex> and <tex>Y</tex> be sets then a <strong>function</strong> <tex>f:X  \to  Y</tex>
    is a mapping that sends each element of <tex>X</tex> to a unique element of <tex>Y</tex>,
    denoted by <tex>f(x) = y</tex>.
    Function is a special case of <link href="def-000W.xml" type="local" addr="def-000W" title="Relation">relation</link>, and it is a relation that is left-total and right-unique.
    <tex display="block">         f  \in  X  \times  Y  \text { and }  \forall  x  \in  X,  \exists ! y  \in  Y, (x,y)  \in  f     </tex>
    <tex>X</tex> is said to be the <strong>domain</strong> of <tex>f</tex> and <tex>Y</tex> is said to be the <strong>codomain</strong> of <tex>f</tex>,
    where we denote <tex>X =  \text {dom }  f</tex> and <tex>Y =  \text {cod }  f</tex>.
</p><p>
    Two functions <tex>f:X \to  Y</tex> and <tex>g:Y \to  Z</tex> can be <strong>composed</strong> to form a new function <tex>g  \circ  f : X  \to  Z</tex>,
    where the composition is defined by
    <tex display="block">         (g  \circ  f)(x) = g(f(x))      </tex></p><p>
    The set of all functions from <tex>X</tex> to <tex>Y</tex> is denoted by <tex>\hom _ \text {set} (X, Y)</tex>.
</p></mainmatter> </tree><p>
    The isomorphism function is defined as follows
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1284</anchor> <taxon>Definition</taxon> <addr>def-002H</addr><route>def-002H.xml</route>    <title>Set Isomorphism</title>   </frontmatter> <mainmatter><p>
    Let <tex>X</tex> and <tex>Y</tex> be sets and <tex>f: X  \to  Y</tex> be a function.
    The function <tex>f</tex> is called an <strong>isomorphism</strong> if it is both <link href="def-002D.xml" type="local" addr="def-002D" title="Injective">injective</link> and <link href="def-002F.xml" type="local" addr="def-002F" title="Surjective">surjective</link>.
    In other words, there exists a function <tex>g: Y  \to  X</tex> such that
    <tex display="block">         g  \circ  f =  \text {id} _X  \text { and } f  \circ  g =  \text {id} _Y     </tex>
    where <tex>\text {id} _X</tex> and <tex>\text {id} _Y</tex> are the <strong>identity functions</strong> on <tex>X</tex> and <tex>Y</tex> respectively.
    And we say <tex>f</tex> is <strong>invertible</strong> and <tex>g</tex> is the <strong>inverse</strong> of <tex>f</tex>.
    If there is a isomorphism between <tex>X</tex> and <tex>Y</tex>, we say <tex>X</tex> and <tex>Y</tex> are <strong>isomorphic</strong>,
    denoted by <tex>X  \cong  Y</tex>.
    Isomorphism is an <link href="def-000X.xml" type="local" addr="def-000X" title="Equivalence Relation">equivalence relation</link>.
</p></mainmatter> </tree><p>
    With isomorphism, we can define the cardinality of a set.
    Two isomorphic sets have the same cardinality.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1285</anchor> <taxon>Definition</taxon> <addr>def-002I</addr><route>def-002I.xml</route>    <title>Cardinality</title>   </frontmatter> <mainmatter><p>
    Let <tex>X</tex> be a set and <tex>n  \in   \mathbb {N}</tex>. 
    <tex>A</tex> si said to have <strong>cardinality</strong> <tex>n</tex>, denoted by <tex> |A|= n</tex>,
    if there exists an isomorphism between <tex>A</tex> and <tex>S_n =  \{   1,2, \cdots ,n   \}</tex>.
    If <tex>A</tex> has finite cardinality, we say <tex>A</tex> is <strong>finite</strong>, otherwise
    we say <tex>A</tex> is <strong>infinite</strong>, denoted by <tex>|A|  \geq   \infty</tex>.
</p></mainmatter> </tree><p>
    The next topic is the product of sets
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1286</anchor> <taxon>Definition</taxon> <addr>def-002J</addr><route>def-002J.xml</route>    <title>Product of Sets</title>   </frontmatter> <mainmatter><p>
    Let <tex>X</tex> and <tex>Y</tex> be sets, then the <strong>Cartesian product</strong> of <tex>X</tex> and <tex>Y</tex> is the set
    <tex display="block">         X  \times  Y =  \set {(x,y)  \mid  x  \in  X  \text { and } y  \in  Y}     </tex>
    There are two natural projections from the Cartesian product to the original sets, namely
    <tex display="block">          \pi _1 : X  \times  Y  \to  X  \text { and }  \pi _2 : X  \times  Y  \to  Y     </tex></p></mainmatter> </tree><p>
    This leads to an improtant concept named <strong>universal property</strong>.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1287</anchor> <taxon>Lemma</taxon> <addr>thm-000J</addr><route>thm-000J.xml</route>    <title>Universal Property for Product of Sets</title>   </frontmatter> <mainmatter><p>
    Let <tex>X</tex> and <tex>Y</tex> be sets.
    For any set <tex>A</tex> and function
    <tex>f: A  \to  X</tex> and <tex>g: A  \to  Y</tex>,
    there exists a <em>unique</em> function <tex>h: A  \to  X  \times  Y</tex> such that
    the following diagram commutes:
    
    <center><embedded-tex hash="4157eb89f51117c585cb94d00d036a56"><embedded-tex-preamble>
       \usepackage {tikz-cd}
       \usepackage {amssymb}
    </embedded-tex-preamble> <embedded-tex-body>
         
         \begin {tikzcd}
            &amp; {X \times  Y}  \\ 
            X &amp;&amp; Y  \\ 
            &amp; A
             \arrow [&quot;{ \pi _1}&quot;', from=1-2, to=2-1]
             \arrow [&quot;{ \pi _2}&quot;, from=1-2, to=2-3]
             \arrow [&quot;f&quot;, from=3-2, to=2-1]
             \arrow [&quot;g&quot;', from=3-2, to=2-3]
             \arrow [&quot;{ \exists !}&quot;, dashed, from=3-2, to=1-2]
         \end {tikzcd}
     
    </embedded-tex-body></embedded-tex></center>

    We might denote the unique function by <tex>\langle  f,g  \rangle : A  \to  X  \times  Y</tex>.
    It is sufficient to define <tex>\langle  f,g  \rangle (a) = (f(a),g(a))</tex> for all <tex>a \in  A</tex> as the unique function.
</p></mainmatter> </tree><p>
    Dual to the product of sets, we have the coproduct of sets
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1288</anchor> <taxon>Definition</taxon> <addr>def-002K</addr><route>def-002K.xml</route>    <title>Coproduct of Sets</title>   </frontmatter> <mainmatter><p>
    Let <tex>X</tex> and <tex>Y</tex> be sets, then the <strong>coproduct</strong> of <tex>X</tex> and <tex>Y</tex> is 
    defined as the <strong>disjoint union</strong> of <tex>X</tex> and <tex>Y</tex>, denoted by <tex>X  \sqcup  Y</tex>.
    There are two natural injections from the original sets to the coproduct, namely
    <tex display="block">         i_1 : X  \to  X  \sqcup  Y  \text { and } i_2 : Y  \to  X  \sqcup  Y     </tex></p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1289</anchor> <taxon>Lemma</taxon> <addr>thm-000K</addr><route>thm-000K.xml</route>    <title>Universal Property for Coproduct of Sets</title>   </frontmatter> <mainmatter><p>
    Let <tex>X</tex> and <tex>Y</tex> be sets. For any set <tex>A</tex> and function
    <tex>f : X  \to  A</tex> and <tex>g : Y  \to  A</tex>, there exists a <em>unique</em> function
    <tex>h : X  \sqcup  Y  \to  A</tex> such that the following diagram commutes:
    
    <center><embedded-tex hash="31473672edfba5d6215d76dd08a01a24"><embedded-tex-preamble>
       \usepackage {tikz-cd}
       \usepackage {amssymb}
    </embedded-tex-preamble> <embedded-tex-body>
         
         \begin {tikzcd}
            &amp; A  \\ 
            X &amp;&amp; Y  \\ 
            &amp; {X \sqcup  Y}
             \arrow [&quot;{i_1}&quot;', from=2-1, to=3-2]
             \arrow [&quot;{i_2}&quot;, from=2-3, to=3-2]
             \arrow [&quot;f&quot;, from=2-1, to=1-2]
             \arrow [&quot;g&quot;', from=2-3, to=1-2]
             \arrow [&quot;{ \exists !}&quot;, dashed, from=3-2, to=1-2]
         \end {tikzcd}   
     
    </embedded-tex-body></embedded-tex></center>

    We might denote the unique as <tex>f \sqcup  g: X  \sqcup  Y  \to  A</tex>.
</p></mainmatter> </tree><p>
    In this section we discuss the <em>limits</em> of variously-shaped diagrams of sets.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1290</anchor> <taxon>Definition</taxon> <addr>def-002L</addr><route>def-002L.xml</route>    <title>Pullback of Sets</title>   </frontmatter> <mainmatter><p>
    Suppose we have sets <tex>X</tex>, <tex>Y</tex>, and <tex>Z</tex> and functions
    <tex>f : X  \to  Z</tex> and <tex>g : Y  \to  Z</tex>.
    
    <center><embedded-tex hash="aea0109fc5888410344cbfd1c2bfad2d"><embedded-tex-preamble>
       \usepackage {tikz-cd}
       \usepackage {amssymb}
    </embedded-tex-preamble> <embedded-tex-body>
         
         \begin {tikzcd}
            &amp; Y  \\ 
            X &amp; Z
             \arrow [&quot;f&quot;', from=2-1, to=2-2]
             \arrow [&quot;g&quot;, from=1-2, to=2-2]
         \end {tikzcd}
     
    </embedded-tex-body></embedded-tex></center>

    Its <strong>fiber product</strong> is the set
    <tex display="block">         X  \times _Z Y =  \{   (x,w,y)  \mid  f(x) = w = g(y)   \}      </tex>
    There are obvious projections 
    <tex>          \pi _1 : X  \times _Z Y  \to  X  \text { and }  \pi _2 : X  \times _Z Y  \to  Y     </tex>
    such that the following diagram commutes (<tex>W = X  \times _Z Y</tex>):
    
    <center><embedded-tex hash="9194755931a1b27a68b010256252d579"><embedded-tex-preamble>
       \usepackage {tikz-cd}
       \usepackage {amssymb}
    </embedded-tex-preamble> <embedded-tex-body>
         
         \begin {tikzcd}
            W &amp; Y  \\ 
            X &amp; Z
             \arrow [&quot;f&quot;', from=2-1, to=2-2]
             \arrow [&quot;g&quot;, from=1-2, to=2-2]
             \arrow [&quot;{ \pi _2}&quot;, from=1-1, to=1-2]
             \arrow [&quot;{ \pi _1}&quot;', from=1-1, to=2-1]
             \arrow [&quot; \lrcorner &quot;{anchor=center, pos=0.125}, draw=none, from=1-1, to=2-2]
         \end {tikzcd}
     
    </embedded-tex-body></embedded-tex></center>

    The <strong>pullback</strong> is defined to be any set <tex>W  \cong  X \times _Z Y</tex>
    The corner symbol indicates <tex>W</tex> is a <em>pullback</em></p></mainmatter> </tree><p>
    The pullback also satisfies the universal property.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1291</anchor> <taxon>Lemma</taxon> <addr>thm-000L</addr><route>thm-000L.xml</route>    <title>Universal Property for Pullback</title>   </frontmatter> <mainmatter><p>
    Suppose the given diagram:
    
    <center><embedded-tex hash="5e6f2da5af96f3d2b5aa340ada59f646"><embedded-tex-preamble>
       \usepackage {tikz-cd}
       \usepackage {amssymb}
    </embedded-tex-preamble> <embedded-tex-body>
         
         \begin {tikzcd}
            &amp; Y  \\ 
            X &amp; Z
             \arrow [&quot;t&quot;', from=2-1, to=2-2]
             \arrow [&quot;u&quot;, from=1-2, to=2-2]
         \end {tikzcd}
     
    </embedded-tex-body></embedded-tex></center>

    For any set <tex>A</tex> and commutative solid arrow diagram as below
    (functions <tex>f:A \to  X</tex> and <tex>g:A \to  Y</tex> such that <tex>t \circ  f = u \circ  g</tex>):
    
    <center><embedded-tex hash="2d3ff6bcdbabe8193a5f90642c805f62"><embedded-tex-preamble>
       \usepackage {tikz-cd}
       \usepackage {amssymb}
    </embedded-tex-preamble> <embedded-tex-body>
         
         \usetikzlibrary {arrows}
         \begin {tikzcd}
            &amp; {X \times _ZY}  \\ 
             \\ 
            &amp; A  \\ 
            X &amp;&amp; Y  \\ 
            &amp; Z
             \arrow [&quot;f&quot;', from=3-2, to=4-1]
             \arrow [&quot;g&quot;, from=3-2, to=4-3]
             \arrow [&quot;t&quot;', from=4-1, to=5-2]
             \arrow [&quot;u&quot;, from=4-3, to=5-2]
             \arrow [&quot;{ \exists !}&quot;, dashed, from=3-2, to=1-2]
             \arrow [&quot;{ \pi _1}&quot;', bend right, from=1-2, to=4-1]
	         \arrow [&quot;{ \pi _2}&quot;, bend left, from=1-2, to=4-3]
         \end {tikzcd}
     
    </embedded-tex-body></embedded-tex></center>

    there exists a <em>unique</em> arrow <tex>\langle  f,g  \rangle _Z: A \to  X \times _Z Y</tex> such that
    <tex display="block">          \pi _1 \circ \langle  f,g  \rangle _Z = f  \text { and }  \pi _2 \circ \langle  f,g  \rangle _Z = g     </tex></p></mainmatter> </tree></mainmatter> </tree><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1292</anchor> <taxon>Math Analysis</taxon> <addr>math-0004</addr><route>math-0004.xml</route>  <date><year>2024</year> <month>1</month> <day>27</day></date>  <title>The Construction of <tex>\mathbb {R}</tex></title>   </frontmatter> <mainmatter><p>
    We start constructing <tex>\mathbb {R}</tex> from <tex>\mathbb {Q}</tex> by a way that it satisfies the existence theorem,
    the core of construction.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1293</anchor> <taxon>Theorem</taxon> <addr>thm-0003</addr><route>thm-0003.xml</route>    <title>Existence theorem</title>   </frontmatter> <mainmatter><p>
    There exists an ordered field <tex>\mathbb {R}</tex> that satisfies the <link href="def-0012.xml" type="local" addr="def-0012" title="Least upper bound property">least upper bound property</link>.
    Moreover <tex>\mathbb {R}</tex> contains <tex>\mathbb {Q}</tex> as a subfield.
</p></mainmatter> </tree><p>
    The least-upper-bound property mentioned above is defined:
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1294</anchor> <taxon>Definition</taxon> <addr>def-0012</addr><route>def-0012.xml</route>    <title>Least upper bound property</title>   </frontmatter> <mainmatter><p>
    A set <tex>S</tex> has the least upper bound property if every non-empty subset <tex>T</tex> of <tex>S</tex> that is bounded above has a least upper bound <tex>\sup  T</tex>.
</p></mainmatter> </tree><p>
    Why do we need the least-upper-bound property?
    Consider the set <tex>S =  \{ x  \in   \mathbb {Q} | x^2 &lt; 2 \}</tex>.
    <tex>S</tex> is bounded above by <tex>2</tex>, but it does not have a least upper bound in <tex>\mathbb {Q}</tex>.
    Therefore we can't express <tex>\sqrt {2}</tex> in field <tex>\mathbb {Q}</tex> since some &quot;gaps&quot; exist.
    This fact motivates us to construct a more complete field <tex>\mathbb {R}</tex>.
    We have constructed <tex>\mathbb {Q}</tex> from <tex>\mathbb {Z}</tex>, and now we construct <tex>\mathbb {R}</tex> from <tex>\mathbb {Q}</tex>.
</p><p>
    Then we should find a way to express &quot;<tex>\sqrt {2}</tex>&quot; using <tex>\mathbb {Q}</tex>.
    A crucial idea is <strong>approximating</strong> <tex>\sqrt {2}</tex> by a sequence of rational numbers.
    <tex display="block">          \sqrt {2} :=  \{  p^2&lt;2  \lor  p&lt;0, p \in \mathbb {Q}  \}      </tex>
    We can cut the number axis into two pieces by <tex>\sqrt {2}</tex>, such cut is called a <strong>Dedekind cut</strong>. 
    A cut should be well-defined rather than just an intuitive concept.
</p><p>
    As we use set theory to construct <tex>\mathbb {R}</tex>, it motivates us to define Dedekind cut as a set.
    It should satisfies some properties:
    <ul><li>Can't be empty or the whole <tex>\mathbb {Q}</tex></li>
        <li>Closed downward</li>
        <li>Contains not the largest number</li></ul>
    A formal definition is given below:
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1295</anchor> <taxon>Definition</taxon> <addr>def-0013</addr><route>def-0013.xml</route>    <title>Dedekind cuts</title>   </frontmatter> <mainmatter><p>
    A Dedekind cut is a partition of the rationals <tex>\mathbb {Q}</tex> into two non-empty sets <tex>L</tex> and <tex>R</tex> such that:
    <ul><li><tex>L \neq \emptyset</tex></li>
        <li><tex>R \neq \emptyset</tex></li>
        <li>if <tex>x,y \in \mathbb {Q}, x&lt;y</tex> and <tex>y \in  L</tex> then <tex>x \in  L</tex></li>
        <li>if <tex>p \in  L</tex> then exists <tex>q \in  L</tex> such that <tex>p&lt;q</tex></li></ul></p></mainmatter> </tree><p>
    Now we can defined the real number as a set of Dedekind cuts.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1296</anchor> <taxon>Definition</taxon> <addr>def-0014</addr><route>def-0014.xml</route>    <title>Real Number System</title>   </frontmatter> <mainmatter><p>
    The element of <tex>\mathbb {R}</tex> is a <link href="def-0013.xml" type="local" addr="def-0013" title="Dedekind cuts">Dedekind Cut</link> in <tex>\mathbb {Q}</tex>.
    <tex display="block">          \mathbb {R} :=  \{  L | (L,R)  \text { is a Dedekind Cut}  \}      </tex></p></mainmatter> </tree><p>
    Now define the order relation on <tex>\mathbb {R}</tex>.
    We have defined <tex>\mathbb {R}</tex> as the set of Dedekind cuts, so we can define the strict partial order relation <tex>&lt;</tex> on <tex>\mathbb {R}</tex> by the set operation <tex>\subset</tex>.
    The irreflexive, asymmetric and transitive properties are trivial. 
</p></mainmatter> </tree><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1297</anchor> <taxon>Linear Algebra</taxon> <addr>math-0001</addr><route>math-0001.xml</route>  <date><year>2024</year> <month>1</month> <day>26</day></date>  <title>Introduction to Vector Space</title>   </frontmatter> <mainmatter><p>
    This note introduces the concept of vector space.
    Refer to <link href="linear-algebra-2015.xml" type="local" addr="linear-algebra-2015" title="Linear Algebra Done Right">Linear Algebra Done Right</link>.
</p><p>
    The motivation for the definition of a vector space comes from the properties
    of vectors in Euclidean space <tex>\mathbb {R}^n</tex> and <tex>\mathbb {C}^n</tex>.
    The definition abstracts and generalizes these properties.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1298</anchor> <taxon>Definition</taxon> <addr>def-000H</addr><route>def-000H.xml</route>    <title>Vector Space</title>   </frontmatter> <mainmatter><p>
    A vector space over a <link href="def-0006.xml" type="local" addr="def-0006" title="Field">field</link> <tex>F</tex> is a non-empty set <tex>V</tex> together with a binary operation and a binary function that satisfy the axioms listed below. 
    In this context, the elements of <tex>F</tex> are commonly called <strong>vectors</strong>, and the elements of <tex>F</tex> are called <strong>scalars</strong>.
    <ul><li>Commutativity: <tex>              \forall  x, y  \in  V, x + y = y + x         </tex></li>
        <li>Associativity: <tex>              \forall  x, y, z  \in  V, (x + y) + z = x + (y + z)         </tex></li>
        <li>Additive Identity: <tex>              \exists  0  \in  V  \text { such that }  \forall  x  \in  V, x + 0 = x         </tex></li>
        <li>Multiplicative Identity: <tex>              \forall  x  \in  V, 1x = x         </tex></li>
        <li>Additive Inverse: <tex>              \forall  x  \in  V,  \exists  y  \in  V  \text { such that } x + y = 0         </tex></li>
        <li>Distributivity: <tex>              \forall  x, y  \in  V,  \forall  c, d  \in  F, c(x + y) = cx + cy, (c + d)x = cx + dx         </tex></li></ul></p><p>
    Elements of a vector space are called <strong>vectors</strong> or <strong>points</strong>.
</p></mainmatter> </tree><p>
    When dealing with vector spaces, we usually interested only in subspaces.
    And the union of subspaces is rarely a subspace, thus
    we are more interested with sums of subspaces.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1299</anchor> <taxon>Definition</taxon> <addr>def-000I</addr><route>def-000I.xml</route>    <title>Linear Subspace</title>   </frontmatter> <mainmatter><p>
    A subset <tex>U</tex> of a vector space <tex>V</tex> over a field <tex>F</tex> is called a <strong>subspace</strong> of <tex>V</tex> if <tex>U</tex> is itself a <strong>vector space</strong> over <tex>F</tex> with the operations of addition and scalar multiplication on <tex>V</tex>.
    The subset also satisfies the following axioms (vice versa):
    <ul><li>Additive identity: <tex>0 \in  U</tex></li>
        <li>Closure: <tex>\forall  u,v \in  U, u+v \in  U</tex></li>
        <li>Closed Scalar multiplication: <tex>\forall  u \in  U,  \forall  c \in  F, cu \in  U</tex></li></ul></p></mainmatter> </tree><p>
    After that we can define the sum of subsets.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1300</anchor> <taxon>Definition</taxon> <addr>def-000J</addr><route>def-000J.xml</route>    <title>Sum of subsets</title>   </frontmatter> <mainmatter><p>
    Let <tex>U_1,  \dots , U_n</tex> be subsets of a vector space <tex>V</tex>.
    The <strong>sum</strong> of <tex>U_1,  \dots , U_n</tex> is defined as
    <tex display="block">U_1 +  \dots  + U_n =  \{ u_1 +  \dots  + u_n  \mid  u_i  \in  U_i \}</tex>.
</p></mainmatter> </tree><p>
    The sum of subspaces is the smallest subspace that contains all the subspaces.
</p><p>
    Every element in <tex>U_1 +  \dots  + U_n</tex> can be written as a sum of elements <tex>u_i</tex> in <tex>U_i</tex>:
    <tex display="block">         u_1+ \cdots +u_n     </tex>
    We will interested in cases where each vector in <tex>U_1 +  \dots  + U_n</tex> can be represented in the form above
    in only one way. This leads to the definition of direct sum.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1301</anchor> <taxon>Definition</taxon> <addr>def-000K</addr><route>def-000K.xml</route>    <title>Direct Sum</title>   </frontmatter> <mainmatter><p>
    Let <tex>U_1,  \dots , U_n</tex> be subspaces of a vector space <tex>V</tex>.
    The <strong>direct sum</strong> of <tex>U_1,  \dots , U_n</tex> is defined as
    <tex display="block">         U_1  \oplus   \dots   \oplus  U_n =  \{ u_1 +  \dots  + u_n  \mid  u_i  \in  U_i \}      </tex>
    if every element in <tex>U_1  \oplus   \dots   \oplus  U_n</tex> can be written as <tex>u_1 +  \dots  + u_n </tex> in only one way.
    This definition requires every vector in the sum have a unique representation.
</p></mainmatter> </tree></mainmatter> </tree><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1302</anchor> <taxon>Linear Algebra</taxon> <addr>math-0002</addr><route>math-0002.xml</route>  <date><year>2024</year> <month>1</month> <day>26</day></date>  <title>Finite Dimensional Vector Space</title>   </frontmatter> <mainmatter><p>
    This note introduces the concept of finite-dimensional vector space.
    Refer to <link href="linear-algebra-2015.xml" type="local" addr="linear-algebra-2015" title="Linear Algebra Done Right">Linear Algebra Done Right</link>.
</p><p>
    Adding up scalar mulitples of vectors in a list gives a linear combination.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1303</anchor> <taxon>Definition</taxon> <addr>def-000L</addr><route>def-000L.xml</route>    <title>Linear Combination</title>   </frontmatter> <mainmatter><p>
    Let <tex>V</tex> be a <link href="def-000H.xml" type="local" addr="def-000H" title="Vector Space">vector space</link> over a field <tex>F</tex>.
    Let <tex>v_1,  \dots , v_n</tex> be vectors in <tex>V</tex>.
    A <strong>linear combination</strong> of <tex>v_1,  \dots , v_n</tex> is an expression of the form
    <tex display="block">         a_1 v_1 +  \dots  + a_n v_n     </tex>
    where <tex>a_1,  \dots , a_n  \in  F</tex>.
</p></mainmatter> </tree><p>
    To talk about a structure, we usually define a collection of this structure.
    Hence we have span for linear combinations.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1304</anchor> <taxon>Definition</taxon> <addr>def-000M</addr><route>def-000M.xml</route>    <title>Linear Span</title>   </frontmatter> <mainmatter><p>
    Let <tex>V</tex> be a vector space over a field <tex>F</tex>.
    Let <tex>v_1,  \dots , v_n</tex> be vectors in <tex>V</tex>.
    The <strong>span</strong> of <tex>v_1,  \dots , v_n</tex> is defined as
    <tex display="block">          \text {span} (v_1,  \dots , v_n) =  \{ a_1 v_1 +  \dots  + a_n v_n  \mid  a_i  \in  F \}      </tex>
    The span of empty set is defined to be <tex>\{ 0 \}</tex>.    
</p><p>
    If <tex>\text {span} (v_1,  \dots , v_n) = V</tex>, we say that <tex>v_1,  \dots , v_n</tex> <strong>spans</strong> <tex>V</tex>.
</p></mainmatter> </tree><p>
    Suppose we have span <tex>S= \text {span} (v_1,  \dots , v_n)</tex>. (Span is trivially a subspace.)
    Obviously for all <tex>v_j (1  \leq  j  \leq  n)</tex>, <tex>v_j  \in  S</tex>.
    Because subspaces are closed under scalar multiplication and addition, every
    subspace of <tex>V</tex> containing <tex>v_1,  \dots , v_n</tex> must contain <tex>S</tex>.
    Thus we conclude that <tex>S</tex> is the smallest subspace containing <tex>v_1,  \dots , v_n</tex>.
</p><p>
    The discussion about <strong>spans</strong> leads to a key definition in linear algebra.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1305</anchor> <taxon>Definition</taxon> <addr>def-000N</addr><route>def-000N.xml</route>    <title>Finite-Dimensional Vector Space</title>   </frontmatter> <mainmatter><p>
    A <link href="def-000H.xml" type="local" addr="def-000H" title="Vector Space">vector space</link> <tex>V</tex> is called <strong>finite-dimensional</strong> if some <link href="def-000G.xml" type="local" addr="def-000G" title="List">list</link> of vectors <tex>v_1,  \dots , v_n</tex> <link href="def-000M.xml" type="local" addr="def-000M" title="Linear Span">spans</link> <tex>V</tex>.
</p></mainmatter> </tree><p>
    The opposite of finite-dimensional is infinite-dimensional.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1306</anchor> <taxon>Definition</taxon> <addr>def-000O</addr><route>def-000O.xml</route>    <title>Infinite-dimensional vector space</title>   </frontmatter> <mainmatter><p>
    A vector space <tex>V</tex> is called <strong>infinite-dimensional</strong> if it is not <link href="def-000N.xml" type="local" addr="def-000N" title="Finite-Dimensional Vector Space">finite-dimensional</link>.
</p></mainmatter> </tree><p>
    Consider the situation that there is only one way to
    express a vector <tex>v</tex> as a linear combination of vectors in a list <tex>v_1,  \dots , v_n</tex>.
    What property of the list <tex>v_1,  \dots , v_n</tex> does this situation imply? The answer is
    linear independence.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1307</anchor> <taxon>Definition</taxon> <addr>def-000P</addr><route>def-000P.xml</route>    <title>Linearly independent</title>   </frontmatter> <mainmatter><p>
    A set of vectors <tex>\{ v_1,  \dots , v_n \}</tex> is called <strong>linearly independent</strong> if
    <tex display="block">a_1 v_1 +  \dots  + a_n v_n = 0</tex>
    implies that <tex>a_1 =  \dots  = a_n = 0</tex>.
    The trivial case of <tex>\{ 0 \}</tex> is also considered linearly independent.
</p></mainmatter> </tree><p>
    If some vectors are not linearly independent, then there are more than one way to
    express a vector as a linear combination of vectors in the list. This leads to 
    the following definition.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1308</anchor> <taxon>Definition</taxon> <addr>def-000Q</addr><route>def-000Q.xml</route>    <title>Linearly dependent</title>   </frontmatter> <mainmatter><p>
    A set of vectors <tex>\{ v_1,  \dots , v_n \}</tex> is called <strong>linearly dependent</strong> if
    <tex display="block">a_1 v_1 +  \dots  + a_n v_n = 0</tex>
    for some <tex>a_1,  \dots , a_n  \in   \mathbb {F}</tex> with at least one <tex>a_i  \neq  0</tex> (not all <tex>0</tex>).
</p></mainmatter> </tree><p>
    The following lemma is a direct consequence of the definition of linear independence.
    It states that for a given linearly dependent list, we can always remove a vector
    without changing the span.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1309</anchor> <taxon>Lemma</taxon> <addr>thm-0001</addr><route>thm-0001.xml</route>    <title>Linear Dependence Lemma</title>   </frontmatter> <mainmatter><p>
    Let <tex>v_1,  \dots , v_n</tex> be vectors in a vector space <tex>V</tex> over a field <tex>\mathbb {F}</tex>.
    If <tex>v_1,  \dots , v_n</tex> are linearly dependent, then there exists <tex>1  \leq  i  \leq  n</tex> such that
    <ul><li><tex>v_i  \in   \text {span} (v_1,  \dots , v_{i-1})</tex></li>
        <li>Remove <tex>v_i</tex> from the list <tex>v_1,  \dots , v_n</tex> and the span does not change</li></ul></p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1310</anchor> <taxon>Lemma</taxon> <addr>thm-0002</addr><route>thm-0002.xml</route>    <title>Length of linearly independent list <tex>\leq</tex> length of spanning list</title>   </frontmatter> <mainmatter><p>
    In a finite dimensional vector space, the length of a linearly independent list is less than or equal to the length of a spanning list.
</p></mainmatter> </tree><p>
    We have discussed linear independent lists and spanning lists.
    Now we are ready to define a basis.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1311</anchor> <taxon>Definition</taxon> <addr>def-000R</addr><route>def-000R.xml</route>    <title>Basis</title>   </frontmatter> <mainmatter><p>
    A basis of <tex>V</tex> is a list of vectors in <tex>V</tex>
    that is linearly independent and spans <tex>V</tex>. 
</p><p><strong>Criterion for basis</strong>
    A list of vectors <tex>\{ v_1,  \dots , v_n \}</tex> is a basis of <tex>V</tex> if and only if
    every <tex>v  \in  V</tex> can be written <strong>uniquely</strong> as a linear combination of <tex>v_1,  \dots , v_n</tex>.
</p></mainmatter> </tree><p>
    For instance, we have standard basis <tex>\{ e_1,  \dots , e_n \}</tex> for <tex>\mathbb {F}^n</tex>,
    where <tex>e_i</tex> is the vector with <tex>1</tex> at <tex>i</tex>-th position and <tex>0</tex> elsewhere.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1312</anchor> <taxon>Theorem</taxon> <addr>thm-0005</addr><route>thm-0005.xml</route>    <title>Spanning List contains a basis</title>   </frontmatter> <mainmatter><p>
    Every spanning list in a vector space can be reduced to a basis.
</p></mainmatter> </tree><p>
    From the <link href="thm-0005.xml" type="local" addr="thm-0005" title="Spanning List contains a basis">theorem</link> we can infer a corollary.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1313</anchor> <taxon>Corollary</taxon> <addr>thm-0006</addr><route>thm-0006.xml</route>    <title>Basis of finite-dimensional vector space</title>   </frontmatter> <mainmatter><p>
    Every finite-dimensional vector space has a basis.
</p></mainmatter> </tree><p>
    The next result states for a spanning list can be reduced to a basis.
    We can adjoin one or more vectors to a linearly independent list to form a basis.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1314</anchor> <taxon>Theorem</taxon> <addr>thm-0007</addr><route>thm-0007.xml</route>    <title>Linearly dependent list extends to a basis</title>   </frontmatter> <mainmatter><p>
    Every linearly independent list of vectors in  a finite-dimensional vector space can be extended to a basis.
</p></mainmatter> </tree><p>
    Remind the definition of <link href="der-000K" type="external">direct sum</link>, we can now show that
    every subspace of a finite-dimensional vecrtor space can be paired
    with another subspace to form a direct sum of the whole space.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1315</anchor> <taxon>Theorem</taxon> <addr>thm-0008</addr><route>thm-0008.xml</route>    <title>Direct Sum of Subspaces of <tex>V</tex></title>   </frontmatter> <mainmatter><p>
    Suppose <tex>V</tex> is a finite dimensional vector space,
    and <tex>U</tex> is a subspace of <tex>V</tex>.
    Then there exists a subspace <tex>W</tex> of <tex>V</tex> such that
    <tex>V = U  \oplus  W</tex>.
</p></mainmatter> </tree><p>
    This post discusses about <em>finite-dimensional vector space</em>.
    But we have not yet defined what is dimension.
    We tempted to define the dimension as the length of basis intuitively.
    With this definition we should prove its well-definedness.
    That is, every basis has the same length.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1316</anchor> <taxon>Theorem</taxon> <addr>thm-0009</addr><route>thm-0009.xml</route>    <title>Basis length is invariant</title>   </frontmatter> <mainmatter><p>
    Let <tex>V</tex> be a finite-dimensional vector space.
    Then every basis of <tex>V</tex> has the same length.
</p></mainmatter> </tree><p>
    This can be proved by <link href="thm-0002.xml" type="local" addr="thm-0002" title="Length of linearly independent list  length of spanning list">Lemma 8</link>.
    Now we can formally define the dimension of such spaces.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1317</anchor> <taxon>Definition</taxon> <addr>def-001V</addr><route>def-001V.xml</route>    <title>Dimension</title>   </frontmatter> <mainmatter><p>
    The <strong>dimension</strong> of a finite-dimensional vector space <tex>V</tex> is the length of any basis of the vector space.
    Denoted by <tex>\dim  V</tex>.
</p></mainmatter> </tree><p>
    Every subspace of a finite-dimensional vector space is also finite-dimensional.
    Hence we can talk about the dimension of a subspace.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1318</anchor> <taxon>Theorem</taxon> <addr>thm-000A</addr><route>thm-000A.xml</route>    <title>Dimension of a subspace</title>   </frontmatter> <mainmatter><p>
    Let <tex>V</tex> be a finite-dimensional vector space,
    and <tex>U</tex> be a subspace of <tex>V</tex>.
    Then <tex>\dim  U  \leq   \dim  V</tex>.
</p></mainmatter> </tree><p>
    According to the definition of <link href="def-000P.xml" type="local" addr="def-000P" title="Linearly independent">linearly independent</link>,
    to show a list of vectors is a basis, we only need to show it is linearly independent,
    and it spans the whole space.
    The next theorems simplifies the task:
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1319</anchor> <taxon>Theorem</taxon> <addr>thm-000B</addr><route>thm-000B.xml</route>    <title>Linearly independent list of the right length is a basis</title>   </frontmatter> <mainmatter><p>
    Let <tex>V</tex> be a finite-dimensional vector space.
    Then every <link href="def-000P.xml" type="local" addr="def-000P" title="Linearly independent">linearly independent</link> list of vectors in <tex>V</tex> with length equal to <tex>\dim  V</tex> is a basis of <tex>V</tex>.
</p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1320</anchor> <taxon>Theorem</taxon> <addr>thm-000C</addr><route>thm-000C.xml</route>    <title>Spanning list of the right length is a basis</title>   </frontmatter> <mainmatter><p>
    Let <tex>V</tex> be a finite-dimensional vector space.
    Then every <link href="def-000M.xml" type="local" addr="def-000M" title="Linear Span">spanning</link> list of vectors in <tex>V</tex> with length equal to <tex>\dim  V</tex> is a basis of <tex>V</tex>.
</p></mainmatter> </tree><p>
    Now we move to the discussion of the dimension of the sum of two subspaces.
    This is analogous to the <link href="thm-000E.xml" type="local" addr="thm-000E" title="Inclusion-Exclusion Principle">inclusion-exclusion principle</link>.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1321</anchor> <taxon>Theorem</taxon> <addr>thm-000D</addr><route>thm-000D.xml</route>    <title>Dimension of a sum</title>   </frontmatter> <mainmatter><p>
    Let <tex>V</tex> be a finite-dimensional vector space,
    and <tex>U</tex> and <tex>W</tex> be subspaces of <tex>V</tex>.
    Then
    <tex display="block">          \dim (U + W) =  \dim  U +  \dim  W -  \dim (U  \cap  W).     </tex></p></mainmatter> </tree></mainmatter> </tree><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1322</anchor> <taxon>Linear Algebra</taxon> <addr>math-0005</addr><route>math-0005.xml</route>  <date><year>2024</year> <month>1</month> <day>31</day></date>  <title>Linear Maps</title>   </frontmatter> <mainmatter><p>
    This note introduces the concept of linear maps.
    Refer to <link href="linear-algebra-2015.xml" type="local" addr="linear-algebra-2015" title="Linear Algebra Done Right">Linear Algebra Done Right</link>.
</p><p>
    Now we arrive at the main topic of this chapter: linear maps. 
    In classic mathematics, to understand the properties of the structure or space,
    we often study the maps between them.
    For vector spaces we study the <strong>linear maps</strong>.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1323</anchor> <taxon>Definition</taxon> <addr>def-0025</addr><route>def-0025.xml</route>    <title>Linear Map</title>   </frontmatter> <mainmatter><p>
    A <strong>linear map</strong> is a function between two vector spaces that preserves the operations of addition and scalar multiplication.
    In other words, a function <tex>T: V  \to  W</tex> where <tex>V,W</tex> are vector spaces if the following conditions are satisfied:
    <ul><li>Additivity: <tex>T(u+v) = T(u) + T(v)</tex> for all <tex>u,v  \in  V</tex></li>
        <li>Homogeneity: <tex>T( \alpha  v) =  \alpha  T(v)</tex> for all <tex>\alpha   \in   \mathbb {F}</tex> and <tex>v  \in  V</tex></li></ul>
    Sometimes we ignore the brackets and write <tex>T v</tex> instead of <tex>T(v)</tex>.
</p></mainmatter> </tree><p>
    Now we can talk about the set of all linear maps between two vector spaces.
    <tex display="block">          \mathcal {L} (V,W) =  \{    T: V  \to  W | T  \text { is a linear map}   \}      </tex></p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1324</anchor> <taxon>Example</taxon> <addr>eg-0002</addr><route>eg-0002.xml</route>    <title>Differentiation is linear map</title>   </frontmatter> <mainmatter><p>
    Define <tex>D \in \mathcal {L} ( \mathcal {P}( \mathbb {R} ), \mathcal {P}( \mathbb {R} ))</tex> (recall that <tex>\mathcal {P}</tex> means <link href="def-0027.xml" type="local" addr="def-0027" title="Polynomial">set of polynomials</link>) by
    <tex display="block">         D(f) = f'     </tex>
    We can see that <tex>D</tex> a linear map.
    <ul><li>Additivity: <tex>D(f+g) = (f+g)' = f' + g' = D(f) + D(g)</tex></li>
        <li>Homogeneity: <tex>D( \alpha  f) = ( \alpha  f)' =  \alpha  f' =  \alpha  D(f)</tex></li></ul></p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1325</anchor> <taxon>Definition</taxon> <addr>eg-0003</addr><route>eg-0003.xml</route>    <title>Integration is linear map</title>   </frontmatter> <mainmatter><p>
    Let <tex>V</tex> be the vector space of all continuous functions on the interval <tex>[a,b]</tex>.
    The map <tex>I: V  \to  V</tex> defined by
    <tex display="block">         I(f) =  \int _a^x f(t) dt     </tex>
    is a <strong>linear map</strong>.
    In other words, <tex>I</tex> preserves the operations of addition and scalar multiplication:
    For all <tex>f,g  \in  V</tex> and all <tex>\alpha   \in   \mathbb {R}</tex>,
    <tex display="block">         I(f+g) = I(f) + I(g)  \quad   \text {and}  \quad  I( \alpha  f) =  \alpha  I(f)     </tex></p></mainmatter> </tree><p>
    We can find a linear map that takes on <em>whatever values we wish</em> on the 
    vectors in a basis by the following theorem.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1326</anchor> <taxon>Theorem</taxon> <addr>thm-000F</addr><route>thm-000F.xml</route>    <title>Linear maps and basis of domain</title>   </frontmatter> <mainmatter><p>
    Let <tex>v_1, v_2,  \ldots , v_n</tex> be a basis of vector space <tex>V</tex>.
    Then for any vector space <tex>W</tex> and any vectors <tex>w_1, w_2,  \ldots , w_n</tex> in <tex>W</tex>,
    there exists a unique linear map <tex>T: V  \to  W</tex> such that
    <tex display="block">         T(v_i) = w_i  \quad   \text {for all}  \quad  i = 1,2, \ldots ,n     </tex></p></mainmatter> </tree><p>
    Now let's turn to the algebraic operations over the set of linear maps <tex>\mathcal {L} (V,W)</tex>.
    We begin by defining the addition and scalar multiplication of linear maps.
    This leads to a surprising result: the set of linear maps is actually a vector space.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1327</anchor> <taxon>Definition</taxon> <addr>def-0029</addr><route>def-0029.xml</route>    <title>Addition and scalar multiplication over <tex>\mathcal {L} (V,W)</tex></title>   </frontmatter> <mainmatter><p>
    Let <tex>T_1, T_2  \in   \mathcal {L} (V,W)</tex>.
    We define the <strong>addition</strong> of <tex>T_1</tex> and <tex>T_2</tex> as the linear map <tex>T_1 + T_2: V  \to  W</tex> such that
    <tex display="block">         (T_1 + T_2)(v) = T_1(v) + T_2(v)  \quad   \text {for all}  \quad  v  \in  V     </tex>
    The scalar multiplication of a linear map <tex>T  \in   \mathcal {L} (V,W)</tex> by a scalar <tex>c  \in   \mathbb {F}</tex> is the linear map <tex>cT: V  \to  W</tex> such that
    <tex display="block">         (cT)(v) = cT(v)  \quad   \text {for all}  \quad  v  \in  V     </tex>
    With these operations, <tex>\mathcal {L} (V,W)</tex> is a <link href="def-000H.xml" type="local" addr="def-000H" title="Vector Space"><strong>vector space</strong></link> over the field <tex>\mathbb {F}</tex>.
    Note that the additive identity of <tex>\mathcal {L} (V,W)</tex> is the <strong>zero map</strong> <tex>0: V  \to  W</tex> such that
    <tex display="block">         0(v) = 0  \quad   \text {for all}  \quad  v  \in  V     </tex></p></mainmatter> </tree><p>
    Usually it makes no sense to multiply two linear maps. But we can define
    an operation called the <strong>product</strong> of linear maps, which is just the composition of the two functions.
    This can form a <strong>monoid</strong> or even a <strong>group</strong> under certain conditions.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1328</anchor> <taxon>Definition</taxon> <addr>def-002A</addr><route>def-002A.xml</route>    <title>Product of Linear Maps</title>   </frontmatter> <mainmatter><p>
    Let <tex>T_1: V  \to  W</tex> and <tex>T_2: W  \to  U</tex> be linear maps.
    We define the <strong>product</strong> of <tex>T_1</tex> and <tex>T_2</tex> as the linear map <tex>T_2  \circ  T_1: V  \to  U</tex> such that
    <tex display="block">         (T_2  \circ  T_1)(v) = T_2(T_1(v))  \quad   \text {for all}  \quad  v  \in  V     </tex>
    Note that this is just the composition of the two functions <tex>T_1</tex> and <tex>T_2</tex>. 
    And we usually denote <tex>T_2  \circ  T_1</tex> by <tex>T_2T_1</tex>.
    The product of linear maps is associative, that is,
    <tex display="block">         (T_3  \circ  T_2)  \circ  T_1 = T_3  \circ  (T_2  \circ  T_1)     </tex>
    for any linear maps <tex>T_1: V  \to  W</tex>, <tex>T_2: W  \to  U</tex>, and <tex>T_3: U  \to  X</tex>.
    The identity map <tex>I_V: V  \to  V</tex> is the identity element of the set of linear maps <tex>\mathcal {L} (V,V)</tex> under the product operation.
    That is, for any linear map <tex>T: V  \to  V</tex>,
    <tex display="block">         I_V  \circ  T = T  \circ  I_V = T     </tex>
    where <tex>I_V</tex> is the identity map on <tex>V</tex>.
    The set of all linear maps from a vector space to itself, <tex>\mathcal {L} (V,V)</tex>, forms a <link href="def-0007.xml" type="local" addr="def-0007" title="Monoid"><strong>monoid</strong></link> under the product operation.
    The set of all invertible linear maps from a vector space to itself, <tex>\mathcal {L} (V,V)^*</tex>, forms a group under the product operation.
    The identity map is the identity element of the <link href="def-0001.xml" type="local" addr="def-0001" title="Group"><strong>group</strong></link> <tex>\mathcal {L} (V,V)^*</tex>.
</p><p>
    With addition we also have the distributive law for the product of linear maps.
    That is, for any linear maps <tex>S,S_1,S_2: V  \to  W</tex> and <tex>T,T_1,T_2: U \to  V</tex>:
    <tex display="block">         (S_1 + S_2)T = S_1T + S_2T  \quad   \text {and}  \quad  T(S_1 + S_2) = TS_1 + TS_2     </tex></p></mainmatter> </tree><p>
    In algebra, we have a structure named <strong>kernel</strong>, which is the set of all elements that are mapped to the zero element.
    For linear maps, the kernel is the <strong>null space</strong></p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1329</anchor> <taxon>Definition</taxon> <addr>def-002C</addr><route>def-002C.xml</route>    <title>Null Space</title>   </frontmatter> <mainmatter><p>
    For <tex>T: V  \to  W</tex>, the <strong>null space</strong> of <tex>T</tex> is the set of all vectors in <tex>V</tex> that are mapped to <tex>0</tex> in <tex>W</tex>.
    <tex display="block">          \text {null }  T =  \{   v  \in  V | T(v) = 0   \}      </tex>
    The null space of <tex>T</tex> is a subspace of <tex>V</tex>.
</p></mainmatter> </tree><p>
    The injective linear map is defined like normal <link href="def-002D.xml" type="local" addr="def-002D" title="Injective">injective</link> functions.
    To check whether a linear map is injective, we can just check whether the null space is trivial.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1330</anchor> <taxon>Theorem</taxon> <addr>thm-000G</addr><route>thm-000G.xml</route>    <title>Injectivity equivalent to Kernel Triviality</title>   </frontmatter> <mainmatter><p>
    Let <tex>T: V  \to  W</tex> be a linear map. Then <tex>T</tex> is injective if and only if <tex>\text {null }  T =  \{   0   \}</tex>.
</p></mainmatter> </tree><p>
    The image of a linear map is the set of all elements that are mapped to by some element in the domain.
    This is called the <strong>range</strong> of the linear map, just like <link href="def-002E.xml" type="local" addr="def-002E" title="Range">range</link> of normal function.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1331</anchor> <taxon>Theorem</taxon> <addr>thm-000H</addr><route>thm-000H.xml</route>    <title>Range is a subspace</title>   </frontmatter> <mainmatter><p>
    If <tex>T: V  \to  W</tex> is a linear map, then the range of <tex>T</tex> is a subspace of <tex>W</tex>.
</p></mainmatter> </tree><p>
    The next theorem plays a crucial role in the study of linear maps.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1332</anchor> <taxon>Theorem</taxon> <addr>thm-000I</addr><route>thm-000I.xml</route>    <title>Fundamental Theorems of Linear Maps</title>   </frontmatter> <mainmatter><p>
    Let <tex>V</tex> be finite-dimensional vector space and <tex>T : V  \to  W</tex> be a linear map. 
    Then <tex>\text {range }  T</tex> is finite-dimensional and 
    <tex display="block">          \dim  V =  \dim   \text {range }  T +  \dim   \text {null }  T     </tex></p></mainmatter> </tree><p>
    Now we can show that no linear map from a finite-dimensional vector space
    to a <em>smaller</em> (In dimension) vector space can be <link href="def-002D.xml" type="local" addr="def-002D" title="Injective">injective</link>.
    This can be easily proved by the fundamental theorem of linear maps.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1333</anchor> <taxon>Lemma</taxon> <addr>thm-000M</addr><route>thm-000M.xml</route>    <title>Map to smaller dimension is not injective</title>   </frontmatter> <mainmatter><p>
    Let <tex>V</tex> and <tex>W</tex> be finite-dimensional vector spaces, 
    and <tex>\dim  V &gt;  \dim  W</tex>.
    Then no linear map <tex>T:V \to  W</tex> is injective.
</p></mainmatter> </tree><p>
    Similarly, we can show that no linear map from a finite-dimensional vector space
    to a <em>larger</em> (In dimension) vector space can be <link href="def-002F.xml" type="local" addr="def-002F" title="Surjective">surjective</link>.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1334</anchor> <taxon>Lemma</taxon> <addr>thm-000N</addr><route>thm-000N.xml</route>    <title>Map to bigger dimension is not surjective</title>   </frontmatter> <mainmatter><p>
    Let <tex>V</tex> and <tex>W</tex> be finite-dimensional vector spaces, 
    and <tex>\dim  V &lt;  \dim  W</tex>.
    Then no linear map <tex>T:V \to  W</tex> is surjective.
</p></mainmatter> </tree><p>
    These two lemmas are very important in the study of linear equations.
    The idea here is to express linear equations system in terms of linear maps.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1335</anchor> <taxon>Example</taxon> <addr>eg-0004</addr><route>eg-0004.xml</route>    <title>Homogeneous Linear Equations System</title>   </frontmatter> <mainmatter><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1336</anchor>      <title>Reprase in terms of a linear map the question of whether a <link href="def-002Q.xml" type="local" addr="def-002Q" title="Homogeneous Linear Equations">homogeneous system linear equations</link> has a nonzero solution.</title>   </frontmatter> <mainmatter><p>
        Let <tex>A</tex> be the coefficient matrix of a homogeneous linear system.
        <tex display="block">             A =  \begin {bmatrix}                 a_{11} &amp; a_{12} &amp;  \cdots  &amp; a_{1n}  \\                  a_{21} &amp; a_{22} &amp;  \cdots  &amp; a_{2n}  \\                   \vdots  &amp;  \vdots  &amp;  \ddots  &amp;  \vdots   \\                  a_{m1} &amp; a_{m2} &amp;  \cdots  &amp; a_{mn}              \end {bmatrix}         </tex>
        The equation <tex>A \vec {x} =  \vec {0}</tex> has a trivial solution <tex>\vec {x} =  \vec {0}</tex>.
        The question here is whether there is a nontrivial solution.
    </p><p>
        Define <tex>T:  \mathbb {F} ^n  \to   \mathbb {F} ^m</tex> by
        <tex display="block">             T( \vec {x}) = A \vec {x}         </tex>
        Then the question of whether the homogeneous linear system has a nontrivial solution is equivalent to 
        asking <tex>\text {null }  T</tex> is nontrivial.
        That is, <tex>T</tex> is <link href="thm-000G.xml" type="local" addr="thm-000G" title="Injectivity equivalent to Kernel Triviality">not injective</link>.
    </p></mainmatter> </tree></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1337</anchor> <taxon>Theorem</taxon> <addr>thm-000O</addr><route>thm-000O.xml</route>    <title>Homogeneous system of linear equations</title>   </frontmatter> <mainmatter><p> 
    A homogeneous system of linear equations
    with more variables than equations has 
    a nontrivial solution.
</p></mainmatter> </tree><p>
    We have seen that <link href="thm-000M.xml" type="local" addr="thm-000M" title="Map to smaller dimension is not injective">map to smaller dimension is not injective</link>.
    <tex>T</tex> is not injective if <tex>n &gt; m</tex>. This results the theorem above.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1338</anchor> <taxon>Example</taxon> <addr>eg-0005</addr><route>eg-0005.xml</route>    <title>Inhomogeneous Linear Equations System</title>   </frontmatter> <mainmatter><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1339</anchor>      <title>Rephrase in terms of a linear map the question of whether a inhomogeneous system linear equations has no solutions
    for some choice of constant terms.</title>   </frontmatter> <mainmatter><p>
    Let <tex>A</tex> be the coefficient matrix of a inhomogeneous linear system.
    <tex display="block">         A =  \begin {bmatrix}             a_{11} &amp; a_{12} &amp;  \cdots  &amp; a_{1n}  \\              a_{21} &amp; a_{22} &amp;  \cdots  &amp; a_{2n}  \\               \vdots  &amp;  \vdots  &amp;  \ddots  &amp;  \vdots   \\              a_{m1} &amp; a_{m2} &amp;  \cdots  &amp; a_{mn}          \end {bmatrix}     </tex>
    The equation <tex>A \vec {x} =  \vec {b}</tex> has a solution <tex>\vec {x} = A^{-1} \vec {b}</tex>.
    </p><p>
        Define <tex>T:  \mathbb {F} ^n  \to   \mathbb {F} ^m</tex> by
        <tex display="block">             T( \vec {x}) = A \vec {x}         </tex>
        Then the statement that inhomogeneous linear system has no solutions is equivalent to 
        <tex>\vec {b}  \not \in   \text {range }  T</tex>.
        Thus the question is rephrased as not having a solution for some choice of <tex>\vec {b}</tex>.
        What condition ensures <tex>T</tex> is not surjective.
    </p></mainmatter> </tree></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1340</anchor> <taxon>Theorem</taxon> <addr>thm-000P</addr><route>thm-000P.xml</route>    <title>Inhomogeneous system of linear equations</title>   </frontmatter> <mainmatter><p>
    An inhomogeneous system of linear equations
    with more equations than variables has 
    no solution for some choice of the constant term.
</p></mainmatter> </tree><p>
    Let <tex>v_1, v_2,  \cdots , v_n</tex> be a basis of <tex>V</tex>.
    We know that for any value of a linear map <tex>T:V \to  W</tex>,
    can be determined by values <tex>\{   T(v_1), T(v_2),  \cdots , T(v_n)   \}</tex>.
    This leads to the definition of the matrix representation of a linear map.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1341</anchor> <taxon>Definition</taxon> <addr>def-002R</addr><route>def-002R.xml</route>    <title>Matrix</title>   </frontmatter> <mainmatter><p>
    Let <tex>m,n \in   \mathbb {Z} ^+</tex>.
    A <tex>m \times  n</tex> matrix is a rectangular array of elements of a field <tex>\mathbb {F}</tex>
    with <tex>m</tex> <strong>rows</strong> and <tex>n</tex> <strong>columns</strong>.
    <tex display="block">         A =  \begin {bmatrix}             a_{11} &amp; a_{12} &amp;  \cdots  &amp; a_{1n}  \\              a_{21} &amp; a_{22} &amp;  \cdots  &amp; a_{2n}  \\               \vdots  &amp;  \vdots  &amp;  \ddots  &amp;  \vdots   \\              a_{m1} &amp; a_{m2} &amp;  \cdots  &amp; a_{mn}          \end {bmatrix}     </tex>
    The notation <tex>A_{jk}</tex> refers to the element in the <tex>j</tex>-th row and <tex>k</tex>-th column.
</p></mainmatter> </tree><p>
    Now we can define the matrix representation of a linear map.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1342</anchor> <taxon>Definition</taxon> <addr>def-002S</addr><route>def-002S.xml</route>    <title>Matrix of Linear Maps</title>   </frontmatter> <mainmatter><p>
    Let <tex>T \in   \mathcal {L} (V,W)</tex>,
    <tex>\{   v_1, \ldots ,v_n   \} \subset  V</tex> be a basis of <tex>V</tex>,
    and <tex>\{   w_1, \ldots ,w_m   \} \subset  W</tex> be a basis of <tex>W</tex>.
    The <strong>matrix of <tex>T</tex></strong> with respect to these bases is
    the <tex>m \times  n</tex> matrix <tex>\mathcal {M} (T)</tex> such that
    <tex display="block">         T(v_j) =  \sum _{i=1}^m  \mathcal {M} (T)_{ij}w_i     </tex>
    Or we denote <tex>\mathcal {M} (T)</tex> as <tex>\mathcal {M} (T, (v_1, \ldots ,v_n), (w_1, \ldots ,w_m))</tex>.
</p><p>
    If <tex>T</tex> maps <tex>n</tex>-dimensional vector space to <tex>m</tex>-dimensional vector space,
    then <tex>\mathcal {M} (T)</tex> is a <tex>m \times  n</tex> matrix.
</p>
    <tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1343</anchor>      <title>
    <strong>Addition</strong>
</title>   </frontmatter> <mainmatter>
    For two same-size matrix <tex>A,B</tex>,
    the sum of <tex>A</tex> and <tex>B</tex> is the matrix <tex>C</tex> such that
    <tex display="block">         C_{ij} = A_{ij} + B_{ij}     </tex>
    In the language of linear maps <tex>S,T \in   \mathcal {L} (V,W)</tex>,
    <tex display="block">          \mathcal {M} (T+S) =  \mathcal {M} (T) +  \mathcal {M} (S)     </tex>
</mainmatter> </tree>

    <tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1344</anchor>      <title>
    <strong>Scalar Multiplication</strong>
</title>   </frontmatter> <mainmatter>
    For a scalar <tex>c</tex> and a matrix <tex>A</tex>,
    the product of <tex>c</tex> and <tex>A</tex> is the matrix <tex>B</tex> such that
    <tex display="block">         B_{ij} = cA_{ij}     </tex>
    In the language of linear maps <tex>T \in   \mathcal {L} (V,W)</tex>,
    <tex display="block">          \mathcal {M} (cT) = c \mathcal {M} (T)     </tex>
</mainmatter> </tree>

    <tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1345</anchor>      <title>
    <strong>Set of Matrices</strong>
</title>   </frontmatter> <mainmatter>
    The set of all <tex>m \times  n</tex> matrices with elements in <tex>\mathbb {F}</tex> is denoted as <tex>\mathcal {M} _{m \times  n}( \mathbb {F} )</tex>
    or <tex>\mathbb {F} ^{m \times  n}</tex>.
</mainmatter> </tree>
</mainmatter> </tree><p>
    We can see that <tex>\mathbb {F} ^{m \times  n}</tex> is itself a vector space.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1346</anchor> <taxon>Theorem</taxon> <addr>thm-000Q</addr><route>thm-000Q.xml</route>    <title><tex>\dim \mathbb {F} ^{m \times  n} = mn</tex></title>   </frontmatter> <mainmatter><p><tex>\mathbb {F} ^{m \times  n}</tex> is a vector space with dimension <tex>mn</tex>.
</p></mainmatter> </tree><p>
    Consider linear maps <tex>T:U \to  V</tex> and <tex>S:V \to  W</tex>.
    The composition of linear maps is <tex>ST</tex>.
    Does the composition of linear maps have a matrix representation?
    <tex display="block">          \mathcal {M} (ST) =  \mathcal {M} (S) \mathcal {M} (T)     </tex>
    This makes no sense now but indicates the definition of <strong>matrix multiplication</strong>.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1347</anchor> <taxon>Definition</taxon> <addr>def-002T</addr><route>def-002T.xml</route>    <title>Matrix Multiplication</title>   </frontmatter> <mainmatter><p>
    Let <tex>A</tex> be a <tex>m \times  n</tex> matrix and <tex>B</tex> be a <tex>n \times  p</tex> matrix.
    Then <tex>AC</tex> is defined as the <tex>m \times  p</tex> matrix <tex>C</tex> such that
    <tex display="block">         C_{ij} =  \sum _{k=1}^n A_{ik}B_{kj}     </tex></p>
    <tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1348</anchor>      <title>
    <strong>Derivation</strong>
</title>   </frontmatter> <mainmatter>
    Let <tex>T:U \to  V</tex> and <tex>S:V \to  W</tex> be linear maps.
    Denote <tex>A =  \mathcal {M} (S)</tex> and <tex>C =  \mathcal {M} (T)</tex>.
    Then the composition of linear maps <tex>ST</tex> is computed
    <tex display="block">          \begin {align*}             (ST)(u)_k &amp;= S( \sum _{r=1}^n C_{rk}v_r)  \\              &amp;=  \sum _{r=1}^n C_{rk}S(v_r)  \\              &amp;=  \sum _{r=1}^n C_{rk} \sum _{s=1}^m A _{sr}w_s  \\              &amp;=  \sum _{s=1}^m \left ( \sum _{r=1}^n C_{rk}A_{sr} \right )w_s  \\           \end {align*}     </tex>
    Thus <tex>\mathcal {M} (ST)</tex> is the <tex>m \times  p</tex> whose entries are
    <tex display="block">          \mathcal {M} (ST)_{sk} =  \sum _{r=1}^n A_{sr}C_{rk}     </tex>
</mainmatter> </tree>
</mainmatter> </tree><p>
    Now we see that the desired matrix multiplication holds.
    Matrix multiplication is not commutative in general.
    However, it satisfies the associative law and the distributive law.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1349</anchor> <taxon>Definition</taxon> <addr>def-002Y</addr><route>def-002Y.xml</route>    <title><tex>A_{j \cdot }</tex> and <tex>A_{ \cdot  j}</tex></title>   </frontmatter> <mainmatter><p>
    Let <tex>A</tex> be a <tex>m \times  n</tex> matrix.
    <ul><li>
            If <tex>1 \leq  j \leq  m</tex> then <tex>A_{j \cdot }</tex> is the <tex>j</tex>-th row of <tex>A</tex>,
            defined as a <tex>1 \times  n</tex> matrix. (A row vector)
        </li>
        <li>
            If <tex>1 \leq  j \leq  n</tex> then <tex>A_{ \cdot  j}</tex> is the <tex>j</tex>-th column of <tex>A</tex>,
            defined as a <tex>m \times  1</tex> matrix. (A column vector)
        </li></ul></p></mainmatter> </tree><p>
    With the notation we can think of matrix multiplication in another perspective.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1350</anchor> <taxon>Lemma</taxon> <addr>thm-000R</addr><route>thm-000R.xml</route>    <title>Entry pf matrix product</title>   </frontmatter> <mainmatter><p>
    Suppose <tex>A</tex> is an <tex>m \times  n</tex> matrix and <tex>B</tex> is an <tex>n \times  p</tex> matrix.
    Then the entry of the product <tex>AB</tex> is:
    <tex display="block">         (AB)_{ij} = A_{i \cdot }B_{ \cdot  j}     </tex>
    for <tex>1 \leq  i \leq  m</tex> and <tex>1 \leq  j \leq  p</tex>.
</p></mainmatter> </tree><p>
    We have an interesting observation.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1351</anchor> <taxon>Lemma</taxon> <addr>thm-000S</addr><route>thm-000S.xml</route>    <title>Linear Combination of columns</title>   </frontmatter> <mainmatter><p>
    Let <tex>A</tex> be an <tex>m \times  n</tex> matrix,
    and <tex>c</tex> is a <tex>1 \times  1</tex> matrix.
    <tex display="block">         c =  \begin {pmatrix} c_1  \\  c_2  \\   \vdots   \\  c_n  \end {pmatrix}     </tex>
    Then <tex>Ac = c_1A_{ \cdot  1} + c_2A_{ \cdot  2} +  \cdots  + c_nA_{ \cdot  n}</tex>.
    In other words, <tex>Ac</tex> is a linear Combination of the columns of <tex>A</tex>,
    with the scalars that multiply the columns coming from <tex>c</tex>.
</p></mainmatter> </tree><p>
    Now we begin the study the invertibility of linear maps.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1352</anchor> <taxon>Definition</taxon> <addr>def-002Z</addr><route>def-002Z.xml</route>    <title>Inverse</title>   </frontmatter> <mainmatter><p>
    A linear map <tex>T \in \mathcal {L} (V,W)</tex> is said to be <tex>invertible</tex> if 
    there exists a linear map <tex>S \in \mathcal {L} (W,V)</tex> such that:
    <tex display="block">          \begin {align*}             T \cdot  S &amp;=  \text {id} _V  \\              S \cdot  T &amp;=  \text {id} _W          \end {align*}     </tex>
    where <tex>\text {id}</tex> is the identity map.
    If a linear map <tex>T</tex> is invertible, 
    then the map <tex>S</tex> is <strong>unique</strong> and is called the <strong>inverse</strong> of <tex>T</tex>, denoted <tex>T^{-1}</tex>.
</p><p>
    An <strong>isomorphism</strong> is a linear map that is invertible.
    Two vector spaces are said to be <strong>isomorphic</strong> if there exists an isomorphism between them.
</p></mainmatter> </tree><p>
    A linear map is invertible if and only if
    it is <strong>bijective</strong>.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1353</anchor> <taxon>Theorem</taxon> <addr>thm-000T</addr><route>thm-000T.xml</route>    <title>Isomorphism of equal dimensions</title>   </frontmatter> <mainmatter><p>
    Two finite-dimensional vector spaces over <tex>\mathbb {F}</tex>
    are isomorphic iff they have the same <link href="def-001V.xml" type="local" addr="def-001V" title="Dimension">dimension</link>.
</p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1354</anchor> <taxon>Theorem</taxon> <addr>thm-000U</addr><route>thm-000U.xml</route>    <title><tex>\mathcal {L} (V,W)</tex> is isomorphic to <tex>\mathbb {F} ^{m \times  n}</tex></title>   </frontmatter> <mainmatter><p>
    Let <tex>v_1, v_2,  \ldots , v_n</tex> be a basis for <tex>V</tex>,
    and <tex>w_1, w_2,  \ldots , w_m</tex> be a basis for <tex>W</tex>.
    Then <tex>\mathcal {M}</tex> is an isomorphism between <tex>\mathcal {L} (V,W)</tex> and <tex>\mathbb {F} ^{m \times  n}</tex>.
</p></mainmatter> </tree><p>
    This has a trivial corollary.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1355</anchor> <taxon>Corollary</taxon> <addr>thm-000V</addr><route>thm-000V.xml</route>    <title>Dimension product</title>   </frontmatter> <mainmatter><p>
    Let <tex>V</tex> and <tex>W</tex> be finite-dimensional vector spaces.
    Then <tex>\mathcal {L} (V,W)</tex> is finite-dimensional and
    <tex display="block">          \dim ( \mathcal {L} (V,W)) =  \dim (V) \dim (W).     </tex></p></mainmatter> </tree></mainmatter> </tree><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1356</anchor> <taxon>Category Theory</taxon> <addr>math-0006</addr><route>math-0006.xml</route>    <title>Category Theory of Utilitarianism</title>   </frontmatter> <mainmatter><p>
    This note is about the category theory and its applications.
    Instead of reading a well-organized book, I prefer to write down the things I learned
    from <link href="ncatlab.xml" type="local" addr="ncatlab" title="NLab">ncatlab</link> and papers.
</p>
    <tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1357</anchor>      <title>
    <strong>Ideas</strong></title>   </frontmatter> <mainmatter>
        Intuitively, a category is a collection of objects and arrows between them,
        such arrows can be composed and there is an identity arrow for each object.
    <p>
        There are commonly two ways to define a category, which are equivalent in usual 
        foundations of mathematics. One of them generalizes the notion of <strong>internal category</strong>
        nicely while the other one is more convenient for <strong>enriched category</strong>.
    </p>
    <p>
        The major difference is whether they use a single collection of all morphisms or
        several collections of morphisms (<strong>family of collections</strong> indexed by pairs of objects)
    </p>
</mainmatter> </tree>
<tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1358</anchor> <taxon>Definition</taxon> <addr>def-003E</addr><route>def-003E.xml</route>    <title>Category</title>   </frontmatter> <mainmatter>
    <tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1359</anchor>      <title><strong>With one collection</strong></title>   </frontmatter> <mainmatter>
    A <strong>category</strong> <tex>C</tex> consists
    <ul><li>
            A collection of <strong>objects</strong> <tex>C_0</tex> (<tex>\text {Ob} (C)</tex>).
        </li>
        <li>
            A collection <tex>C_1</tex> (<tex>\text {Mor} (C)</tex>) of <strong>morphisms</strong> (arrows).
        </li>
        <li>
            For every morphism <tex>f</tex> there are an object <tex>s(f)</tex> (<strong>source</strong>, domain) and an object <tex>t(f)</tex> (<strong>target</strong>, codomain).
        </li>
        <li>
            For every pair of morphisms <tex>f, g</tex> such that <tex>t(f) = s(g)</tex>, there is a morphism <tex>g  \circ  f</tex> (<strong>composition</strong>) (Also written <tex>gf</tex> or <tex>f;g</tex>).
        </li>
        <li>
            For every object <tex>x</tex>, there is a morphism <tex>\text {id} _x</tex> (or <tex>1_x</tex>) called <strong>identity</strong>.
        </li>
        <li>
            The following properties hold:
            <ul><li><tex>s(g  \circ  f) = s(f)</tex> and <tex>t(g  \circ  f) = t(g)</tex>.
                </li>
                <li><tex>s(1_x) = x</tex> and <tex>t(1_x) = x</tex>.
                </li>
                <li>
                    Composition is <strong>associative</strong>: <tex>h  \circ  (g  \circ  f) = (h  \circ  g)  \circ  f</tex> when <tex>t(f) = s(g)</tex>, and <tex>t(g) = s(h)</tex>.
                </li>
                <li>
                    Composition satifies the <strong>identity laws</strong>: <tex>f  \circ   \text {id} _x = f</tex> and <tex>\text {id} _y  \circ  f = f</tex> if <tex></tex></li></ul></li></ul>
    If the identity map and its axioms are omitted then one speaks of a <strong>semicategory</strong>.
</mainmatter> </tree>

    <tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1360</anchor>      <title><strong>With a family of collections of morphisms</strong></title>   </frontmatter> <mainmatter>
    A <strong>category</strong> <tex>C</tex> consists
    <ul><li>
            A collection of <strong>objects</strong> <tex>C_0</tex> (<tex>\text {Ob} (C)</tex>).
        </li>
        <li>
            For every pair of objects <tex>x, y</tex>, a collection <tex>C_1(x, y)</tex> (<tex>\hom _C(x,y)</tex>) of <strong>morphisms</strong> from <tex>x</tex> to <tex>y</tex>.
        </li>
        <li>
            For each pair of morphisms <tex>f</tex> in <tex>C_1(x,y)</tex> and <tex>g</tex> in <tex>C_1(y,z)</tex>, a morphism <tex>g  \circ  f</tex> in <tex>C_1(x,z)</tex>.
            called their <strong>composition</strong>.
        </li>
        <li>
            For each object <tex>x</tex>, a morphism <tex>\text {id} _x</tex> in <tex>C_1(x,x)</tex> called the <strong>identity</strong> on <tex>x</tex>.
        </li>
        <li>
            The following properties hold:
            <ul><li>Composition is <strong>associative</strong>: <tex>h  \circ  (g  \circ  f) = (h  \circ  g)  \circ  f</tex> for all <tex>f</tex> in <tex>C_1(x,y)</tex>, <tex>g</tex> in <tex>C_1(y,z)</tex>, and <tex>h</tex> in <tex>C_1(z,w)</tex>.</li>
                <li>Composition satifies the <strong>identity laws</strong>: <tex>f  \circ   \text {id} _x = f</tex> and <tex>\text {id} _y  \circ  f = f</tex> for all <tex>f</tex> in <tex>C_1(x,y)</tex>.</li></ul></li></ul>
    Usually we write <tex>\text {Mor} (C)</tex> for the disjoint union <tex>\bigsqcup _{x,y  \in  C_0} C_1(x,y)</tex>.
</mainmatter> </tree>
</mainmatter> </tree><p>
    Its common to talk about some objects and their morphisms.
    Informally, a <strong>diagram</strong> in a category <tex>C</tex> consists of some 
    objects of <tex>C</tex> connected by some morphisms of <tex>C</tex>.
</p><p>
    This terminology is often used when speaking about <strong>limits</strong> or 
    <strong>colimits</strong> of a diagram.
</p><p>
    One formal way to define a diagram is to use a <strong>functor</strong> from a (very) small category to <tex>C</tex>.
    That is, a functor whose domain is a small category.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1361</anchor> <taxon>Definition</taxon> <addr>def-003F</addr><route>def-003F.xml</route>    <title>Small Category</title>   </frontmatter> <mainmatter><p>
    A <link href="def-003E.xml" type="local" addr="def-003E" title="Category">category</link> is said to be <strong>small</strong> 
    if it has a <strong>samll set</strong> (i.e. a set but not a proper class) of objects and morphisms.
    In other words a small category is an <strong>internal category</strong> in category of sets.
</p></mainmatter> </tree><p>
    We did not explain what a <strong>functor</strong> is, but it is very natural thought.
    Briefly, a functor is a <strong>homomorphism</strong> between two categories.
    It maps objects to objects and morphisms to morphisms, preserving the structure of the categories.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1362</anchor> <taxon>Definition</taxon> <addr>def-003G</addr><route>def-003G.xml</route>    <title>Functor</title>   </frontmatter> <mainmatter><p>
    A <strong>functor</strong> <tex>F</tex> from a category <tex>C</tex> to a category <tex>D</tex> is a map
    sending each <tex>x \in  C</tex> to an object <tex>F(x) \in  D</tex> and each morphism
    <tex>f:x \to  y</tex> in <tex>C</tex> to morphism <tex>F(f):F(x) \to  F(y)</tex> in <tex>D</tex>, such that 
    <ul><li>
            Composition is preserved: <tex>F(g \circ  f) = F(g) \circ  F(f)</tex>.
        </li>
        <li>
            Identity is preserved: <tex>F( \text {id} _x) =  \text {id} _{F(x)}</tex>.
        </li></ul></p></mainmatter> </tree><p>
    For the sake of completeness, we state the definition of the <strong>functor category</strong>
    and the <strong>natural transformations</strong> between functors.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1363</anchor> <taxon>Definition</taxon> <addr>def-003I</addr><route>def-003I.xml</route>    <title>Functor Category</title>   </frontmatter> <mainmatter><p>
    Let <tex>C</tex> and <tex>D</tex> be categories, the functor category <tex>D^C</tex> 
    (or <tex>[C,D]</tex>) is the category whose
    <ul><li>
            objects are functors from <tex>C</tex> to <tex>D</tex>.
        </li>
        <li>
            morphisms are <strong>natural transformations</strong> between functors.
        </li></ul></p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1364</anchor> <taxon>Definition</taxon> <addr>def-003J</addr><route>def-003J.xml</route>    <title>Natural Transformation</title>   </frontmatter> <mainmatter><p>
    Let <tex>C</tex> and <tex>D</tex> be categories and <tex>F,G:C \to  D</tex> be functors.
    A <strong>natural transformation</strong> <tex>\alpha :F \Rightarrow   G</tex> is 
    an assignment to every object <tex>x \in  C</tex> of a morphism <tex>\alpha _x:F(x) \to  G(x)</tex> in <tex>D</tex>,
    (called the <strong>component</strong> of <tex>\alpha</tex> at <tex>x</tex>)
    the following diagram commutes in <tex>D</tex>:
    
    <center><embedded-tex hash="bce411235fd5c6731abb602d7c12b697"><embedded-tex-preamble>
       \usepackage {tikz-cd}
       \usepackage {amssymb}
    </embedded-tex-preamble> <embedded-tex-body>
         
         \begin {tikzcd}
            {F(x)} &amp;&amp; {F(y)}  \\ 
             \\ 
            {G(x)} &amp;&amp; {G(y)}
             \arrow [&quot;{F(f)}&quot;, from=1-1, to=1-3]
             \arrow [&quot;{G(f)}&quot;, from=3-1, to=3-3]
             \arrow [&quot;{ \alpha _x}&quot;, from=1-1, to=3-1]
             \arrow [&quot;{ \alpha _y}&quot;, from=1-3, to=3-3]
         \end {tikzcd}
     
    </embedded-tex-body></embedded-tex></center></p></mainmatter> </tree><p>
    We state the concise functorial definition of diagrams of the shape of categories.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1365</anchor> <taxon>Definition</taxon> <addr>def-003H</addr><route>def-003H.xml</route>    <title>Diagram</title>   </frontmatter> <mainmatter><p>
    Let <tex>C</tex> be a category and <tex>J</tex> be a small category.
    A <strong>diagram</strong> of shape <tex>J</tex> in <tex>C</tex> is a functor <tex>X:J \to  C</tex>.
    The category of <tex>J</tex>-shaped diagrams in <tex>C</tex> is the <link href="def-003I.xml" type="local" addr="def-003I" title="Functor Category">functor category</link> <tex>C^J</tex>.
</p></mainmatter> </tree><p>
    A limit of a diagram <tex>F:D \to  C</tex> is an object <tex>\lim  F</tex> of <tex>C</tex>
    equipped with morphisms to the objects <tex>F(d)</tex> for all <tex>d \in  D</tex>,
    such that everything in sight commutes.
</p></mainmatter> </tree><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1366</anchor> <taxon>Compute Science</taxon> <addr>cs-0001</addr><route>cs-0001.xml</route>  <date><year>2024</year> <month>1</month> <day>29</day></date>  <title>Is JavaScript an untyped language?</title>   </frontmatter> <mainmatter><p>
    This is a note about the the argument that JavaScript is an untyped language.
    Most opinions came from the References.
</p><p>
    The first thing I want to classify is the word <strong>strong typing</strong> and <strong>weak typing</strong> are meaningless.
    In a limit case we can compare two languages that have similar type system, and talk about which one is <em>stronger</em>.
    But for the common case, it's totally nonsense.
</p><p>
    Static and dynamic typing is a meaningful classsification. But the discussion about dynamic and static languages is mostly wrong on the Internet.
    Dynamic language is a popular concept, however, it is rather a <strong>marketing</strong> than a well-defined terminology.
    It's designed to confuse rather than inform.
</p><p>
    In fact, dynamic typing is just a special case of static typing.
    It limits more than contributes.The root of the problem is the confusion 
    between type and class. It's very useful to have multiple classes of values
    of a same type.
    They are interchangeable because they represent values of the same type.
    Only the form of presentation differs.
</p><p>
    The distinction between two classes of the same type is dynamic.
    But this does not conflict with the fact that only one static type.
    In type theory this is what we called <strong>Sum Type</strong>.
    Being a sum type we can dispatch on the class of the value of the type,
    and decide what to do at runtime.
</p><p>
    This characteristics is same to dynamic language where values can be classified into
    various forms that can be distinguished at runtime.
    The answer is now clear: dynamic language classifies all values in this way.
    What they do just merge all values of the language into a single type.
    The so-called <strong>untyped</strong> language is just <strong>unityped</strong>.
</p><p>
    Therefore, JavaScript is definitely untyped.
</p>
    <p><strong>References</strong></p>
    <ul><li><link href="https://existentialtype.wordpress.com/2011/03/19/dynamic-languages-are-static-languages/" type="external">Dynamic and static language</link></li>
        <li><link href="https://stackoverflow.com/questions/964910/is-javascript-an-untyped-language" type="external">stackoverflow</link></li>
        <li><link href="https://blogs.perl.org/users/ovid/2010/08/what-to-know-before-debating-type-systems.html" type="external">What to know before debating type systems</link></li>
        <li><em>Practical Foundations for Programming Languages</em>, Robert Harper</li></ul>
</mainmatter> </tree><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1367</anchor> <taxon>Compute Science</taxon> <addr>cs-0002</addr><route>cs-0002.xml</route>    <title>Primitive Recursion in Lambda Calculus</title>   </frontmatter> <mainmatter><p> 
    We begin with the <strong>schema of iteration</strong> and then proceed 
    the more complex schema of primitive recursion and general recursion.
    Refer to <link href="tapl.xml" type="local" addr="tapl" title="Types and Programming Languages">TAPL</link>.
</p>
    <tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1368</anchor>      <title><strong>Function Composition</strong></title>   </frontmatter> <mainmatter>
    Giving two functions <tex>f, g</tex> we can compose then to get a new function <tex>f \circ  g = f(g(x))</tex>.
    Using <tex>\lambda</tex>-notation, we can define the composition of two functions as follows:
    <tex display="block">         f \circ  g =  \lambda  x.f(g(x))     </tex>
    And the composition operation is also a lambda abstraction.
    <tex display="block">          \circ  = B =  \lambda  f. \lambda  g. \lambda  x.f(g(x))     </tex>
    Composing identity function with any function does not change the function.
    We expect the following equation to hold:
    <tex display="block">         f \circ  I = f = I \circ  f     </tex>
    where <tex>I</tex> is the identity function. This can be verified by the following calculation:
    <tex display="block">          \begin {align*}             B \space  f \space  I &amp;= ( \lambda  f. \lambda  g. \lambda  x.f(g(x))) \space  f \space  I  \\              &amp; \to _ \beta   \lambda  g. \lambda  x.f(g(x)) \space  I  \\              &amp; \to _ \beta   \lambda  x.f(I(x))  \\              &amp; \to _ \beta   \lambda  x.f(x)  \\              &amp; =_ \eta  f          \end {align*}     </tex>
    The last step requires an extensional equality, which is the called <strong>eta-conversion</strong>.
    <tex display="block">          \text {for} \space  x \not \in \text {FV} (f) , \lambda  x.f(x)  =_ \eta  f     </tex>
    It makes more sense to use the equation from right to left called <strong>eta-expansion</strong> 
    (And more discipline has to be imposed or expansion does not terminate).
</mainmatter> </tree>

    <tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1369</anchor>      <title><strong>Non-termination</strong></title>   </frontmatter> <mainmatter>
    <p>
        The well-known <link href="eg-0007.xml" type="local" addr="eg-0007" title="Divergent Combinator"><strong>divergent combinator</strong></link> implies that 
        the lambda calculus is not strongly normalizing.
    </p>
    <p>
        However, we can always compute a normal form if one exists.
        Though there are many reduction strategies,
        there is a complete one for expressions that have normal form.
        This kind of reduction strategy is called <strong>normal order reduction</strong> or
        <strong>leftmost-outermost reduction</strong>. It scans through the expression from left to right
        and when it find a redex, it reduces it by applying beta reduction and returns to the beginning.
    </p>
    <p>
        The notation of leftmost-outermost reduction is closely related to the 
        notion of <strong>call-by-name evaluation</strong> in programming languages.
        (A little more distance to <strong>call-by-need</strong> evaluation in Haskell)
    </p>
    <p>
        In contrast, <strong>call-by-value</strong> evaluation is not complete, which would 
        reduce the argument of a function before applying beta reduction.
    </p>
</mainmatter> </tree>

    <tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1370</anchor>      <title><strong>Church-Rosser Property</strong></title>   </frontmatter> <mainmatter>
    The outcome of a computation <tex>e</tex> is its normal form.
    It is naturally to ask the question whether the normal form is unique.
    The key to this question is the <strong>Church-Rosser property</strong> or <strong>confluence</strong>:
    If <tex>e \to ^* e_1</tex> and <tex>e \to ^* e_2</tex>, then there exists a term <tex>e_3</tex> such that
    <tex display="block">         e_1 \to ^* e_3 \space \text {and} \space  e_2 \to ^* e_3     </tex>
    This implies the uniqueness of the normal form.
</mainmatter> </tree>

    <tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1371</anchor>      <title><strong>Representing Natural Numbers</strong></title>   </frontmatter> <mainmatter>
    <p>
        We can represent natural numbers in lambda calculus by using the 
        <strong>Church numerals</strong> or <strong>Church encoding</strong>.
        The two abstractions should be related in some ways: 
        one <tex>x</tex> stands for zero and the other <tex>f</tex> stands for the successor function.
    </p>
    <p>
        The Church numeral <tex>n</tex> is a function that takes two arguments <tex>f</tex> and <tex>x</tex> and applies <tex>f</tex> to <tex>x</tex> <tex>n</tex> times.
        The Church numeral <tex>0</tex> is defined as the identity function <tex>\lambda  f. \lambda  x.x</tex>.
        The Church numeral <tex>1</tex> is defined as the successor of <tex>0</tex>:
        <tex display="block">             1 =  \lambda  f. \lambda  x.f(x)         </tex>
        The Church numeral <tex>2</tex> is defined as the successor of <tex>1</tex>:
        <tex display="block">             2 =  \lambda  f. \lambda  x.f(f(x))         </tex>
        And so on.
    </p>
    <p>
        The Church numeral <tex>n</tex> is defined as the successor of <tex>n-1</tex>:
        <tex display="block">             n =  \lambda  f. \lambda  x.f^n(x)         </tex>
        where <tex>f^n(x)</tex> means applying <tex>f</tex> to <tex>x</tex> <tex>n</tex> times.
    </p>
    <p>
        The successor function is defined as follows:
        <tex display="block">             S =  \lambda  n. \lambda  f. \lambda  x.f(n \space  f \space  x)         </tex></p>
    
 
   
   <tree expanded="true" show-heading="true" show-metadata="false" toc="false" numbered="true" root="false"><frontmatter><anchor>1372</anchor> <taxon>Proof</taxon>       <parent>cs-0002</parent> </frontmatter> <mainmatter>
            <tex display="block">                  \begin {align*}                     S \space  n &amp;= ( \lambda  n. \lambda  f. \lambda  x.f(n \space  f \space  x)) \space  n  \\                      &amp; \to _ \beta   \lambda  f. \lambda  x.f(n \space  f \space  x)  \\                      &amp; \to _ \beta   \lambda  f. \lambda  x.f^n(x)  \\                      &amp; \to _ \beta  n+1                  \end {align*}             </tex>
        </mainmatter> </tree>
 

    <p>
        Using the iteration property we can define mathematical functions 
        over the natural numbers in lambda calculus.
        The addition of two Church numerals <tex>m</tex> and <tex>n</tex> is defined as follows:
        <tex display="block">             m+n =  \lambda  n. \lambda  k. n \space  S \space  k         </tex></p>
    <p>
        The multiplication of two Church numerals <tex>m</tex> and <tex>n</tex> is defined by
        iterating the addition function <tex>m</tex> times:
        <tex display="block">             m*n =  \lambda  n. \lambda  k. n \space  (k + )  \space  0         </tex></p>
    <p>
        The exponentiation of two Church numerals <tex>m</tex> and <tex>n</tex> is defined as follows:
        <tex display="block">             m^n =  \lambda  m. \lambda  n. n \space  (m *)  \space  1         </tex></p>
</mainmatter> </tree>

    <tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1373</anchor>      <title><strong>The Schema of Iteration</strong></title>   </frontmatter> <mainmatter>
    <p>
        As we saw before, a natural number <tex>n</tex> is represented by a function 
        that iterates its first argument <tex>n</tex> times on its second argument.
        <tex display="block">             n =  \lambda  g. \lambda  c.g^n(c)         </tex>
        Another way to specify such a function schematically is 
        <tex display="block">              \begin {align*}                 f  \space0  &amp;= c  \\                  f (n+1) &amp;= g \space  (f \space  n)              \end {align*}         </tex>
        If such a function satisfies such a <strong>schema of iteration</strong>, then it can 
        be defined in the lambda calculus on Church numerals as
        <tex display="block">             f =  \lambda  n.n  \space  g  \space  c         </tex>
        This definition is <strong>total</strong> which means it is defined for all natural numbers.
        Let's define the multiplication again
        <tex display="block">              \begin {align*}                 m*0 &amp;= 0  \\                  m*(n+1) &amp;= m + (m*n)              \end {align*}         </tex>
        To fit our schema of iteration, we can define the multiplication by abstracting over <tex>k</tex>:
        <tex display="block">              \begin {align*}                  \text {times} \space  0 &amp;=  \lambda  k.0  \\                   \text {times} \space  (n+1) &amp;=  \lambda  k.k + ( \text {times} \space  n \space  k)              \end {align*}         </tex>
        where the <tex>c</tex> and <tex>g</tex> are
        <tex display="block">              \begin {align*}                 c &amp;=  \lambda  k.0  \\                  g &amp;=  \lambda  r. \lambda  k.k+(r \space  k)              \end {align*}         </tex>
        and we obtain
        <tex display="block">              \text {times} =  \lambda  n.n( \lambda  rk. k + (r \space  k))( \lambda  k.0)         </tex></p>
</mainmatter> </tree>

    <tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1374</anchor>      <title><strong>The Schema of Primitive Recursion</strong></title>   </frontmatter> <mainmatter>
    <p>
        Everything appears simply until we think of a very simple function,
        the <strong>predecessor function</strong> <tex>\text {pred}</tex> defined by
        <tex display="block">              \begin {align*}                  \text {pred} \space  0 = 0  \\                   \text {pred} \space  (n+1) = n              \end {align*}         </tex>
        What we would need is the <strong>schema of primitive recursion</strong>
        <tex display="block">              \begin {align*}                 f \space  0 &amp;= c  \\                  f \space  (n+1) &amp;= g \space  n \space  (f \space  n)              \end {align*}         </tex>
        With which we can define the predecessor function by 
        <tex display="block">             g =  \lambda  x. \lambda  y.x         </tex></p>
    
    
    <tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1375</anchor>      <title><link href="cs-0002.xml" type="local" addr="cs-0002" title="Primitive Recursion in Lambda Calculus">Primitive Recursion in Lambda Calculus</link> › <strong>Define predecessor function</strong></title>  <parent>cs-0002</parent> </frontmatter> <mainmatter>
        The key idea is to gain access to <tex>n</tex> in the schema of 
        primitive recursion by rebuilding it during the iteration.
        <tex display="block">              \text {pred}_2 \space  n =  \langle  n,  \text {pred} \space  n  \rangle          </tex>
        The key step is to express the definition by a schema of iteration
        rather than primitive recursion.
        <tex display="block">              \text {pred}_2 \space  0 =  \langle  0, 0  \rangle          </tex>
        We need a helper function for the successor case
        <tex display="block">              \text {letPair} \space \langle  e_1,e_2 \rangle \space  k = k \space  e_1 \space  e_2         </tex>
        This function passes the elements of the pair to a <strong>continuation</strong> <tex>k</tex>.
        <tex display="block">              \text {pred}_2 (n+1) =  \text {letPair} \space  ( \text {pred}_2 \space  n) \space  ( \lambda  xy.  \langle  x+1, x  \rangle )          </tex>
    </mainmatter> </tree>

    
    <tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1376</anchor>      <title><link href="cs-0002.xml" type="local" addr="cs-0002" title="Primitive Recursion in Lambda Calculus">Primitive Recursion in Lambda Calculus</link> › <strong>Define Pairs</strong></title>  <parent>cs-0002</parent> </frontmatter> <mainmatter>
        Now we need to define pairs and <tex>\text {letPair}</tex>.
        The idea is to simply abstract over the continuation itself.
        <tex display="block">              \begin {align*}                  \langle  x,y \rangle  &amp;=  \lambda  k.k \space  x \space  y  \\                   \text {pair} &amp;=  \lambda  x. \lambda  y. \lambda  k.k \space  x \space  y  \\                    \text {letPair} &amp;=  \lambda  p.p              \end {align*}         </tex>
        The letPair is not really needed here.
    </mainmatter> </tree>

    
    <tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1377</anchor>      <title><link href="cs-0002.xml" type="local" addr="cs-0002" title="Primitive Recursion in Lambda Calculus">Primitive Recursion in Lambda Calculus</link> › <strong>Summary</strong></title>  <parent>cs-0002</parent> </frontmatter> <mainmatter>
        Summarizing the above and we obtain the full definition of the predecessor function.
        <tex display="block">              \begin {align*}                  \text {pred}_2 &amp;=  \lambda  n.n \space  ( \lambda  p.p ( \lambda  xy. \text {pair}  \space  (x+1)  \space  x)) \space   \text {pair} ( \space  0  \space  0) \\                    \text {pred} &amp;=  \lambda  n. ( \text {pred}_2 \space  n)  \space  ( \lambda  xy.y)              \end {align*}            </tex>
    </mainmatter> </tree>

    
    <tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1378</anchor>      <title><link href="cs-0002.xml" type="local" addr="cs-0002" title="Primitive Recursion in Lambda Calculus">Primitive Recursion in Lambda Calculus</link> › <strong>General Primitive Recursion</strong></title>  <parent>cs-0002</parent> </frontmatter> <mainmatter>
        The general case of primitive recursion follows by a similar pattern.
        We begin by defining a function <tex>f_2</tex>:
        <tex display="block">             f_2 \space  n =  \langle  n, f \space  n  \rangle          </tex>
        We can define <tex>f_2</tex> using the schema of iteration
        <tex display="block">              \begin {align*}                 f_2 \space  0 &amp;=  \langle  0, c  \rangle   \\                  f_2 \space  (n+1) &amp;=  \text {letPair} \space  (f_2 \space  n) \space  ( \lambda  xy. \langle  x+1, g \space  x \space  y  \rangle )  \\                  f \space  n &amp;=  \text {letPair} \space  (f_2 \space  n) \space  ( \lambda  xy.y)              \end {align*}         </tex>
    </mainmatter> </tree>

</mainmatter> </tree>
<p>
    When computing over natural numbers we can restrict the functions that can be 
    formed in schematic ways to obtain a language in which all functions <strong>terminate</strong>.
    Because if <tex>c</tex> and <tex>g</tex> are terminating then so is <tex>f</tex> formed from them by primitive recursion.
</p></mainmatter> </tree><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1379</anchor> <taxon>Computer Science</taxon> <addr>cs-0003</addr><route>cs-0003.xml</route>  <date><year>2024</year> <month>3</month> <day>2</day></date>  <title>Recursion</title>   </frontmatter> <mainmatter><p>
    In this note we complete the development of <strong>recursion</strong>.
    Refer to <link href="tapl.xml" type="local" addr="tapl" title="Types and Programming Languages">TAPL</link></p>
  
    
    <tree expanded="true" show-heading="true" show-metadata="false" toc="false" numbered="true" root="false"><frontmatter><anchor>1380</anchor>    <date><year>2024</year> <month>3</month> <day>2</day></date>     </frontmatter> <mainmatter><strong>General Recursion</strong>
    <p>
        Let's first consider the <strong>greatest common divisor</strong> function <tex>\gcd (a,b)</tex></p>
    <tex display="block">          \begin {align*}              \gcd \space  a \space  a &amp;= a  \\               \gcd \space  a \space  b &amp;=  \gcd \space  (a-b) \space  b   \text { if } b &gt; a  \\               \gcd \space  a \space  b &amp;=  \gcd \space  a \space  (b-a)   \text { if } a &gt; b          \end {align*}     </tex>
    This recursion is terminating because the arguments are decreasing.
    We can deal with this case currently and let's be hold.
    We consider the most general schema of recursion.
    <tex display="block">         f = h \space  f     </tex>
    which means that in the right-hand side we can make arbitrary recursive
    calls to the function <tex>f</tex>. For <tex>\gcd</tex> we have
    <tex display="block">         h =  \lambda  gab.  \text {if } (a=b) \space  a \space ((              \text {if } \space (a&gt;b) \space (g \space  (a-b) \space  b) \space (g \space  a \space  (b-a))         ))     </tex>
    How can we define <tex>f</tex> explicitly when given <tex>h</tex> so that <tex>f = h \space  f</tex>,
    which called a <strong>fixed point</strong> pf <tex>h</tex>. If we believe <strong>Church-Turing thesis</strong>,
    then any partial recursive function should be representable on Church numerals in lambda calculus.
    Hence we can find such <tex>f</tex> and the answer is called <strong>Y-combinator</strong>.
    We want that if <tex>f = Y \space  h</tex> and <tex>f=h \space  f</tex>, so we get <tex>Y \space  h = h  \space  (Y \space  h)</tex>.
    <tex display="block">         Y \space  h = h \space  (Y \space  h) = h \space  (h \space  (Y \space  h)) = h \space  (h \space  (h \space  (Y \space  h))) =  \cdots      </tex>
    This iterates infinitely. The definition of <tex>Y</tex> is:
    <tex display="block">         Y =  \lambda  h.( \lambda  x. h \space  (x \space  x)) \space  ( \lambda  x. h \space  (x \space  x))     </tex>
    The application <tex>x \space  x</tex> will replicate <tex>Y \space  h</tex>:
    <tex display="block">          \begin {align*}             Y \space  h &amp;= ( \lambda  x. h \space  (x \space  x)) \space  ( \lambda  x. h \space  (x \space  x))  \\              &amp;= h \space  (( \lambda  x. h \space  (x \space  x)) \space  ( \lambda  x. h \space  (x \space  x)))  \\              &amp;= h \space  (Y \space  h)          \end {align*}     </tex>
    The partial recursive functions include functions that are <strong>undefined</strong> (have no normal form) 
    on some arguments, hence we can't always find an answer.
    Consider <tex>f=f</tex> as a recursion schema and <tex>h= \text {id}</tex>.
    <tex display="block">         Y \space  h = Y \space   \text {id}  = ( \lambda  x.  \text {id} \space  (x \space  x)) \space  ( \lambda  x.  \text {id} \space  (x \space  x))         = ( \lambda  x. x \space  x) \space  ( \lambda  x. x \space  x) =  \Omega      </tex>
    The function <tex>f= \Omega</tex> solves the equation <tex>f=f</tex> by giving a divergent result.
</mainmatter> </tree>
  

  
    
    <tree expanded="true" show-heading="true" show-metadata="false" toc="false" numbered="true" root="false"><frontmatter><anchor>1381</anchor>    <date><year>2024</year> <month>3</month> <day>2</day></date>     </frontmatter> <mainmatter><strong>Define Functions By Recursion</strong>
    <p>Consider the factorial function:</p>
    <tex display="block">          \text {fact} \space  n =  \text {if } (n=0) \space  1 \space  (n \space   \text {fact} \space  (n-1))     </tex>
    This requires a test <tex>\text {if0}</tex> satisfies:
    <tex display="block">          \begin {align*}              \text {if0}(0,c,d) &amp;=c \\               \text {if0}(n+1,c,d) &amp;=d          \end {align*}     </tex>
    We can define <tex>\text {if0}</tex> by (Recall that <tex>K= \lambda  xy.x</tex>):
    <tex display="block">          \text {if0} =  \lambda  ncd. n \space (K \space  d) \space  c     </tex>
    The argument of Y combinator is defined:
    <tex display="block">         h_ \text {fact} =  \lambda  g.  \lambda  n.  \text {if0} \space  n \space  1 \space  (n \space  g \space  (n-1))     </tex>
    and
    <tex display="block">          \text {fact} = Y \space  h_ \text {fact}     </tex>
</mainmatter> </tree>
  
</mainmatter> </tree><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1382</anchor> <taxon>Computer Science</taxon> <addr>cs-0004</addr><route>cs-0004.xml</route>    <title>Scanners</title>   </frontmatter> <mainmatter><p>
    The scanner's task is to transform a stream of characters into a stream 
    of words in the input language. Each word must be classified into a 
    <strong>syntactic category</strong>.
    This note refers to <link href="eng-compiler-2022.xml" type="local" addr="eng-compiler-2022" title="Engineering a Compiler">Engineering a compiler</link></p><p>
    The first stage of a compiler is to perform <strong>lexical analysis</strong> by a scanner.
    The parser or <strong>syntax analyzer</strong> will fit the stream of words to a grammatical 
    model of the input language.
</p><p>
    Scanner construction has a strong foundation in formal language theory.
    Scanners are based on <strong>recognizers</strong> that simulate <strong>deterministic finite automata</strong>.
    We can specify the lexical structure using a set of <strong>regular expression</strong>.
</p><p>
    Each time a scanner recognizes a word, it will return a <strong>token</strong> that
    contains the word (<strong>lexeme</strong>) and its syntactic category.
    The scanner uses <strong>microsyntax</strong> (the lexical structure of a language) to 
    find and classify words. <strong>Keywords</strong> are <strong>identifiers</strong> but have special meanings.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1383</anchor>      <title>A first look at recognizers</title>   </frontmatter> <mainmatter><p>
        A char-by-char algorithm to recognize words is trivial.
        Consider we want to recognize the word <code>new</code>.
        We can write down the following code.
    </p><pre>    c &lt;- nextChar();
    if (c == 'n') {
        c &lt;- nextChar();
        if (c == 'e') {
            c &lt;- nextChar();
            if (c == 'w') {
                return newToken();
            }
        }
    }
    reportError();</pre><p>
        We can also represent the code fragment using the simple <strong>transition diagram</strong>.
    </p>
    <center><embedded-tex hash="6e7465cf950ac87387ac76b9da5fb72d"><embedded-tex-preamble>
       \usepackage {tikz-cd}
       \usepackage {amssymb}
    </embedded-tex-preamble> <embedded-tex-body>
         
         \begin {tikzcd}
            {S_0} &amp; {S_1} &amp; {S_2} &amp; {S_3}
             \arrow [&quot;n&quot;, from=1-1, to=1-2]
             \arrow [&quot;e&quot;, from=1-2, to=1-3]
             \arrow [&quot;w&quot;, from=1-3, to=1-4]
         \end {tikzcd}
     
    </embedded-tex-body></embedded-tex></center>
</mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1384</anchor>      <title>A formalism for recognizers</title>   </frontmatter> <mainmatter><p>
        Transition diagrams can be viewed as formal mathematics objects called <strong>finite automata</strong>.
    </p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1385</anchor> <taxon>Definition</taxon> <addr>def-003M</addr><route>def-003M.xml</route>    <title>Finite Automata</title>   </frontmatter> <mainmatter><p>
    A <strong>finite automata (FA)</strong> is a five-tuple <tex>(S, \Sigma , \delta ,s_0,S_A)</tex> where
    <ul><li><tex>S</tex> is the finite set of states in the recognizer including <tex>s_e</tex>,
            the error state.
        </li>
        <li><tex>\Sigma</tex> is the finite alphabet used by the recognizer. 
            <tex>\Sigma</tex> is the union of the edge labels in the transition diagram.
        </li>
        <li><tex>\delta (s, c)</tex> is the recognizer's transition function, which 
            maps each state <tex>s \in  S</tex> and character <tex>c \in \Sigma</tex> into some next state.
            In state <tex>s_i</tex> with input character <tex>c</tex> the FA takes the transition
            <tex>s_i  \xrightarrow {c}  \delta (s_i, c)</tex>.
        </li>
        <li><tex>s_0 \in  S</tex> is the initial state of the recognizer.
        </li>
        <li><tex>S_A</tex> is the set of accepting states, <tex>S_A \subseteq  S</tex>.
        </li></ul>
    <tex>\delta</tex> is only partially defined. For all other combinations of the 
    state <tex>s_i</tex> and input char <tex>c</tex> we can define <tex>\delta (s_i,c)=s_e</tex>.
</p></mainmatter> </tree><p>
        An FA <strong>accpets</strong> a string <tex>x</tex> and iff starting in <tex>s_0</tex>,
        the sequence of chars in  <tex>x</tex> takes the FA to an accepting state
        when the entire string has been consumed.
    </p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1386</anchor> <taxon>Definition</taxon> <addr>def-003N</addr><route>def-003N.xml</route>    <title>Accepts</title>   </frontmatter> <mainmatter><p>
    If the string <tex>x</tex> consists characters <tex>x_1, x_2,  \ldots , x_n</tex> then the
    FA <tex>(S, \Sigma , \delta ,s_0,S_A)</tex> <strong>accpets</strong> <tex>x</tex> iff there is a sequence
    <tex display="block">          \delta (              \delta (                  \dots \delta ( \delta (                      \delta (s_0,x_1),x_2),x_3) \dots ,                 x_{n-1}             )             ,x_n         ) \in  S_A     </tex></p></mainmatter> </tree></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1387</anchor>      <title>Recognize more complex words</title>   </frontmatter> <mainmatter><p>
        The char-by-char model is very simple and now we consider about numbers.
        For simplicity, we consider only unsigned integers: An unsigned integer is either
        zero or series of one or more digits where the first one is non-zero.
    </p>
    <center><embedded-tex hash="50bfeb9a1f9d7a5d006a58e636b046a2"><embedded-tex-preamble>
       \usepackage {tikz-cd}
       \usepackage {amssymb}
    </embedded-tex-preamble> <embedded-tex-body>
         
         \begin {tikzcd}
            &amp; {S_2}
             \arrow [out=65, in=25 ,loop,&quot;0..9&quot;]
             \\ 
            {S_0}  \\ 
            &amp; {S_1}
             \arrow [&quot;0&quot;, from=2-1, to=3-2]
             \arrow [&quot;{1..9}&quot;, from=2-1, to=1-2]
         \end {tikzcd}
     
    </embedded-tex-body></embedded-tex></center>
<p>
        And the code implementation can be:
    </p><pre>    state &lt;- s0;
    char &lt;-nextChar();
    while (state != se and char != eof) {
        state &lt;- delta(state, char);
        char &lt;- nextChar();
    }
    if (state in SA) {
        return Acceptance();
    } else {
        reportError();
    }</pre><p>
        Another example is to recognize <strong>identifiers</strong> which are sequences of letters and digits,
        starting with a letter. Many languages include other special characters for identifiers.
        The FA for unsigned integers and identifiers are different in syntactic categories.
    </p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1388</anchor>      <title>Regular Expressions</title>   </frontmatter> <mainmatter><p>
        The set of words accpeted by a finite automata <tex>F</tex> forms a language <tex>L(F)</tex>.
        The transition diagram of <tex>F</tex> specifies the syntactic structure of <tex>L(F)</tex>.
        But such representation is complex and non-intuitive.
        Most systems use a notation called <strong>regular expressions</strong> to specify the language.
        Any language described by an RE is considered a <strong>regular language</strong>.
        RE is equivalent to FA.
    </p><p>
        To work with REs in a rigorous way, we need a foraml definition.
    </p></mainmatter> </tree></mainmatter> </tree><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1389</anchor> <taxon>Computer Science</taxon> <addr>cs-0005</addr><route>cs-0005.xml</route>  <date><year>2024</year> <month>3</month> <day>5</day></date>  <title>Simply Typed Lambda Calculus and Representation Theorems</title>   </frontmatter> <mainmatter><p>
    We have explored the power of lambda calculus.
    Church's original purpose of the pure calculus of functions 
    was a new foundations of mathematics distinct from set theory.
    Unfortunately the original lambda calculus is <strong>inconsistent</strong> (Every proposition has a proof).
    Church returned to the ideas by Russell and Whitehead and developed the <strong>Church's Simple Theory of Types</strong>.
    <strong>SLTC</strong> is a typed interpretation of the lambda calculus with only one type constructor <tex>\to</tex>
    that builds function types. 
</p><p>
    We follow the converntion that function type constructor <tex>\to</tex> is right-associative.
    We write <tex>e: \tau</tex> if expression <tex>e</tex> has type <tex>\tau</tex>.
    <tex display="block">          \lambda  x.x: \tau \to \tau      </tex>
    But the type is not unique. The booleans can be typed:
    <tex display="block">          \begin {align*}              \text {true} &amp;=  \lambda  x. \lambda  y.x :  \alpha \to ( \beta \to \alpha ) \\               \text {false} &amp;=  \lambda  x. \lambda  y.y :  \alpha \to ( \beta \to \beta )          \end {align*}     </tex></p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1390</anchor>    <date><year>2024</year> <month>3</month> <day>5</day></date>  <title>Typing Judgment</title>   </frontmatter> <mainmatter><p>
        Wem can formalize judgments about expressions and types using <strong>inference rules</strong>.
        For instance:
        <tex display="block">              \frac {                 e_1: \tau _1 \quad  e_2: \tau _2             }{                 e_1 \space  e_2: \tau _2             }         </tex>
        The application <tex>e_1 \space  e_2</tex> has type <tex>\tau _2</tex> if <tex>e_1</tex> has type 
        <tex>\tau _1 \to \tau _2</tex> and <tex>e_2</tex> has type <tex>\tau _1</tex>.
    </p><p>
        We can record the types of variable in a <strong>typing context</strong>.
        <tex display="block">              \Gamma  : \equiv  x_1: \tau _1, \ldots ,x_n: \tau _n         </tex>
        And we always assume that all variables declared in a context are distinct.
        This avoids any ambiguity when we try to determine the type of a variable.
        The typing judgment now becomes
        <tex display="block">              \Gamma \vdash  e: \tau          </tex></p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1391</anchor>    <date><year>2024</year> <month>3</month> <day>5</day></date>  <title>The Limits of Simple Types</title>   </frontmatter> <mainmatter><p>
        Are there expressions that cannot be typed in the simple type system?
        Yes, for example, <tex>\Omega = \lambda  x.x \space  x</tex> cannot be typed.
        But how do we prove that  <tex>\Omega</tex> cannot be typed?
    </p><p>
        We can apply the typing rules and get a contradiction.
        <tex>\Omega</tex> is a lambda abstraction hence we can assume that it has type <tex>\tau \to \sigma</tex>.
        Then <tex>x</tex> has type <tex>\tau</tex> and <tex>x \space  x</tex> has type <tex>\sigma</tex>.
        By the application of <tex>x \space  x</tex> we get that <tex>\tau = \tau \to \sigma</tex>, 
        which does not exist.
    </p><p>
        To recover from this in full generality we need <strong>recursive types</strong>.
        <tex display="block">              \tau  =F \tau          </tex>
        where <tex>F= \lambda \alpha . \alpha \to \sigma</tex> and we might have a solution with 
        <tex>\tau =Y \space  F</tex>. But such solution is not available to us. We do not have 
        function from types to types <tex>F</tex> and a type level Y combinator.
        However it is ok to construct recursive types (we would do later).
    </p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1392</anchor>    <date><year>2024</year> <month>3</month> <day>5</day></date>  <title>Characterizing the Booleans</title>   </frontmatter> <mainmatter><p>
        We now show that the representation of the booleans is correct.
    </p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1393</anchor>    <date><year>2024</year> <month>3</month> <day>5</day></date>  <title>Representation of Booleans</title>   </frontmatter> <mainmatter><p>
            If <tex>\emptyset \vdash  e: \alpha \to ( \alpha \to \alpha )</tex> and <tex>e</tex> is a normal form, 
            then <tex>e =  \text {true}</tex> or <tex>e =  \text {false}</tex>.
        </p></mainmatter> </tree><p>We will later combine this with the following theorems which yields 
    correctness of the representation of the booleans.</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1394</anchor> <taxon>Definition</taxon> <addr>def-003O</addr><route>def-003O.xml</route>    <title>Weak Normalization</title>   </frontmatter> <mainmatter><p>
    If <tex>\Gamma \vdash  e: \tau</tex> then <tex>e \to _{ \beta }^*e'</tex> for a <strong>normal form</strong> <tex>e'</tex>.
    And we can define <strong>subject reduction</strong>, if <tex>\Gamma \vdash  e: \tau</tex> and <tex>e \to _{ \beta }e'</tex> then <tex>\Gamma \vdash  e': \tau</tex>.
</p></mainmatter> </tree></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1395</anchor>    <date><year>2024</year> <month>3</month> <day>5</day></date>  <title>Reduction revised</title>   </frontmatter> <mainmatter><p>
        Our characterization of normal forms is quite simple: terms that do not reduce.
        However, this is a <strong>negative</strong> condition, which is difficult to work with in proofs.
        We would like to have a <strong>positive</strong> condition, which is easier to work with.
    </p><p>
        We tend to give definitions in the form of inference rules.
        The property then holds if the judgment can be derived using the rules.
        (This closely related to the <strong>inductive deefintion</strong>).
        Before defining the normal forms we formally define <strong>beta reduction</strong>.
        The judgment here <tex>e \to  e'</tex> expressing that <tex>e</tex> reduces to <tex>e'</tex>.
        <tex display="block">              \begin {align*}                  \frac {}{( \lambda  x.e_1)e_2 \to  e_1[x:=e_2]} ( \text {red/beta})  \\                   \frac {e \to  e'}{ \lambda  x.e  \to   \lambda  x.e'} ( \text {red/lam})  \\                   \frac {e_1 \to  e_1'}{e_1 \space  e_2 \to  e_1' \space  e_2} ( \text {red/app}_1)  \\                   \frac {e_2 \to  e_2'}{e_1 \space  e_2 \to  e_1 \space  e_2'} ( \text {red/app}_2)                              \end {align*}         </tex></p><p>
        A <strong>normal form</strong> is an expression that does not reduce.
        To give a proper formalization,, we need a separate judgment for <strong>neutral terms</strong>
        which do not create a redex when applied to an argument.
        <tex display="block">                                        \begin {align*}                  \frac {e \text { normal} }{ \lambda  x.e \text { normal} } ( \text {normal/lam})  \\                   \frac {e \text { neutral} }{e \text { normal} } ( \text {normal/var})  \\                   \frac {}{x \text { neutral} } ( \text {neutral/var})  \\                   \frac {e_1 \text { neutral} \quad  e_2 \text { normal} }{e_1 \space  e_2 \text { neutral} } ( \text {neutral/app})  \\               \end {align*}         </tex></p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1396</anchor>    <date><year>2024</year> <month>3</month> <day>5</day></date>  <title>Normal Forms and Reduction</title>   </frontmatter> <mainmatter><p>
        The characterization of normal forms via inference rules is compact,
        but is it really the same as saying that an expression does not reduce?
        We break this down into the following two properties.
        <ul><li>
                For all expressions <tex>e</tex>, either <tex>e</tex> reduces or <tex>e</tex> is normal
            </li>
            <li>
                For all expressions <tex>e</tex>, it is not that case <tex>e</tex> reduces and <tex>e</tex> is normal
            </li></ul></p><p>
        We have theorem that ensures that the first property holds. (Proof is omitted).
    </p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1397</anchor> <taxon>Theorem</taxon> <addr>thm-0010</addr><route>thm-0010.xml</route>    <title>Reduction and normal forms</title>   </frontmatter> <mainmatter><p>
    For every expression <tex>e</tex>, either <tex>e \to  e'</tex> for some expression <tex>e'</tex> or <tex>e</tex> is a normal form.
</p></mainmatter> </tree></mainmatter> </tree></mainmatter> </tree><tree expanded="false" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1398</anchor> <taxon>Computer Science</taxon> <addr>cs-0006</addr><route>cs-0006.xml</route>  <date><year>2024</year> <month>3</month> <day>11</day></date>  <title>Introduction to Programming Language Semantic</title>   </frontmatter> <mainmatter><p>
    I decided to read one paper or article every week.
    This week's topic is programming language semantics, refer to <strong>Graham Huttons</strong>'s 
    paper <link href="pl-123.xml" type="local" addr="pl-123" title="Programming language semantics: It's easy as 1,2,3">Programming language semantics: It's easy as 1,2,3</link>.
</p><p><strong>Semantics</strong> is the general term for the study of meaning.
    <strong>Programming language semantics</strong> gives precise mathematical meaning to programs.
    We use a simple <strong>arithmetic expression language</strong> 
    (including integers and addition only) to illustrate the basic concepts.
    This is an example of <strong>Occam's razor</strong>, a philosophical principle that favours the 
    simplest explanation for a phenomenon.
</p><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1399</anchor>    <date><year>2024</year> <month>3</month> <day>11</day></date>  <title>Arithmetic Expressions</title>   </frontmatter> <mainmatter><p>
        Now let's define our language of arithmetic expressions
        built up from the set of integers and the operation of addition.
        Use a <strong>context-free</strong> grammar.
        <tex display="block">             E: \equiv \mathbb {Z} | E+E         </tex></p><p>
        An expression is either an integer value or the addition of two sub-expressions.
        We assume that parentheses can be <strong>freely</strong> used as required to disambiguate expressions 
        written in normal textual form. This grammar can be easily translated into 
        a <strong>Haskell</strong> data type.
    </p>
  <pre><code class="language-haskell">data Expr = Val Integer | Add Expr Expr</code></pre>
</mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1400</anchor>    <date><year>2024</year> <month>3</month> <day>11</day></date>  <title>Denotational Semantics</title>   </frontmatter> <mainmatter><p>
        Now we consider <strong>denotational semantics</strong>, 
        where the terms in a language is defined using a 
        <strong>valuation function</strong> that maps terms into values in an appropriate <strong>semantic domain</strong>.
    </p><p>
        Formally, for a language <tex>T</tex> of syntactic terms comprises
        two components: a set <tex>V</tex> of <strong>semantic values</strong> and a 
        <strong>valuation function</strong> of type <tex>T \to  V</tex> that maps terms to 
        their meanings as values.
        This function is written by enclosing a term in a <strong>semantic brackets</strong> 
        (Also known as Oxford or Strachey brackets),
        writing <tex>\llbracket  t \rrbracket</tex> for the value of term <tex>t</tex>.
        In addition, the valuation function is required to be <strong>compositional</strong>,
        the meaning  of a <strong>compound term</strong> is defined purely in terms of the meaning
        of its sub-terms.
    </p><p>
        Compositionality aids understanding by ensuring that the semantics is modular
        and supports the use of simple <strong>equational reasoning</strong> techniques for proving properties of
        the semantics. When the set of semantic values is clear, a denotational semantics is often
        identified with the underlying valuation function.
    </p><p>
        Taking <tex>V</tex> the Haskell type <code>Integer</code> of integers and define a valuation function
        of type <code>Expr -&gt; Integer</code> (by following equations) we can define the denotational semantics of our expression language.
        <tex display="block">              \begin {align*}                  \llbracket \text {Val}   \space  n \rrbracket  &amp;= n  \\                   \llbracket \text {Add}   \space  e_1 \space  e_2 \rrbracket  &amp;=  \llbracket  e_1 \rrbracket  +  \llbracket  e_2 \rrbracket               \end {align*}         </tex>
        This definition satisfies the compositionality requirement obviously.
        Note that the symbol <tex>+</tex> has two different purposes.
        On the left side, it is a <strong>syntactic</strong> constructor for building terms,
        while on the right side, it is a <strong>semantic</strong> operator for adding integers. 
    </p><p>
        Compositionality simplifies reasoning because it allows us to 
        replace <strong>equals by equals</strong>. For example,
        <tex display="block">              \frac {                  \llbracket  e_1 \rrbracket  = n_1  \quad   \llbracket  e_2 \rrbracket  = n_2             }{                  \llbracket \text {Add}   \space  e_1 \space  e_2 \rrbracket  =                  \llbracket \text {Add}   \space  n_1 \space  n_2 \rrbracket              }         </tex>
        we can freely replace the two argument expressions of an addition by other expressions with the same meanings, 
        and the meaning of the whole addition will remain unchanged.
        Using the definition of the valuation function, we can prove this property.
        <tex display="block">              \begin {align*}                  \llbracket \text {Add}   \space  e_1 \space  e_2 \rrbracket  &amp;=                   \llbracket  e_1 \rrbracket  +  \llbracket  e_2 \rrbracket  ( \text {By definition of }  \llbracket- \rrbracket )  \\                  &amp;=  \llbracket  n_1 \rrbracket  +  \llbracket  n_2 \rrbracket  ( \text {Assumptions})  \\                   &amp;=  \llbracket \text {Add}   \space  n_1 \space  n_2 \rrbracket  ( \text {By definition of }  \llbracket- \rrbracket )              \end {align*}         </tex></p><p>
        Given that terms and their semantics are built up <strong>inductively</strong>,
        proofs about denotational semantics typically  proceed using <strong>structural induction</strong>.
        Let us show that our expression semantics is <strong>total</strong>,
        that is, for every expression <tex>e</tex> there is an integer <tex>n</tex> such that <tex>\llbracket  e \rrbracket  = n</tex>.
    </p>
 
   
   <tree expanded="true" show-heading="true" show-metadata="false" toc="false" numbered="true" root="false"><frontmatter><anchor>1401</anchor> <taxon>Proof</taxon>   <date><year>2024</year> <month>3</month> <day>11</day></date>     </frontmatter> <mainmatter>
        For the base case <tex>e =  \text {Val}   \space  n</tex>, we have <tex>\llbracket  e \rrbracket  = n</tex> trivially.
        For the inductive case <tex>e =  \text {Add}   \space  e_1 \space  e_2</tex>,
        we can assume by induction that <tex>\llbracket  e_1 \rrbracket  = n_1</tex> and <tex>\llbracket  e_2 \rrbracket  = n_2</tex>
        for some integers <tex>n_1</tex> and <tex>n_2</tex>. Then <tex>\llbracket  e \rrbracket  = n_1 + n_2</tex> by definition of the valuation function,
        indicates this case is also true. Therefore, the semantics is total.
    </mainmatter> </tree>
 
<p>
        The valuation function can be translated into a Haskell function
    </p>
  <pre><code class="language-haskell">eval :: Expr -&gt; Integer
eval (Val n) = n
eval (Add x y) = eval x + eval y</code></pre>
<p>
        More generally, a denotational semantics can be viewed as an evaluator (or <strong>interpreter</strong>).
        Even <strong>eval</strong> is defined recursively, the semantics is compositional its behavior
        can be understood  as simply replacing the <strong>constructors</strong> for expressions by other functions.
        In this manner, a denotational semantics can also be viewed as an evaluation function that
        is defined by <strong>folding</strong> over the syntax of the source language.
    </p>
  <pre><code class="language-haskell">eval :: Expr -&gt; Integer
eval = fold id (+)</code></pre>
<p>
        The fold operator captures the ideas of replacing constructors
        of the language by other functions
    </p>
  <pre><code class="language-haskell">fold :: (Integer -&gt; a) -&gt; (a -&gt; a -&gt; a) -&gt; Expr -&gt; a 
fold f g (Val n) = f n
fold f g (Add x y) = g (fold f g x) (fold f g y)</code></pre>
<p>
        Note that the above semantics for expressions does not specify the order
        of evaluation. If we do wish to make evaluation order explicit 
        this requires the introduction of additional structure into the semantics,
        named <strong>abstract machines</strong> (Discuss later).
    </p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1402</anchor>    <date><year>2024</year> <month>3</month> <day>11</day></date>  <title>Small-Step Operational Semantics</title>   </frontmatter> <mainmatter><p>
        Another popular approach to semantics is the <strong>operational approach</strong>,
        where the meaning of terms is defined using an <strong>execution relation</strong>
        that specifies how terms can be executed in an appropriate machine model.
        There are two basic forms of operational semantics:
        <ul><li><strong>small-step</strong>: describes the individual steps of execution
            </li>
            <li><strong>big-step</strong>: describes the overall results of execution
            </li></ul>
        In this section we consider the small-step approach, 
        which is also known as <strong>structural operational semantics</strong>.
    </p><p>
        Formally, a small-step operational semantics for a language <tex>T</tex> of syntactic terms
        comprises two components:
        a set <tex>S</tex> of <strong>execution states</strong> and 
        a <strong>transition relation</strong> of type <tex>S \to  S</tex> that specifies how terms can be executed.
        If there is a transition from state <tex>s</tex> to state <tex>s'</tex> in a single execution step, we write <tex>s \to  s'</tex>.
    </p><p>
        Arithmetic expressions have a simple small-step operational semantics,
        given by taking <tex>S</tex> as the Haskell type. And we define transition relation 
        on <code>Expr</code> by the following inference rules.
        <tex display="block">              \begin {align*}                  \frac {}{                      \text {Add} \space ( \text {Val} \space  n_1) \space ( \text {Val} \space  n_2) \to \text {Val} \space (n_1+n_2)                 }  \\                   \frac {x \to  x'}{ \text {Add}   \space  x \space  y \to \text {Add}   \space  x' \space  y}                   \quad                    \frac {y \to  y'}{ \text {Add}   \space  x \space  y \to \text {Add}   \space  x \space  y'}              \end {align*}         </tex></p><p>
        The first rule states that two values can be added to give a single value and is called a
        <strong>reduction</strong> (or <strong>contraction</strong>) rule.
        An expression that matches such a rule is termed a reducible expression or <strong>redex</strong>.
        The last two rules are called <strong>structural</strong> (or <strong>congruence</strong>) rules as 
        they specify how larger terms can be reduced.
    </p><p>
        The semantics is <strong>non-deterministic</strong> because an expression
        may have more than one possible transition.
        This is obviously from the structural rules, which allow either sub-expression to be reduced first.
    </p><p>
        We can now capture a the relation between the denotational and operational semantics,
        namely that making a transition does not change the denotation of an expression.
        <tex display="block">              \frac {                 e \to  e'             }{                  \llbracket e \rrbracket  =  \llbracket e' \rrbracket              }         </tex>
        This property can be proved by induction on the structure of the expression <tex>e</tex>.
        Note that by using the &quot;equals by equals&quot; and the assumption <tex>x \to  x'</tex> we can easily 
        prove the inductive case. The details are omitted here as it involves quite a bit of 
        case analysis. We will later see the <strong>principle of rule induction</strong>, which gives 
        a simpler and more direct way to prove such properties.
    </p><p>
        Evaluation of an expression using the small-step semantics proceeds by a series of zero
        or more transition steps. Formally we can write <tex>e \to ^* e'</tex> to indicate that <tex>e</tex> can be
        reduced to <tex>e'</tex> in zero or more steps.
        We can generate a transition tree that captures all possible execution
        paths for an expression. Using the list comprehension we can define a 
        function that returns the list of all expressions that can be reduced 
        from a given expression <tex>e</tex> in a single transition.
    </p>
  <pre><code class="language-haskell">trans :: Expr -&gt; [Expr]
trans (Val n) = []
trans (Add (Val n) (Val m)) = [Val (n + m)]
trans (Add x y) = [Add x' y | x' &lt;- trans x] ++ [Add x y' | y' &lt;- trans y]</code></pre>
<p>
        We can define a Haskell datatype for transition trees 
        and an execution function that converts expressions into 
        transition trees.
    </p>
  <pre><code class="language-haskell">data Tree a = Node a [Tree a]
exec :: Expr -&gt; Tree Expr
exec e = Node e [exec e' | e' &lt;- trans e]</code></pre>
<p>
        Though <code>exec</code> is defined recursively, its behavior can be understood as simply applying
        the identity function to give the root of the tree and the transition function to generate a 
        list of residual expressions to be processed to give the subtrees.
        A small-step semantics can be viewed as giving rise to an execution
        function that is defined by <strong>unfolding</strong> to transition trees.
    </p>
  <pre><code class="language-haskell">exec :: Expr -&gt; Tree Expr
exec = unfold id trans</code></pre>
<p>
        The <code>unfold</code> function captures the idea of generating a tree 
        from a seed value <tex>x</tex> by applying a function <tex>f</tex> to give the root 
        and a function <tex>g</tex> to give a list of residual values to be processed
        for the subtrees.
    </p>
  <pre><code class="language-haskell">unfold :: (t -&gt; a) -&gt; (t -&gt; [t]) -&gt; t -&gt; Tree a
unfold f g x = Node (f x) [unfold f g x' | x' &lt;- g x]</code></pre>
<p>
        The operational semantics corresponds to <strong>unfolding to transition trees</strong>,
        while denotational semantics corresponds to <strong>folding over syntax trees</strong>.
        Thinking about semantics in terms of recursion operators reveals a duality
    </p><p>
        The above semantics for expressions does not specify the order of evaluation.
        But we can modify the inference rules to achieve this. Replace the second <tex>\text {Add}</tex> 
        rule by the following rule ensures the first argument of addition is 
        always reduced first.
        <tex display="block">              \frac {                 y \to  y'             }{                  \text {Add}  ( \text {Val} \space  n) \space  y  \to   \text {Add}  ( \text {Val} \space  n) \space  y'             }         </tex></p></mainmatter> </tree><tree expanded="true" show-heading="true" show-metadata="false" toc="true" numbered="true" root="false"><frontmatter><anchor>1403</anchor>    <date><year>2024</year> <month>3</month> <day>11</day></date>  <title>Rule induction</title>   </frontmatter> <mainmatter><p>
        For denotational semantics we have structural induction,
        dual to this, for operational semantics we have <strong>rule induction</strong>.
        This allows us to perform proofs by considering the structure of the rules 
        that are used to define the semantics.
    </p><p>
        We introduce the idea of rule induction using a simple numeric example.
        We begin by inductively defining a set of natural numbers.
        <tex display="block">                  \frac {}{0 \in \mathbb {N} }  \quad                    \frac {n \in \mathbb {N} }{n+1 \in \mathbb {N} }         </tex>
        This is the standard definition of the natural numbers using peano axioms,
        where the first rule states that zero is a natural number and the second rule states that
        if <tex>n</tex> is a natural number then so is <tex>n+1</tex>.
    </p><p>
        For the inductively defined set <tex>\mathbb {N}</tex>. The principle of rule induction
        states that in order to prove a property <tex>P(n)</tex> for all natural numbers <tex>n</tex>,
        it suffices to prove that <tex>P(0)</tex> holds and that if <tex>P(n)</tex> holds then <tex>P(n+1)</tex> holds.
        <tex display="block">              \frac {                 P(0) \quad \forall  n \in \mathbb {N} . P(n) \to  P(n+1)             }{                  \forall  n \in \mathbb {N} . P(n)             }         </tex>
        Notice that this is the well-known <strong>principle of mathematical induction</strong>.
    </p><p>
        The concept of rule induction can easily be generalised to multiple base and 
        inductive cases, to rule with multiple preconditions and so on.
        For the small-step semantics of expressions, we have one base case and two inductive cases.
        Hence if we want to show that some property <tex>P(e,e')</tex> on pairs of expression holds for 
        all transition <tex>e \to  e'</tex>, we can use rule induction:
        <tex display="block">              \frac {                  \begin {align*}                     P( \text {Add} \space ( \text {Val} \space  n_1) \space ( \text {Val} \space  n_2), \text {Val} \space (n_1+n_2))  \\                       \forall  x \to  x'. P(x,x') \to  P( \text {Add} \space  x \space  y, \text {Add} \space  x' \space  y)  \\                       \forall  y \to  y'. P(y,y') \to  P( \text {Add} \space  x \space  y, \text {Add} \space  x \space  y')                  \end {align*}             }{                  \forall  e \to  e'. P(e,e')             }         </tex>
        We write <tex>\forall  x \to  y.P(x,y)</tex> as shorthand for 
        <tex display="block">\forall  x,y.x \to  y \Rightarrow  P(x,y)</tex>. Now we give the proof 
        of the property <tex>\llbracket e \rrbracket  =  \llbracket e' \rrbracket</tex> for all transitions <tex>e \to  e'</tex>.
        <tex display="block">              \forall  e \to  e'.  \llbracket e \rrbracket  =  \llbracket e' \rrbracket          </tex></p>
 
   
   <tree expanded="true" show-heading="true" show-metadata="false" toc="false" numbered="true" root="false"><frontmatter><anchor>1404</anchor> <taxon>Proof</taxon>   <date><year>2024</year> <month>3</month> <day>11</day></date>     </frontmatter> <mainmatter>
        The proof consists of three parts: the base case, the reduction rule and the structural rule.
        <tex display="block">              \begin {align*}                  \llbracket \text {Add} \space ( \text {Val} \space  n) \space ( \text {Val} \space  m) \rrbracket                  &amp;=  \llbracket \text {Val} \space  n \rrbracket  +  \llbracket \text {Val} \space  m \rrbracket   \\                  &amp;= n + m  \\                  &amp;=  \llbracket \text {Val} \space (n+m) \rrbracket               \end {align*}         </tex>
        and
        <tex display="block">              \begin {align*}                  \llbracket \text {Add} \space  x \space  y \rrbracket                  &amp;=  \llbracket x \rrbracket  +  \llbracket y \rrbracket    \\                  &amp;=  \llbracket x' \rrbracket  +  \llbracket y \rrbracket  ( \text {By assumption } \llbracket x \rrbracket = \llbracket x' \rrbracket )  \\                  &amp;=  \llbracket \text {Add} \space  x' \space  y \rrbracket               \end {align*}         </tex>
        and 
        <tex display="block">              \begin {align*}                  \llbracket \text {Add} \space  x \space  y \rrbracket                  &amp;=  \llbracket x \rrbracket  +  \llbracket y \rrbracket    \\                  &amp;=  \llbracket x \rrbracket  +  \llbracket y' \rrbracket  ( \text {By assumption } \llbracket y \rrbracket = \llbracket y' \rrbracket )  \\                   &amp;=  \llbracket \text {Add} \space  x \space  y' \rrbracket               \end {align*}         </tex>
    </mainmatter> </tree>
 
</mainmatter> </tree></mainmatter> </tree></mainmatter> </tree></context> <related/> <backlinks/> <references/></backmatter></tree>