<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="true" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>1268</fr:anchor><fr:addr type="user">thm-0011</fr:addr><fr:route>thm-0011.xml</fr:route><fr:title text="Cramer&apos;s rule">Cramer&apos;s rule</fr:title><fr:taxon>Theorem</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Consider a system of <fr:tex display="inline">n</fr:tex> linear equations for <fr:tex display="inline">n</fr:tex> unknowns, represented in matrix multiplication form as follows:
    <fr:tex display="block">         A \cdot  X = B     </fr:tex>
    where <fr:tex display="inline">A</fr:tex> is a square matrix of order <fr:tex display="inline">n</fr:tex>, <fr:tex display="inline">X</fr:tex> is a column matrix of order <fr:tex display="inline">n</fr:tex> and <fr:tex display="inline">B</fr:tex> is a column matrix of order <fr:tex display="inline">n</fr:tex>.
    <fr:tex display="block">         X = \begin {bmatrix} x_1 \\ x_2 \\ \vdots  \\ x_n \end {bmatrix}     </fr:tex>
    The Cramer&apos;s rule states that the solution to the system of equations is given by:
    <fr:tex display="block">         x_i = \frac {\text {det}(A_i)}{\text {det}(A)}     </fr:tex>
    where <fr:tex display="inline">A_i</fr:tex> is the matrix obtained by replacing the <fr:tex display="inline">i</fr:tex>th column of <fr:tex display="inline">A</fr:tex> by <fr:tex display="inline">B</fr:tex>.</fr:p></fr:mainmatter><fr:backmatter><fr:tree toc="false" numbered="false" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:title text="Context">Context</fr:title><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:tree toc="true" numbered="false" show-heading="true" show-metadata="true" expanded="false" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>1269</fr:anchor><fr:addr type="user">math-0008</fr:addr><fr:route>math-0008.xml</fr:route><fr:title text="Matrix Computation">Matrix Computation</fr:title><fr:taxon>Linear Algebra</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>6</fr:month><fr:day>10</fr:day></fr:date><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>This post shows operations and applications over matrix, refers to Wikipedia.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>759</fr:anchor><fr:addr type="user">def-0043</fr:addr><fr:route>def-0043.xml</fr:route><fr:title text="Transpose">Transpose</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>The <fr:strong>transpose</fr:strong> of a matrix <fr:tex display="inline">A</fr:tex>, denoted by <fr:tex display="inline">A^T</fr:tex> is
    the matrix obtained by swapping the rows and columns of <fr:tex display="inline">A</fr:tex>.
    It satisfies the following properties:
    <fr:ul><fr:li><fr:tex display="inline">(A^T)^T = A</fr:tex></fr:li>
        <fr:li><fr:tex display="inline">(A + B)^T = A^T + B^T</fr:tex></fr:li>
        <fr:li><fr:tex display="inline">(cA)^T = cA^T</fr:tex></fr:li>
        <fr:li><fr:tex display="inline">(AB)^T = B^TA^T</fr:tex></fr:li></fr:ul></fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>If the following condition satisfies:
    <fr:tex display="block">         a_{ij} = a_{ji} \quad  \forall  i,j     </fr:tex>
    Then the matrix is called symmetric.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>760</fr:anchor><fr:addr type="user">def-001N</fr:addr><fr:route>def-001N.xml</fr:route><fr:title text="Symmetric Matrix">Symmetric Matrix</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>A square matrix <fr:tex display="inline">A</fr:tex> is symmetric if it is equal to its transpose:
    <fr:tex display="block">         A = A^T     </fr:tex>
    This also implies <fr:tex display="inline">A^{-1} A^T = I</fr:tex></fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>761</fr:anchor><fr:addr type="user">def-0044</fr:addr><fr:route>def-0044.xml</fr:route><fr:title text="Determinant">Determinant</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>The determinant of a <fr:tex display="inline">n\times  n</fr:tex> square matrix <fr:tex display="inline">A</fr:tex> is commonly denoted <fr:tex display="inline">\det  A</fr:tex> or <fr:tex display="inline">|A|</fr:tex>.
    It satisfies the following properties:
    <fr:ul><fr:li><fr:tex display="inline">\det  A^T = \det  A</fr:tex></fr:li>
        <fr:li><fr:tex display="inline">\det  AB = \det  A \det  B</fr:tex></fr:li>
        <fr:li><fr:tex display="inline">\det  \lambda  A = \lambda ^n \det  A</fr:tex></fr:li></fr:ul></fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>To compute the inverse of a matrix, we need <fr:strong>Adjugate matrix</fr:strong>.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>762</fr:anchor><fr:addr type="user">def-0045</fr:addr><fr:route>def-0045.xml</fr:route><fr:title text="First Minor and Cofactor">First Minor and Cofactor</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>If <fr:tex display="inline">A</fr:tex> is a square matrix, then the <fr:strong>minor</fr:strong> of the entry in the i-th row and j-th 
    column (also called the <fr:tex display="inline">(i, j)</fr:tex> minor, or a first minor) is the <fr:strong>determinant</fr:strong> of 
    the sub-matrix formed by deleting the i-th row and j-th column.
    The <fr:tex display="inline">(i, j)</fr:tex> minor is denoted as <fr:tex display="inline">M_{ij}</fr:tex>.
    The <fr:strong>Cofactor</fr:strong> is obtained by multiplying the minor by <fr:tex display="inline">(-1)^{i+j}</fr:tex>.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>763</fr:anchor><fr:addr type="user">def-0046</fr:addr><fr:route>def-0046.xml</fr:route><fr:title text="Cofactor Matrix">Cofactor Matrix</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>The matrix formed by all of the <fr:link type="local" href="def-0045.xml" addr="def-0045" title="First Minor and Cofactor">cofactors</fr:link> of a square matrix <fr:tex display="inline">A</fr:tex> is called the cofactor matrix,
    or <fr:strong>comatrix</fr:strong>:
    <fr:tex display="block">         C = \left [              \begin {array}{cccc}                 C_{11} &amp; C_{12} &amp; \cdots  &amp; C_{1n} \\                 C_{21} &amp; C_{22} &amp; \cdots  &amp; C_{2n} \\                 \vdots  &amp; \vdots  &amp; \ddots  &amp; \vdots  \\                 C_{n1} &amp; C_{n2} &amp; \cdots  &amp; C_{nn}             \end {array}         \right ]     </fr:tex>
    The <fr:strong>Adjugate matrix</fr:strong> of <fr:tex display="inline">A</fr:tex> is the transpose of the cofactor matrix.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>Then the inverse of <fr:tex display="inline">A</fr:tex> is the transpose of the cofactor matrix times the reciprocal of the determinant of <fr:tex display="inline">A</fr:tex>:
    <fr:tex display="block">         A^{-1} = \frac {1}{\det  A} \cdot  \text {adj} A = \frac {1}{\det  A} \cdot  C^T     </fr:tex></fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>764</fr:anchor><fr:addr type="user">def-0047</fr:addr><fr:route>def-0047.xml</fr:route><fr:title text="Singular Matrix">Singular Matrix</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>A square matrix that is not <fr:strong>invertible</fr:strong> is called <fr:strong>singular</fr:strong> or degenerate</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>765</fr:anchor><fr:addr type="machine">#299</fr:addr><fr:route>unstable-299.xml</fr:route><fr:title text="An important property of the inverse of a matrix">An important property of the inverse of a matrix</fr:title><fr:date><fr:year>2024</fr:year><fr:month>6</fr:month><fr:day>10</fr:day></fr:date><fr:authors></fr:authors><fr:parent>math-0008</fr:parent></fr:frontmatter><fr:mainmatter><fr:p><fr:tex display="block">             A \cdot  \text {adj} A = \text {adj} A \cdot  A = \det  A \cdot  I         </fr:tex></fr:p>
 
   
   <fr:tree toc="false" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>766</fr:anchor><fr:addr type="machine">#300</fr:addr><fr:route>unstable-300.xml</fr:route><fr:taxon>Proof</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>6</fr:month><fr:day>10</fr:day></fr:date><fr:authors></fr:authors><fr:parent>#299</fr:parent></fr:frontmatter><fr:mainmatter>
        Let <fr:tex display="inline">A \cdot  \text {adj} A = (b_{ij})</fr:tex> and we have
        <fr:tex display="block">             b_{ij} = a_{i1}A_{j1} + a_{i2}A_{j2} + \cdots  + a_{in}A_{jn} = \delta _{ij} \cdot  \det  A         </fr:tex>
        Hence we have <fr:tex display="inline">A \cdot  \text {adj} A = \det  A \cdot  I</fr:tex> 
    </fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree>
 
</fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>767</fr:anchor><fr:addr type="machine">#301</fr:addr><fr:route>unstable-301.xml</fr:route><fr:title text="Matrix Polynomial and Computation">Matrix Polynomial and Computation</fr:title><fr:date><fr:year>2024</fr:year><fr:month>6</fr:month><fr:day>10</fr:day></fr:date><fr:authors></fr:authors><fr:parent>math-0008</fr:parent></fr:frontmatter><fr:mainmatter><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>768</fr:anchor><fr:addr type="user">def-0048</fr:addr><fr:route>def-0048.xml</fr:route><fr:title text="Matrix Polynomial">Matrix Polynomial</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>A <fr:strong>matrix polynomial</fr:strong> is a polynomial with square matrices as variables.
    The general form of a matrix polynomial is:
    <fr:tex display="block">         P(A) = \sum _{i=0}^{n} a_i A^i     </fr:tex>
    where <fr:tex display="inline">A^0 = I</fr:tex> is the identity matrix.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>If <fr:tex display="inline">A</fr:tex> is a diagonal matrix, then the polynomial of <fr:tex display="inline">A</fr:tex> is the diagonal matrix of the polynomial of the diagonal elements of <fr:tex display="inline">A</fr:tex>.
        <fr:tex display="block">             p(A) = \begin {bmatrix}                 p(a_{11}) &amp; 0 &amp; \cdots  &amp; 0 \\                 0 &amp; p(a_{22}) &amp; \cdots  &amp; 0 \\                 \vdots  &amp; \vdots  &amp; \ddots  &amp; \vdots  \\                 0 &amp; 0 &amp; \cdots  &amp; p(a_{nn})             \end {bmatrix}         </fr:tex></fr:p><fr:p>If <fr:tex display="inline">A = P\Lambda  P^{-1}</fr:tex>, then <fr:tex display="inline">A^k = P \Lambda  ^k P^{-1}</fr:tex> and hence
        <fr:tex display="block">             p(A) = a_0 I + a_1 A + a_2 A^2 + \cdots  + a_n A^n = P \Lambda  P^{-1}         </fr:tex></fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>769</fr:anchor><fr:addr type="machine">#302</fr:addr><fr:route>unstable-302.xml</fr:route><fr:title text="Solving a Linear System">Solving a Linear System</fr:title><fr:date><fr:year>2024</fr:year><fr:month>6</fr:month><fr:day>10</fr:day></fr:date><fr:authors></fr:authors><fr:parent>math-0008</fr:parent></fr:frontmatter><fr:mainmatter><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>770</fr:anchor><fr:addr type="user">thm-0011</fr:addr><fr:route>thm-0011.xml</fr:route><fr:title text="Cramer&apos;s rule">Cramer&apos;s rule</fr:title><fr:taxon>Theorem</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Consider a system of <fr:tex display="inline">n</fr:tex> linear equations for <fr:tex display="inline">n</fr:tex> unknowns, represented in matrix multiplication form as follows:
    <fr:tex display="block">         A \cdot  X = B     </fr:tex>
    where <fr:tex display="inline">A</fr:tex> is a square matrix of order <fr:tex display="inline">n</fr:tex>, <fr:tex display="inline">X</fr:tex> is a column matrix of order <fr:tex display="inline">n</fr:tex> and <fr:tex display="inline">B</fr:tex> is a column matrix of order <fr:tex display="inline">n</fr:tex>.
    <fr:tex display="block">         X = \begin {bmatrix} x_1 \\ x_2 \\ \vdots  \\ x_n \end {bmatrix}     </fr:tex>
    The Cramer&apos;s rule states that the solution to the system of equations is given by:
    <fr:tex display="block">         x_i = \frac {\text {det}(A_i)}{\text {det}(A)}     </fr:tex>
    where <fr:tex display="inline">A_i</fr:tex> is the matrix obtained by replacing the <fr:tex display="inline">i</fr:tex>th column of <fr:tex display="inline">A</fr:tex> by <fr:tex display="inline">B</fr:tex>.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>Matrix partitioning is the process of dividing a matrix into smaller submatrices. 
        This is often done to simplify the computation of matrix operations, such as matrix multiplication.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>771</fr:anchor><fr:addr type="user">def-0049</fr:addr><fr:route>def-0049.xml</fr:route><fr:title text="Matrix Partitioning">Matrix Partitioning</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Let <fr:tex display="inline">A \in  \mathbb {C}^{m\times  n} </fr:tex>. A <fr:strong>partitioning</fr:strong> of <fr:tex display="inline">A</fr:tex> is a representation of <fr:tex display="inline">A</fr:tex> in the form
    <fr:tex display="block">         A = \begin {bmatrix}             A_{11} &amp; A_{12} &amp; \cdots  &amp; A_{1q} \\             A_{21} &amp; A_{22} &amp; \cdots  &amp; A_{2q} \\             \vdots  &amp; \vdots  &amp; \ddots  &amp; \vdots  \\             A_{p1} &amp; A_{p2} &amp; \cdots  &amp; A_{pq}         \end {bmatrix}     </fr:tex>
    where <fr:tex display="inline">A_{ij} \in  \mathbb {C}^{m_i \times  n_j} </fr:tex> for <fr:tex display="inline">1 \leq  i \leq  p</fr:tex> and <fr:tex display="inline">1 \leq  j \leq  q</fr:tex> such that
    <fr:tex display="block">         \sum _{i=1}^p m_i = m \quad  \text {and} \quad  \sum _{j=1}^q n_j = n.     </fr:tex>
    The partitioned matrix operations are similar to the operations on the normal matrix.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>If the partitioned matrix is formed as diagonal blocks, then we can compute the determinant of the matrix by the following formula:
        <fr:tex display="block">             \det  A = \det  A_1 \cdot  \det  A_2 \cdots  \det  A_n         </fr:tex>
        And the inverse of the matrix is
        <fr:tex display="block">             A^{-1} = \begin {bmatrix}                 A_1^{-1} &amp; O &amp; \cdots  &amp; O \\                 O &amp; A_2^{-1} &amp; \cdots  &amp; O \\                 \vdots  &amp; \vdots  &amp; \ddots  &amp; \vdots  \\                 O &amp; O &amp; \cdots  &amp; A_n^{-1}             \end {bmatrix}         </fr:tex></fr:p><fr:p>The column partitioning of matrix is useful. 
        If we have <fr:tex display="inline">m\times  s</fr:tex> matrix <fr:tex display="inline">A = (a_{ij})</fr:tex> and <fr:tex display="inline">s\times  n</fr:tex> matrix <fr:tex display="inline">B=(b_{ij})</fr:tex>,
        their product can be written:
        <fr:tex display="block">             AB = \begin {bmatrix} A_1 \\ A_2 \\ \vdots  A_m \end {bmatrix}             \begin {bmatrix}                 B_1 &amp; B_2 &amp; \cdots  &amp; B_n             \end {bmatrix} =              \begin {bmatrix}                 A_1B_1 &amp; A_1B_2 &amp; \cdots  &amp; A_1B_n \\                 A_2B_1 &amp; A_2B_2 &amp; \cdots  &amp; A_2B_n \\                 \vdots  &amp; \vdots  &amp; \ddots  &amp; \vdots  \\                 A_mB_1 &amp; A_mB_2 &amp; \cdots  &amp; A_mB_n             \end {bmatrix}         </fr:tex>
        We can show that <fr:tex display="inline">A=O\iff  A^TA=O</fr:tex>.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>772</fr:anchor><fr:addr type="machine">#303</fr:addr><fr:route>unstable-303.xml</fr:route><fr:title text="Matrix Transformation">Matrix Transformation</fr:title><fr:taxon>Section</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>6</fr:month><fr:day>10</fr:day></fr:date><fr:authors></fr:authors><fr:parent>math-0008</fr:parent></fr:frontmatter><fr:mainmatter><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>773</fr:anchor><fr:addr type="user">def-004A</fr:addr><fr:route>def-004A.xml</fr:route><fr:title text="Elementary Operations">Elementary Operations</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>There are three types of elementary matrices, which correspond to three types of row operations
    (respectively, column operations, row operations are equivalent to multiplying on the left by the
    corresponding elementary matrix, and column operations are equivalent to multiplying on the right
    by the corresponding elementary matrix):
    <fr:ul><fr:li><fr:strong>Row switching</fr:strong>: A row within the matrix can be switched with another row.
            <fr:tex display="block">                 P_{i,j} = \begin {bmatrix}                     1 \\                     &amp; \ddots  \\                     &amp; &amp; 0 &amp; &amp;  1 \\                      &amp; &amp; &amp; \ddots  \\                      &amp; &amp; 1 &amp; &amp; 0 \\                      &amp; &amp; &amp; &amp; &amp; \ddots  \\                     &amp; &amp; &amp; &amp; &amp; &amp; 1                 \end {bmatrix}             </fr:tex></fr:li>
        <fr:li><fr:strong>Row multiplication</fr:strong>: Each element in a row can be multiplied by a non-zero constant.
            <fr:tex display="block">                 D_i(k) = \text {diag} (1, \cdots , k, \cdots , 1)             </fr:tex></fr:li>
        <fr:li><fr:strong>Row additio</fr:strong>: A row can be replaced by the sum of that row and a multiple of another row.
            <fr:tex display="block">                 T_{i,j} = \begin {bmatrix}                     1 \\                     &amp; \ddots  \\                     &amp; &amp; 1 &amp; &amp; k \\                      &amp; &amp; &amp; \ddots  \\                      &amp; &amp; &amp; &amp; 1 \\                      &amp; &amp; &amp; &amp; &amp; \ddots  \\                     &amp; &amp; &amp; &amp; &amp; &amp; 1                 \end {bmatrix}             </fr:tex></fr:li></fr:ul></fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>774</fr:anchor><fr:addr type="user">def-004B</fr:addr><fr:route>def-004B.xml</fr:route><fr:title text="Column / Row Equivalence">Column / Row Equivalence</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Two matrices <fr:tex display="inline">A,B</fr:tex> are column / row equivalent if one can 
    be obtained from the other by a finite sequence of <fr:link type="local" href="def-004A.xml" addr="def-004A" title="Elementary Operations">elementary operations</fr:link>,
    denoted <fr:tex display="inline">A \sim  B</fr:tex>.
    The column / row equivalence is an <fr:link type="local" href="def-000X.xml" addr="def-000X" title="Equivalence Relation">equivalence relation</fr:link>.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>We can show that a matrix <fr:tex display="inline">A</fr:tex> is invertible iff there are finite elementary matrices
        <fr:tex display="inline">E_1, E_2, \cdots , E_n</fr:tex> such that
        <fr:tex display="block">             A = E_1E_2\cdots  E_n         </fr:tex></fr:p><fr:p>From above we can deduce that a square matrix <fr:tex display="inline">A</fr:tex> is invertible iff <fr:tex display="inline">A\sim  E</fr:tex>.
        This trick can be used for solving a linear system and computing the inverse of a matrix.
        For instance, given <fr:tex display="inline">AX=B</fr:tex> we can solve <fr:tex display="inline">X</fr:tex> by the following steps:
        Let <fr:tex display="inline">P</fr:tex> be a matrix such that <fr:tex display="inline">PA=I</fr:tex> where <fr:tex display="inline">I</fr:tex> is the identity matrix.
        Hence <fr:tex display="inline">P = A^{-1}</fr:tex> and we have <fr:tex display="inline">X = PB</fr:tex>, we can do elementary operations over matrix <fr:tex display="inline">(A, B)</fr:tex>
        to get the solution of <fr:tex display="inline">X</fr:tex>.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>775</fr:anchor><fr:addr type="machine">#304</fr:addr><fr:route>unstable-304.xml</fr:route><fr:title text="Rank">Rank</fr:title><fr:date><fr:year>2024</fr:year><fr:month>6</fr:month><fr:day>10</fr:day></fr:date><fr:authors></fr:authors><fr:parent>math-0008</fr:parent></fr:frontmatter><fr:mainmatter><fr:p>Now let&apos;s talk about the concept of <fr:strong>rank</fr:strong>.
        In linear algebra, the (column)<fr:strong>rank</fr:strong> of a matrix <fr:tex display="inline">A</fr:tex> is the dimension of the vector space 
        generated (or spanned) by its columns.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>776</fr:anchor><fr:addr type="user">def-004J</fr:addr><fr:route>def-004J.xml</fr:route><fr:title text="Rank">Rank</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>The rank of a linear map or operator <fr:tex display="inline">\Phi </fr:tex> is defined as the dimension of its range:
    <fr:tex display="block">         \text {rank}(M)\equiv \dim (\text {range }(M))     </fr:tex>
    Note that the range (<fr:strong>column space</fr:strong>) of a matrix <fr:tex display="inline">M</fr:tex> is the <fr:link type="local" href="def-000M.xml" addr="def-000M" title="Linear Span">span</fr:link> of its column vectors.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>Similarly we can define the row rank of a matrix. A fundamental result in linear algebra is 
        that the column rank and the row rank are always equal. Hence we can simply call it the rank of a matrix.
        If the column rank equals to the column size, we say that the matrix is full column rank.</fr:p><fr:p>A common approach to finding the rank of a matrix is to reduce it to a simpler form, 
        generally <fr:strong>row echelon form</fr:strong>, by elementary row operations.
        Row operations do not change the row space (hence do not change the row rank), and, being 
        invertible, map the column space to an isomorphic space (hence do not change the column rank).
        <fr:tex display="block">             A \sim  B \implies  \text {rank}(A) = \text {rank}(B)             \\              \text {rank}(A) = \text {rank}(A^T)         </fr:tex>
        Once in row echelon form, the rank is clearly the same for both row rank and column rank, 
        and equals to the number of <fr:strong>pivots</fr:strong> (or basic columns) and also the number of non-zero rows.</fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>777</fr:anchor><fr:addr type="user">def-004I</fr:addr><fr:route>def-004I.xml</fr:route><fr:title text="Row Echelon Form">Row Echelon Form</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>A matrix is in <fr:strong>row echelon form</fr:strong> if
    <fr:ul><fr:li>All rows having only zero entries are at the bottom</fr:li>
        <fr:li>The leading entry (that is, the <fr:strong>left-most</fr:strong> nonzero entry) of every nonzero row, called
            the <fr:strong>pivot</fr:strong>, is on the right of the leading entry of every row above.</fr:li></fr:ul>
    Some texts add the condition that the leading coefficient must be <fr:tex display="inline">1</fr:tex>
    while others require this only in <fr:strong>reduced row echelon form</fr:strong>.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>For instance, matrix <fr:tex display="inline">A = \begin {bmatrix}1&amp;2&amp;1\\-2&amp;-3&amp;1\\3&amp;5&amp;0\end {bmatrix}</fr:tex> can be transformed into reduced row-echelon form:
        <fr:tex display="block">\begin {aligned}{\begin {bmatrix}1&amp;2&amp;1\\-2&amp;-3&amp;1\\3&amp;5&amp;0\end {bmatrix}}&amp;\xrightarrow  {2R_{1}+R_{2}\to  R_{2}} {\begin {bmatrix}1&amp;2&amp;1\\0&amp;1&amp;3\\3&amp;5&amp;0\end {bmatrix}}\xrightarrow  {-3R_{1}+R_{3}\to  R_{3}} {\begin {bmatrix}1&amp;2&amp;1\\0&amp;1&amp;3\\0&amp;-1&amp;-3\end {bmatrix}}\\&amp;\xrightarrow  {R_{2}+R_{3}\to  R_{3}} \,\,{\begin {bmatrix}1&amp;2&amp;1\\0&amp;1&amp;3\\0&amp;0&amp;0\end {bmatrix}}\xrightarrow  {-2R_{2}+R_{1}\to  R_{1}} {\begin {bmatrix}1&amp;0&amp;-5\\0&amp;1&amp;3\\0&amp;0&amp;0\end {bmatrix}}~.\end {aligned}</fr:tex>
        The final matrix has two non-zero rows and thus the rank of matrix <fr:tex display="inline">A</fr:tex> is <fr:tex display="inline">2</fr:tex>.</fr:p><fr:p>The determinantal rank of a matrix is the order of the largest non-zero minor of the matrix.
        It is also the number of non-zero eigenvalues of the matrix. 
        This does not give an efficient way of computing the rank, but it is useful theoretically: 
        a single non-zero minor witnesses a lower bound for the rank of the matrix,
        which can be useful to prove that certain operations do not lower the rank of a matrix.</fr:p><fr:p>We can conclude the properties of rank:
        <fr:tex display="block">             \begin {align*}                 &amp;0 \leq  \text {rank}(A_{m\times  n}) \leq  \min (m,n) \\                 &amp;\text {rank}(A^T) = \text {rank}(A) \\                  &amp; A \sim  B \implies  \text {rank}(A)=\text {rank}(B) \\                  &amp; P, Q \text { is invertible} \implies  \text {rank}(PAQ) = \text {rank}(A) \\                  &amp; \max (\text {rank}(A), \text {rank}(B)) \leq  \text {rank}(A, B) \leq  \text {rank}(A) + \text {rank}(B) \\                  &amp; \text {rank}(A+B) \leq  \text {rank}(A) + \text {rank}(B) \\                 &amp; \text {rank}(AB) \leq  \min (\text {rank}(A), \text {rank}(B)) \\                  &amp; A_{m\times  n}B_{n\times  l} = O\implies  \text {rank}(A) + \text {rank}(B) \leq  n             \end {align*}         </fr:tex></fr:p><fr:p>With such properties we can prove an important theorem:
        <fr:tex display="block">             AB = O \land  A \text { is full rank} \implies  B = O         </fr:tex>
        which is known as the cancellation law of matrix multiplication.</fr:p><fr:p>The rank of a matrix is also related to the solution of a linear system.
        If the rank of the coefficient matrix is less than the rank of the augmented matrix, 
        then the system is inconsistent. Hence there does not exist a solution.
        Similarly we have the following discussion: For a <fr:tex display="inline">n</fr:tex>-variable linear system <fr:tex display="inline">Ax = b</fr:tex>
        <fr:ul><fr:li>If <fr:tex display="inline">\text {rank}(A) &lt; \text {rank}(A,b)</fr:tex>, the solution does not exist.</fr:li>
            <fr:li>If <fr:tex display="inline">\text {rank}(A)=\text {rank}(A,b)=n</fr:tex>, there is a unique solution.</fr:li>
            <fr:li>Ig <fr:tex display="inline">\text {rank}(A)=\text {rank}(A,b)&lt;n</fr:tex>, there are infinite solutions.</fr:li></fr:ul>
        This leads two fundamental theorems of linear systems:
        <fr:ul><fr:li><fr:tex display="inline">n</fr:tex>-variable regular linear system <fr:tex display="inline">Ax = 0</fr:tex> has a non-trivial solution iff <fr:tex display="inline">\text {rank}(A) &lt; n</fr:tex>.</fr:li>
            <fr:li>Matrix equation <fr:tex display="inline">AX = B</fr:tex> has solutions iff <fr:tex display="inline">\text {rank}(A) = \text {rank}(A,B)</fr:tex>.</fr:li></fr:ul></fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>778</fr:anchor><fr:addr type="machine">#305</fr:addr><fr:route>unstable-305.xml</fr:route><fr:title text="Linear Combinations">Linear Combinations</fr:title><fr:date><fr:year>2024</fr:year><fr:month>6</fr:month><fr:day>10</fr:day></fr:date><fr:authors></fr:authors><fr:parent>math-0008</fr:parent></fr:frontmatter><fr:mainmatter><fr:p></fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>779</fr:anchor><fr:addr type="user">def-000L</fr:addr><fr:route>def-000L.xml</fr:route><fr:title text="Linear Combination">Linear Combination</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Let <fr:tex display="inline">V</fr:tex> be a <fr:link type="local" href="def-000H.xml" addr="def-000H" title="Vector Space">vector space</fr:link> over a field <fr:tex display="inline">F</fr:tex>.
    Let <fr:tex display="inline">v_1, \dots , v_n</fr:tex> be vectors in <fr:tex display="inline">V</fr:tex>.
    A <fr:strong>linear combination</fr:strong> of <fr:tex display="inline">v_1, \dots , v_n</fr:tex> is an expression of the form
    <fr:tex display="block">         a_1 v_1 + \dots  + a_n v_n     </fr:tex>
    where <fr:tex display="inline">a_1, \dots , a_n \in  F</fr:tex>.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree><fr:p>Given two sets of vectors, we say that they are eqaul iff their elements can write as linear combinations of each other.
        This condition can be written using rank:
        <fr:tex display="block">             \text {rank}(A) = \text {rank}(B) = \text {rank}(A, B)         </fr:tex></fr:p><fr:tree toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false" xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>780</fr:anchor><fr:addr type="user">def-000P</fr:addr><fr:route>def-000P.xml</fr:route><fr:title text="Linearly independent">Linearly independent</fr:title><fr:taxon>Definition</fr:taxon><fr:authors></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>A set of vectors <fr:tex display="inline">\{v_1, \dots , v_n\}</fr:tex> is called <fr:strong>linearly independent</fr:strong> if
    <fr:tex display="block">a_1 v_1 + \dots  + a_n v_n = 0</fr:tex>
    implies that <fr:tex display="inline">a_1 = \dots  = a_n = 0</fr:tex>.
    The trivial case of <fr:tex display="inline">\{0\}</fr:tex> is also considered linearly independent.</fr:p></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree></fr:mainmatter><fr:backmatter></fr:backmatter></fr:tree></fr:backmatter></fr:tree>