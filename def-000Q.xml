<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="true" expanded="true" root="false"><fr:frontmatter><fr:anchor>1058</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-000Q</fr:addr><fr:route>def-000Q.xml</fr:route><fr:title>Linearly dependent</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    A set of vectors <fr:tex>\{ v_1,  \dots , v_n \}</fr:tex> is called <fr:strong>linearly dependent</fr:strong> if
    <fr:tex display="block">a_1 v_1 +  \dots  + a_n v_n = 0</fr:tex>
    for some <fr:tex>a_1,  \dots , a_n  \in   \mathbb {F}</fr:tex> with at least one <fr:tex>a_i  \neq  0</fr:tex> (not all <fr:tex>0</fr:tex>).
</fr:p></fr:mainmatter><fr:backmatter><fr:contributions></fr:contributions><fr:context><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="false" show-heading="true" show-metadata="true" expanded="true" root="false"><fr:frontmatter><fr:anchor>1057</fr:anchor><fr:taxon>Linear Algebra</fr:taxon><fr:addr>math-0002</fr:addr><fr:route>math-0002.xml</fr:route><fr:title>Finite Dimensional Vector Space</fr:title><fr:date><fr:year>2024</fr:year><fr:month>1</fr:month><fr:day>26</fr:day></fr:date></fr:frontmatter><fr:mainmatter><fr:p>
    This note introduces the concept of finite-dimensional vector space.
    Refer to <fr:link href="linear-algebra-2015.xml" type="local" addr="linear-algebra-2015">Linear Algebra Done Right</fr:link>.
</fr:p><fr:p>
    Adding up scalar mulitples of vectors in a list gives a linear combination.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>391</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-000L</fr:addr><fr:route>def-000L.xml</fr:route><fr:title>Linear Combination</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Let <fr:tex>V</fr:tex> be a <fr:link href="def-000H.xml" type="local" addr="def-000H">vector space</fr:link> over a field <fr:tex>F</fr:tex>.
    Let <fr:tex>v_1,  \dots , v_n</fr:tex> be vectors in <fr:tex>V</fr:tex>.
    A <fr:strong>linear combination</fr:strong> of <fr:tex>v_1,  \dots , v_n</fr:tex> is an expression of the form
    <fr:tex display="block">         a_1 v_1 +  \dots  + a_n v_n     </fr:tex>
    where <fr:tex>a_1,  \dots , a_n  \in  F</fr:tex>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    To talk about a structure, we usually define a collection of this structure.
    Hence we have span for linear combinations.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>392</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-000M</fr:addr><fr:route>def-000M.xml</fr:route><fr:title>Linear Span</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Let <fr:tex>V</fr:tex> be a vector space over a field <fr:tex>F</fr:tex>.
    Let <fr:tex>v_1,  \dots , v_n</fr:tex> be vectors in <fr:tex>V</fr:tex>.
    The <fr:strong>span</fr:strong> of <fr:tex>v_1,  \dots , v_n</fr:tex> is defined as
    <fr:tex display="block">          \text {span} (v_1,  \dots , v_n) =  \{ a_1 v_1 +  \dots  + a_n v_n  \mid  a_i  \in  F \}      </fr:tex>
    The span of empty set is defined to be <fr:tex>\{ 0 \}</fr:tex>.    
</fr:p><fr:p>
    If <fr:tex>\text {span} (v_1,  \dots , v_n) = V</fr:tex>, we say that <fr:tex>v_1,  \dots , v_n</fr:tex> <fr:strong>spans</fr:strong> <fr:tex>V</fr:tex>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    Suppose we have span <fr:tex>S= \text {span} (v_1,  \dots , v_n)</fr:tex>. (Span is trivially a subspace.)
    Obviously for all <fr:tex>v_j (1  \leq  j  \leq  n)</fr:tex>, <fr:tex>v_j  \in  S</fr:tex>.
    Because subspaces are closed under scalar multiplication and addition, every
    subspace of <fr:tex>V</fr:tex> containing <fr:tex>v_1,  \dots , v_n</fr:tex> must contain <fr:tex>S</fr:tex>.
    Thus we conclude that <fr:tex>S</fr:tex> is the smallest subspace containing <fr:tex>v_1,  \dots , v_n</fr:tex>.
</fr:p><fr:p>
    The discussion about <fr:strong>spans</fr:strong> leads to a key definition in linear algebra.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>393</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-000N</fr:addr><fr:route>def-000N.xml</fr:route><fr:title>Finite-Dimensional Vector Space</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    A <fr:link href="def-000H.xml" type="local" addr="def-000H">vector space</fr:link> <fr:tex>V</fr:tex> is called <fr:strong>finite-dimensional</fr:strong> if some <fr:link href="def-000G.xml" type="local" addr="def-000G">list</fr:link> of vectors <fr:tex>v_1,  \dots , v_n</fr:tex> <fr:link href="def-000M.xml" type="local" addr="def-000M">spans</fr:link> <fr:tex>V</fr:tex>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    The opposite of finite-dimensional is infinite-dimensional.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>394</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-000O</fr:addr><fr:route>def-000O.xml</fr:route><fr:title>Infinite-dimensional vector space</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    A vector space <fr:tex>V</fr:tex> is called <fr:strong>infinite-dimensional</fr:strong> if it is not <fr:link href="def-000N.xml" type="local" addr="def-000N">finite-dimensional</fr:link>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    Consider the situation that there is only one way to
    express a vector <fr:tex>v</fr:tex> as a linear combination of vectors in a list <fr:tex>v_1,  \dots , v_n</fr:tex>.
    What property of the list <fr:tex>v_1,  \dots , v_n</fr:tex> does this situation imply? The answer is
    linear independence.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>395</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-000P</fr:addr><fr:route>def-000P.xml</fr:route><fr:title>Linearly independent</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    A set of vectors <fr:tex>\{ v_1,  \dots , v_n \}</fr:tex> is called <fr:strong>linearly independent</fr:strong> if
    <fr:tex display="block">a_1 v_1 +  \dots  + a_n v_n = 0</fr:tex>
    implies that <fr:tex>a_1 =  \dots  = a_n = 0</fr:tex>.
    The trivial case of <fr:tex>\{ 0 \}</fr:tex> is also considered linearly independent.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    If some vectors are not linearly independent, then there are more than one way to
    express a vector as a linear combination of vectors in the list. This leads to 
    the following definition.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>396</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-000Q</fr:addr><fr:route>def-000Q.xml</fr:route><fr:title>Linearly dependent</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    A set of vectors <fr:tex>\{ v_1,  \dots , v_n \}</fr:tex> is called <fr:strong>linearly dependent</fr:strong> if
    <fr:tex display="block">a_1 v_1 +  \dots  + a_n v_n = 0</fr:tex>
    for some <fr:tex>a_1,  \dots , a_n  \in   \mathbb {F}</fr:tex> with at least one <fr:tex>a_i  \neq  0</fr:tex> (not all <fr:tex>0</fr:tex>).
</fr:p></fr:mainmatter></fr:tree><fr:p>
    The following lemma is a direct consequence of the definition of linear independence.
    It states that for a given linearly dependent list, we can always remove a vector
    without changing the span.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>397</fr:anchor><fr:taxon>Lemma</fr:taxon><fr:addr>thm-0001</fr:addr><fr:route>thm-0001.xml</fr:route><fr:title>Linear Dependence Lemma</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Let <fr:tex>v_1,  \dots , v_n</fr:tex> be vectors in a vector space <fr:tex>V</fr:tex> over a field <fr:tex>\mathbb {F}</fr:tex>.
    If <fr:tex>v_1,  \dots , v_n</fr:tex> are linearly dependent, then there exists <fr:tex>1  \leq  i  \leq  n</fr:tex> such that
    <fr:ul><fr:li><fr:tex>v_i  \in   \text {span} (v_1,  \dots , v_{i-1})</fr:tex></fr:li>
        <fr:li>Remove <fr:tex>v_i</fr:tex> from the list <fr:tex>v_1,  \dots , v_n</fr:tex> and the span does not change</fr:li></fr:ul></fr:p></fr:mainmatter></fr:tree><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>398</fr:anchor><fr:taxon>Lemma</fr:taxon><fr:addr>thm-0002</fr:addr><fr:route>thm-0002.xml</fr:route><fr:title>Length of linearly independent list <fr:tex>\leq</fr:tex> length of spanning list</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    In a finite dimensional vector space, the length of a linearly independent list is less than or equal to the length of a spanning list.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    We have discussed linear independent lists and spanning lists.
    Now we are ready to define a basis.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>399</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-000R</fr:addr><fr:route>def-000R.xml</fr:route><fr:title>Basis</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    A basis of <fr:tex>V</fr:tex> is a list of vectors in <fr:tex>V</fr:tex>
    that is linearly independent and spans <fr:tex>V</fr:tex>. 
</fr:p><fr:p><fr:strong>Criterion for basis</fr:strong>
    A list of vectors <fr:tex>\{ v_1,  \dots , v_n \}</fr:tex> is a basis of <fr:tex>V</fr:tex> if and only if
    every <fr:tex>v  \in  V</fr:tex> can be written <fr:strong>uniquely</fr:strong> as a linear combination of <fr:tex>v_1,  \dots , v_n</fr:tex>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    For instance, we have standard basis <fr:tex>\{ e_1,  \dots , e_n \}</fr:tex> for <fr:tex>\mathbb {F}^n</fr:tex>,
    where <fr:tex>e_i</fr:tex> is the vector with <fr:tex>1</fr:tex> at <fr:tex>i</fr:tex>-th position and <fr:tex>0</fr:tex> elsewhere.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>400</fr:anchor><fr:taxon>Theorem</fr:taxon><fr:addr>thm-0005</fr:addr><fr:route>thm-0005.xml</fr:route><fr:title>Spanning List contains a basis</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Every spanning list in a vector space can be reduced to a basis.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    From the <fr:link href="thm-0005.xml" type="local" addr="thm-0005">theorem</fr:link> we can infer a corollary.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>401</fr:anchor><fr:taxon>Corollary</fr:taxon><fr:addr>thm-0006</fr:addr><fr:route>thm-0006.xml</fr:route><fr:title>Basis of finite-dimensional vector space</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Every finite-dimensional vector space has a basis.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    The next result states for a spanning list can be reduced to a basis.
    We can adjoin one or more vectors to a linearly independent list to form a basis.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>402</fr:anchor><fr:taxon>Theorem</fr:taxon><fr:addr>thm-0007</fr:addr><fr:route>thm-0007.xml</fr:route><fr:title>Linearly dependent list extends to a basis</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Every linearly independent list of vectors in  a finite-dimensional vector space can be extended to a basis.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    Remind the definition of <fr:link href="der-000K" type="external">direct sum</fr:link>, we can now show that
    every subspace of a finite-dimensional vecrtor space can be paired
    with another subspace to form a direct sum of the whole space.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>403</fr:anchor><fr:taxon>Theorem</fr:taxon><fr:addr>thm-0008</fr:addr><fr:route>thm-0008.xml</fr:route><fr:title>Direct Sum of Subspaces of <fr:tex>V</fr:tex></fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Suppose <fr:tex>V</fr:tex> is a finite dimensional vector space,
    and <fr:tex>U</fr:tex> is a subspace of <fr:tex>V</fr:tex>.
    Then there exists a subspace <fr:tex>W</fr:tex> of <fr:tex>V</fr:tex> such that
    <fr:tex>V = U  \oplus  W</fr:tex>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    This post discusses about <fr:em>finite-dimensional vector space</fr:em>.
    But we have not yet defined what is dimension.
    We tempted to define the dimension as the length of basis intuitively.
    With this definition we should prove its well-definedness.
    That is, every basis has the same length.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>404</fr:anchor><fr:taxon>Theorem</fr:taxon><fr:addr>thm-0009</fr:addr><fr:route>thm-0009.xml</fr:route><fr:title>Basis length is invariant</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Let <fr:tex>V</fr:tex> be a finite-dimensional vector space.
    Then every basis of <fr:tex>V</fr:tex> has the same length.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    This can be proved by <fr:link href="thm-0002.xml" type="local" addr="thm-0002">Lemma 8</fr:link>.
    Now we can formally define the dimension of such spaces.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>405</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-001V</fr:addr><fr:route>def-001V.xml</fr:route><fr:title>Dimension</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    The <fr:strong>dimension</fr:strong> of a finite-dimensional vector space <fr:tex>V</fr:tex> is the length of any basis of the vector space.
    Denoted by <fr:tex>\dim  V</fr:tex>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    Every subspace of a finite-dimensional vector space is also finite-dimensional.
    Hence we can talk about the dimension of a subspace.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>406</fr:anchor><fr:taxon>Theorem</fr:taxon><fr:addr>thm-000A</fr:addr><fr:route>thm-000A.xml</fr:route><fr:title>Dimension of a subspace</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Let <fr:tex>V</fr:tex> be a finite-dimensional vector space,
    and <fr:tex>U</fr:tex> be a subspace of <fr:tex>V</fr:tex>.
    Then <fr:tex>\dim  U  \leq   \dim  V</fr:tex>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    According to the definition of <fr:link href="def-000P.xml" type="local" addr="def-000P">linearly independent</fr:link>,
    to show a list of vectors is a basis, we only need to show it is linearly independent,
    and it spans the whole space.
    The next theorems simplifies the task:
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>407</fr:anchor><fr:taxon>Theorem</fr:taxon><fr:addr>thm-000B</fr:addr><fr:route>thm-000B.xml</fr:route><fr:title>Linearly independent list of the right length is a basis</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Let <fr:tex>V</fr:tex> be a finite-dimensional vector space.
    Then every <fr:link href="def-000P.xml" type="local" addr="def-000P">linearly independent</fr:link> list of vectors in <fr:tex>V</fr:tex> with length equal to <fr:tex>\dim  V</fr:tex> is a basis of <fr:tex>V</fr:tex>.
</fr:p></fr:mainmatter></fr:tree><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>408</fr:anchor><fr:taxon>Theorem</fr:taxon><fr:addr>thm-000C</fr:addr><fr:route>thm-000C.xml</fr:route><fr:title>Spanning list of the right length is a basis</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Let <fr:tex>V</fr:tex> be a finite-dimensional vector space.
    Then every <fr:link href="def-000M.xml" type="local" addr="def-000M">spanning</fr:link> list of vectors in <fr:tex>V</fr:tex> with length equal to <fr:tex>\dim  V</fr:tex> is a basis of <fr:tex>V</fr:tex>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    Now we move to the discussion of the dimension of the sum of two subspaces.
    This is analogous to the <fr:link href="thm-000E.xml" type="local" addr="thm-000E">inclusion-exclusion principle</fr:link>.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>409</fr:anchor><fr:taxon>Theorem</fr:taxon><fr:addr>thm-000D</fr:addr><fr:route>thm-000D.xml</fr:route><fr:title>Dimension of a sum</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Let <fr:tex>V</fr:tex> be a finite-dimensional vector space,
    and <fr:tex>U</fr:tex> and <fr:tex>W</fr:tex> be subspaces of <fr:tex>V</fr:tex>.
    Then
    <fr:tex display="block">          \dim (U + W) =  \dim  U +  \dim  W -  \dim (U  \cap  W).     </fr:tex></fr:p></fr:mainmatter></fr:tree></fr:mainmatter></fr:tree></fr:context><fr:related></fr:related><fr:backlinks><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="false" show-heading="true" show-metadata="true" expanded="true" root="false"><fr:frontmatter><fr:anchor>1056</fr:anchor><fr:taxon>Differential Geometry</fr:taxon><fr:addr>math-0007</fr:addr><fr:route>math-0007.xml</fr:route><fr:title>Vector Calculus and Geometry of Space</fr:title><fr:date><fr:year>2024</fr:year><fr:month>4</fr:month><fr:day>5</fr:day></fr:date></fr:frontmatter><fr:mainmatter><fr:p>
    Notes about multi-variable calculus, geometry of space and linear algebra.
    Refer to <fr:link href="A%20Visual%20Introduction%20to%20Differential%20Forms%20and%20Calculus%20on%20Manifolds" type="external">df-cm-2018</fr:link>.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>543</fr:anchor><fr:title>Review of Vector Spaces</fr:title><fr:date><fr:year>2024</fr:year><fr:month>4</fr:month><fr:day>5</fr:day></fr:date><fr:parent>math-0007</fr:parent></fr:frontmatter><fr:mainmatter><fr:p>
        We now start with introducing the vector space over the field of real numbers <fr:tex>\mathbb {R}</fr:tex>.
    </fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>541</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-000H</fr:addr><fr:route>def-000H.xml</fr:route><fr:title>Vector Space</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    A vector space over a <fr:link href="def-0006.xml" type="local" addr="def-0006">field</fr:link> <fr:tex>F</fr:tex> is a non-empty set <fr:tex>V</fr:tex> together with a binary operation and a binary function that satisfy the axioms listed below. 
    In this context, the elements of <fr:tex>V</fr:tex> are commonly called <fr:strong>vectors</fr:strong>, and the elements of <fr:tex>F</fr:tex> are called <fr:strong>scalars</fr:strong>.
    <fr:ul><fr:li>Commutativity: <fr:tex>              \forall  x, y  \in  V, x + y = y + x         </fr:tex></fr:li>
        <fr:li>Associativity: <fr:tex>              \forall  x, y, z  \in  V, (x + y) + z = x + (y + z)         </fr:tex></fr:li>
        <fr:li>Additive Identity: <fr:tex>              \exists  0  \in  V  \text { such that }  \forall  x  \in  V, x + 0 = x         </fr:tex></fr:li>
        <fr:li>Multiplicative Identity: <fr:tex>              \forall  x  \in  V, 1x = x         </fr:tex></fr:li>
        <fr:li>Additive Inverse: <fr:tex>              \forall  x  \in  V,  \exists  y  \in  V  \text { such that } x + y = 0         </fr:tex></fr:li>
        <fr:li>Distributivity: <fr:tex>              \forall  x, y  \in  V,  \forall  c, d  \in  F, c(x + y) = cx + cy, (c + d)x = cx + dx         </fr:tex></fr:li></fr:ul></fr:p><fr:p>
    Elements of a vector space are called <fr:strong>vectors</fr:strong> or <fr:strong>points</fr:strong>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
        Use <fr:tex>\mathbb {R} ^2</fr:tex> as an example we can see (Note that we always treat elements of vector spaces as 
        column vectors and never as row vectors):
        <fr:tex display="block">             c  \cdot   \begin {bmatrix}                 a  \\  b              \end {bmatrix} =  \begin {bmatrix}                 c  \cdot  a  \\  c  \cdot  b              \end {bmatrix}         </fr:tex></fr:p><fr:p>
        Now we will consider a certain type of transformation between vector spaces called a <fr:link href="def-0025.xml" type="local" addr="def-0025"><fr:strong>linear transformation</fr:strong></fr:link>.
        Suppose <fr:tex>T</fr:tex> is a mapping between <fr:tex>\mathbb {R} ^n</fr:tex> and <fr:tex>\mathbb {R} ^m</fr:tex>, that is <fr:tex>T: \mathbb {R} ^n \to \mathbb {R} ^m</fr:tex>, then <fr:tex>T</fr:tex> is a linear transformation if:
        <fr:tex display="block">             T(c  \cdot   \vec {v}) = c  \cdot  T( \vec {v})              \\               T( \vec {v} +  \vec {w}) = T( \vec {v}) + T( \vec {w})         </fr:tex>
        If <fr:tex>T</fr:tex> is a linear transformation from <fr:tex>\mathbb {R} ^m</fr:tex> to <fr:tex>\mathbb {R}</fr:tex> we simply call it a <fr:strong>linear function</fr:strong> or a <fr:strong>linear functional</fr:strong>.
    </fr:p><fr:p>
        We now turn our attention to the relationship between linear transformation and matrices. 
        We just stick to vector spaces <fr:tex>\mathbb {R} ^n</fr:tex> and the standard basis made up of the <fr:strong>Euclidian unit vectors</fr:strong>.
        In order to write linear transformation <fr:tex>T: \mathbb {R} ^n \to \mathbb {R} ^m</fr:tex> as a matrix we need ordered bases for both <fr:tex>\mathbb {R} ^n</fr:tex> and <fr:tex>\mathbb {R} ^m</fr:tex>.
        We can use the intuitively obvious order <fr:tex>e_1 &lt; e_2 &lt;  \cdots  &lt; e_n</fr:tex>.
        Now we can give formal definition of the matrix representation of a linear transformation.
    </fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>542</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-003W</fr:addr><fr:route>def-003W.xml</fr:route><fr:title>Matrix Representation of Linear Transformation over <fr:tex>\mathbb {R} ^n</fr:tex></fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Suppose that <fr:tex>T: \mathbb {R} ^n \to \mathbb {R} ^m</fr:tex> is a linear transformation between vector spaces <fr:tex>\mathbb {R} ^n</fr:tex> and <fr:tex>\mathbb {R} ^m</fr:tex>.
    Let <fr:tex>e_1, e_2,  \ldots , e_n</fr:tex> be the standard basis of <fr:tex>\mathbb {R} ^n</fr:tex> and <fr:tex>f_1, f_2,  \ldots , f_m</fr:tex> be the standard basis of <fr:tex>\mathbb {R} ^m</fr:tex>.
    Then the matrix representation of <fr:tex>T</fr:tex> is the <fr:tex>m  \times  n</fr:tex> matrix <fr:tex>A</fr:tex> such that for <fr:tex>1 \leq  j \leq  n</fr:tex>:
    <fr:tex display="block">         T(e_j) =  \sum _{i=1}^m A_{ij} f_i     </fr:tex>
    where the matrix representation of <fr:tex>T</fr:tex> is given by the <fr:tex>m \times  n</fr:tex> matrix with entries <fr:tex>A_{ij}</fr:tex>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
        The last major topic in this section is the definition of the <fr:link href="def-003X.xml" type="local" addr="def-003X">dual space</fr:link>.
        In our discussion, we only concern the dual space of <fr:tex>\mathbb {R} ^n</fr:tex> which is denoted as <fr:tex>( \mathbb {R} ^n)^*</fr:tex>.
        Now let&apos;s consider the <fr:strong>dual basis</fr:strong> of <fr:tex>( \mathbb {R} ^n)^*</fr:tex> which is denoted as <fr:tex>\{ T_1,  \cdots , T_n \}</fr:tex>, 
        which is defined by:
        <fr:tex display="block">             T_i(e_j) = e^i(e_j) =  \langle  e^i, e_j  \rangle  =  \delta _{j}^i         </fr:tex>
        where <fr:tex>\delta _{ij}</fr:tex> is the <fr:link href="def-001P.xml" type="local" addr="def-001P">Kronecker delta</fr:link>. We say that <fr:tex>T_i</fr:tex> is dual to the vector <fr:tex>e_i</fr:tex>.
        Note that we also denote <fr:tex>T_i</fr:tex> as <fr:tex>e^i</fr:tex> using superscript notation. And the notation <fr:tex>\langle  e^i, e_j  \rangle</fr:tex> d
        indicates the products of row vector <fr:tex>e^i</fr:tex> and column vector <fr:tex>e_j</fr:tex> (Usually used in quantum computing).
        <fr:tex display="block">              \alpha (v) =  \langle   \alpha , v  \rangle  = [a,b]  \times   \begin {bmatrix}                 x  \\  y              \end {bmatrix} = ax + by         </fr:tex>
        This explains wht we always denote elements of the vector space as column vectors, because elements of the dual space 
        are written as row vectors and its very important to distinguish between them.
    </fr:p></fr:mainmatter></fr:tree><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>546</fr:anchor><fr:title>Dot Products</fr:title><fr:date><fr:year>2024</fr:year><fr:month>4</fr:month><fr:day>5</fr:day></fr:date><fr:parent>math-0007</fr:parent></fr:frontmatter><fr:mainmatter><fr:p>
        In linear algebra, <fr:strong>dot product</fr:strong> or <fr:strong>scalar product</fr:strong> is an operation that takes two vectors and returns a scalar.
        Geometrically, it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them.
        Dot product is also used to define lengths and angles.
    </fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>544</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-0041</fr:addr><fr:route>def-0041.xml</fr:route><fr:title>Dot Product (Coordinate Form)</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    The <fr:strong>dot product</fr:strong> of two vectors <fr:tex>\vec {a} = (a_1, a_2,  \cdots , a_n)</fr:tex> and <fr:tex>\vec {b} = (b_1, b_2,  \cdots , b_n)</fr:tex> is defined as
    <fr:tex display="block">          \vec {a} \cdot \vec {b} = a_1b_1 + a_2b_2 +  \cdots  + a_nb_n =  \sum _{i=1}^n a_ib_i.     </fr:tex>
    The dot product is also called the <fr:strong>inner product</fr:strong> or <fr:strong>scalar product</fr:strong>.
    The dot product satisfies the following properties:
    <fr:ul><fr:li><fr:strong>Commutative</fr:strong>: <fr:tex>\vec {a} \cdot \vec {b} =  \vec {b} \cdot \vec {a}</fr:tex></fr:li>
        <fr:li><fr:strong>Distributive</fr:strong>: <fr:tex>\vec {a} \cdot ( \vec {b} +  \vec {c}) =  \vec {a} \cdot \vec {b} +  \vec {a} \cdot \vec {c}</fr:tex></fr:li>
        <fr:li><fr:strong>Bilinear</fr:strong>: <fr:tex>\vec {a} \cdot (k \vec {b}) = k( \vec {a} \cdot \vec {b}) = ( \vec {a} \cdot  k \vec {b})</fr:tex></fr:li>
        <fr:li><fr:strong>Scalar Multiplication</fr:strong>: <fr:tex>(c_1 \vec {a})  \cdot  (c_2 \vec {b}) = c_1c_2( \vec {a} \cdot \vec {b})</fr:tex></fr:li>
        <fr:li><fr:strong>Orthogonality</fr:strong>: If <fr:tex>\vec {a} \cdot \vec {b} = 0</fr:tex>, then <fr:tex>\vec {a}</fr:tex> and <fr:tex>\vec {b}</fr:tex> are <fr:strong>orthogonal</fr:strong></fr:li>
        <fr:li><fr:strong>Product Rule</fr:strong>: If <fr:tex>\vec {a}</fr:tex> and <fr:tex>\vec {b}</fr:tex> are vector valued differentiable functions then the derivative 
            of <fr:tex>\vec {a} \cdot \vec {b}</fr:tex> is given by the rule <fr:tex>( \vec {a} \cdot \vec {b})&apos; =  \vec {a}&apos; \cdot \vec {b} +  \vec {a} \cdot \vec {b}&apos;</fr:tex></fr:li></fr:ul></fr:p></fr:mainmatter></fr:tree><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>545</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-0042</fr:addr><fr:route>def-0042.xml</fr:route><fr:title>Dot Product (Geometric Form)</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    In <fr:strong>Euclidean space</fr:strong>, a <fr:strong>Euclidean vector</fr:strong> is a geometric object that possesses both 
    a norm and a direction. The <fr:strong>dot product</fr:strong> of two vectors <fr:tex>\vec {a}</fr:tex> and <fr:tex>\vec {b}</fr:tex> is defined as
    <fr:tex display="block">          \vec {a} \cdot \vec {b} =  \lVert \vec {a} \rVert \lVert \vec {b} \rVert \cos \theta      </fr:tex>
    where <fr:tex>\theta</fr:tex> is the angle between the two vectors.
</fr:p></fr:mainmatter></fr:tree><fr:p>
        The geometric definition of dot product helps us express the projection of one vector onto another as well as the component of 
        one vector in the direction of another. By simple geometry we can derive the formula for the <fr:strong>projection</fr:strong>
        <fr:tex display="block">              \text {proj}_{ \vec {a}} \vec {b} =  \frac { \vec {a} \cdot \vec {b}}{ \lVert \vec {a} \rVert } \frac { \vec {a}}{ \lVert \vec {a} \rVert }         </fr:tex>
        and the <fr:strong>component</fr:strong> of <fr:tex>\vec {b}</fr:tex> in the direction <fr:tex>\vec {a}</fr:tex> is given by
        <fr:tex display="block">              \text {comp}_{ \vec {a}} \vec {b} =  \lVert \text {proj}_{ \vec {a}} \vec {b} \rVert  =  \frac { \vec {a} \cdot \vec {b}}{ \lVert \vec {a} \rVert }         </fr:tex></fr:p><fr:p>
        Two points determine a line, and so does a point and a vector. Define the base point vector <fr:tex>\vec {b}=(x,y,z)</fr:tex> and
        the direction vector <fr:tex>\vec {v}=(a,b,c)</fr:tex> then the line is given by <fr:tex>\vec {r}(t)</fr:tex>
        <fr:tex display="block">              \vec {r}(t) = t \vec {v} +  \vec {b} = (at+x, bt+y, ct+z)         </fr:tex>
        Solving for <fr:tex>t</fr:tex> in the equation we get
        <fr:tex display="block">             t =  \frac {x-at}{a} =  \frac {y-bt}{b} =  \frac {z-ct}{c}         </fr:tex>
        which is the <fr:strong>equation of line</fr:strong>.
    </fr:p></fr:mainmatter></fr:tree><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>552</fr:anchor><fr:title>Volume and Determinants</fr:title><fr:date><fr:year>2024</fr:year><fr:month>4</fr:month><fr:day>5</fr:day></fr:date><fr:parent>math-0007</fr:parent></fr:frontmatter><fr:mainmatter><fr:p>
        The <fr:strong>determinant</fr:strong> has various properties and applications in linear algebra and geometry.
        For us the most useful thing about it will be how it relates to volume:
        the determinant of a matrix gives the <fr:strong>signed volume</fr:strong> of the parallelepiped that 
        is generated by the vectors given by the matrix columns.
    </fr:p><fr:p>
        Determinants can be introduced in a variety of different ways but many of them are not at all clear.
        It usually relates to volume hence we will actually use our intuitive understanding of volumes and 
        three properties that we expected volume to have to derive the determinant (It is <fr:strong>uniquely</fr:strong> determined!).
    </fr:p><fr:p>
        So how do we expect volume to behave?
        First we expect a unit cube to have a volume of one.
        Second we expect the <fr:strong>degenerate</fr:strong> parallelepiped to have a volume of zero. Basically in <fr:tex>n</fr:tex> dimensions any 
        <fr:tex>n-1</fr:tex> dimensions object has zero <fr:tex>n</fr:tex>-D volume.
        Third we expect that volumes to be <fr:strong>linear</fr:strong>.
        Now with these three properties we move to the actual mathematics.
    </fr:p><fr:p>
        Suppose we have a parallelepiped <fr:tex>\mathscr {P} \in \mathbb {R} ^n</fr:tex> whose edges are given by <fr:tex>v_1, v_2,  \cdots , v_n \in \mathbb {R} ^n</fr:tex>.
        We sat that the parallelepiped <fr:tex>\mathscr {P}</fr:tex> is the <fr:strong>span</fr:strong> of the vectors <fr:tex>v_1, v_2,  \cdots , v_n</fr:tex> and 
        write <fr:tex>\mathscr {P}= \text {span} \{ v_1, v_2,  \cdots , v_n \}</fr:tex> (Note that this span is different from linear span).
        We want to find function <fr:tex>D: \mathbb {R} ^{n \times  n} \to \mathbb {R}</fr:tex> which takes <fr:tex>v_1, v_2,  \cdots , v_n</fr:tex> or a matrix with <fr:tex>v_1, v_2,  \cdots , v_n</fr:tex> as columns
        to a real number which is the volume of <fr:tex>\mathscr {P}</fr:tex>. Now we present the three properties in mathematical form.
    </fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>547</fr:anchor><fr:title>Properties of Volume</fr:title><fr:date><fr:year>2024</fr:year><fr:month>4</fr:month><fr:day>5</fr:day></fr:date></fr:frontmatter><fr:mainmatter><fr:ul><fr:li><fr:tex> D(I) = I </fr:tex> where <fr:tex>I = [e_1, e_2,  \cdots , e_n]</fr:tex> is the identity matrix.
            </fr:li>
            <fr:li><fr:tex> D(v_1, v_2,  \cdots , v_n) = 0 </fr:tex> if <fr:tex>v_i = v_j</fr:tex> for any <fr:tex>i \neq  j</fr:tex>.
            </fr:li>
            <fr:li><fr:tex> D(v_1,  \cdots , v_{j-1}, v+cw, v_{j+1},  \cdots , v_n)  \\                   = D(v_1,  \cdots , v_{j-1}, v, v_{j+1},  \cdots , v_n) + cD(v_1,  \cdots , v_{j-1}, w, v_{j+1},  \cdots , v_n) </fr:tex>
                for any <fr:tex>1  \leq  j  \leq  n</fr:tex>, that is, <fr:tex>D</fr:tex> is linear.
            </fr:li></fr:ul></fr:mainmatter></fr:tree><fr:p>
        Now we use these properties of volume to derive several other useful properties.
        The first property is that the volumes are signed.
    </fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>548</fr:anchor><fr:title>Derived Properties of Volume Function</fr:title><fr:date><fr:year>2024</fr:year><fr:month>4</fr:month><fr:day>5</fr:day></fr:date></fr:frontmatter><fr:mainmatter><fr:ul><fr:li><fr:tex>D</fr:tex> is alternating, if we switch any two vectors the sign changes.
                <fr:tex display="block">                     D(v_1,  \cdots , v_i,  \cdots , v_j,  \cdots , v_n) = -D(v_1,  \cdots , v_j,  \cdots , v_i,  \cdots , v_n)                 </fr:tex></fr:li> 
            <fr:li>
                If <fr:tex>v_1, v_2,  \cdots , v_n</fr:tex> are <fr:link href="def-000Q.xml" type="local" addr="def-000Q">linear dependent</fr:link> then
                <fr:tex display="block">                     D(v_1, v_2,  \cdots , v_n) = 0                 </fr:tex></fr:li>
            <fr:li>
                Adding a multiple of one vector to another does not change the determinant.
            </fr:li></fr:ul></fr:mainmatter></fr:tree><fr:p>
        We almost ready to derive the formula for determinant. The final ingredient we need to do is <fr:strong>permutations</fr:strong>.
    </fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>549</fr:anchor><fr:taxon>Defintion</fr:taxon><fr:addr>def-003Y</fr:addr><fr:route>def-003Y.xml</fr:route><fr:title>Permutation</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    A <fr:strong>permutation</fr:strong> of a set <fr:tex>S</fr:tex> is a bijection from <fr:tex>S</fr:tex> to itself.
    The set of permutation of <fr:tex>\{ 1, \cdots , n \}</fr:tex> is usually denoted by <fr:tex>S_n</fr:tex>.
    We often denote a particular permutation <fr:tex>\sigma</fr:tex> by <fr:strong>Cauchy&apos;s two-line notation</fr:strong>:
    <fr:tex display="block">          \begin {bmatrix}             1 &amp; 2 &amp;  \cdots  &amp; n  \\               \sigma (1) &amp;  \sigma (2) &amp;  \cdots  &amp;  \sigma (n)          \end {bmatrix}     </fr:tex>
    or <fr:strong>Cauchy&apos;s one-line notation</fr:strong>: <fr:tex>( \sigma (1), \sigma (2), \cdots , \sigma (n))</fr:tex>.
    Another common notation is the <fr:strong>cycle notation</fr:strong>:
    <fr:tex display="block">         (i_1 \  i_2 \  \cdots \  i_k)     </fr:tex> which means <fr:tex>i_1  \to  i_2  \to   \cdots   \to  i_k  \to  i_1</fr:tex>.
</fr:p></fr:mainmatter></fr:tree><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>550</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-003Z</fr:addr><fr:route>def-003Z.xml</fr:route><fr:title>Transposition</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    A <fr:link href="def-003Y.xml" type="local" addr="def-003Y">permutation</fr:link> in which only two elements are exchanged is called a <fr:strong>transposition</fr:strong>.
    The notation is <fr:tex>\tau _{i,j}</fr:tex> where <fr:tex>i</fr:tex> and <fr:tex>j</fr:tex> are the two elements exchanged while the others remain fixed.
</fr:p></fr:mainmatter></fr:tree><fr:p>
        Notice that the composition of two permutations is also a permutation. 
        And for any permutation <fr:tex>\sigma</fr:tex> we can perform a series of transpositions to get the identity permutation.
        It turns out that the count of the number of transpositions needed to get the identity permutation is always the same,
        which is called the <fr:strong>parity</fr:strong> of the permutation.
    </fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>551</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-0040</fr:addr><fr:route>def-0040.xml</fr:route><fr:title>Sign of Permutation</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    The <fr:strong>sign</fr:strong> of a <fr:link href="def-003Y.xml" type="local" addr="def-003Y">permutation</fr:link> <fr:tex>\sigma \in  S_n</fr:tex> is a function <fr:tex>\text {sgn} :S_n \to \{ -1,1 \}</fr:tex> defined as
    <fr:tex>\text {sgn} ( \sigma ) = 1</fr:tex> if <fr:tex>\sigma</fr:tex> requires an even number of permutations and 
    <fr:tex>\text {sgn} ( \sigma ) = -1</fr:tex> if <fr:tex>\sigma</fr:tex> requires an odd number of permutations to get the identity permutation.
</fr:p></fr:mainmatter></fr:tree><fr:p>
        Now we define the permutation of unit vectors <fr:tex>E_ \sigma  = [e_{ \sigma (1)}, e_{ \sigma (2)},  \cdots , e_{ \sigma (n)}]</fr:tex>.
        We got the property that
        <fr:tex display="block">             D(E_ \sigma ) =  \text {sgn} ( \sigma )D(I) =  \text {sgn} ( \sigma )         </fr:tex>
        Now we have all the pieces necessary to find a formula that will give the volume of the parallelepiped spanned
        by <fr:tex>n</fr:tex> vectors.
        <fr:tex display="block">              \begin {align*}                 D \left ( \begin {bmatrix}                     a_{11} &amp; a_{12} &amp;  \cdots  &amp; a_{1n}  \\                      a_{21} &amp; a_{22} &amp;  \cdots  &amp; a_{2n}  \\                       \vdots  &amp;  \vdots  &amp;  \ddots  &amp;  \vdots   \\                      a_{n1} &amp; a_{n2} &amp;  \cdots  &amp; a_{nn}                  \end {bmatrix} \right ) &amp;=  \sum _{i_1=1}^n a_{i_11}D \left ( \begin {bmatrix}                     | &amp; a_{12} &amp;  \cdots  &amp; a_{1n}  \\                      e_{i_1} &amp; a_{22} &amp;  \cdots  &amp; a_{2n}  \\                      | &amp;  \vdots  &amp;  \ddots  &amp;  \vdots   \\                      | &amp; a_{n2} &amp;  \cdots  &amp; a_{nn}                  \end {bmatrix} \right )  \\                   &amp;=  \sum _{i_1=1}^n a_{i_11}  \sum _{i_2=1}^n a_{i_22}D \left ( \begin {bmatrix}                     | &amp; | &amp;  \cdots  &amp; a_{1n}  \\                      e_{i_1} &amp; e_{i_2} &amp;  \cdots  &amp; a_{2n}  \\                      | &amp; | &amp;  \vdots  &amp;  \vdots   \\                      | &amp; | &amp;  \cdots  &amp; a_{nn}                  \end {bmatrix} \right )  \\                  &amp;=  \vdots   \\                   &amp;=  \sum _{i_1, i_2,  \cdots , i_n = 1}^{n} a_{i_11}a_{i_22} \cdots  a_{i_nn}D \left (                      \begin {bmatrix}                         | &amp; | &amp;  &amp; |  \\                          e_{i_1} &amp; e_{i_2} &amp;  \cdots  &amp; e_{i_n}  \\                          | &amp; | &amp;  &amp; |  \\                       \end {bmatrix}                  \right )  \\                   &amp;=  \sum _{ \sigma \in  S_n} a_{ \sigma (1)1} \cdots  a_{ \sigma (n)n}                 D \left (                      \begin {bmatrix}                         | &amp; | &amp;  &amp; |  \\                          e_{ \sigma (1)} &amp; e_{ \sigma (2)} &amp;  \cdots  &amp; e_{ \sigma (n)}  \\                          | &amp; | &amp;  &amp; |  \\                       \end {bmatrix}                  \right )  \\                   &amp;=  \sum _{ \sigma \in  S_n} a_{ \sigma (1)1}a_{ \sigma (2)2} \cdots  a_{ \sigma (n)n}  \text {sgn} ( \sigma )  \\                   &amp;=  \sum _{ \sigma \in  S_n}  \text {sgn} ( \sigma )  \prod _{i=1}^n a_{ \sigma (i)i}              \end {align*}         </fr:tex>
        In the forth step we transform the terms because the value of <fr:tex>D</fr:tex> is zero for any <fr:tex>{i_j} = {i_k}</fr:tex>,
        non-zero terms should be permutation of <fr:tex>S_n</fr:tex>.
    </fr:p><fr:p>
        It&apos;s easy to validate that the following properties of the determinant holds:
        <fr:ul><fr:li><fr:tex>D(AB) = D(A)D(B)</fr:tex></fr:li>
            <fr:li><fr:tex>D(A) = D(A^T)</fr:tex></fr:li></fr:ul>
        The second statement for transpose of <fr:tex>A</fr:tex> indicates that all the properties above also holds for row as well. 
    </fr:p></fr:mainmatter></fr:tree><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>553</fr:anchor><fr:title>Derivatives of Multivariable Functions</fr:title><fr:date><fr:year>2024</fr:year><fr:month>4</fr:month><fr:day>5</fr:day></fr:date><fr:parent>math-0007</fr:parent></fr:frontmatter><fr:mainmatter><fr:p>
        In this section we will introduce the idea of the derivative of a multivariable function. 
        Recall that a function <fr:tex>f: \mathbb {R} \to \mathbb {R}</fr:tex> the derivative of <fr:tex>f</fr:tex> at <fr:tex>x_0 \in \mathbb {R}</fr:tex> is given by
        <fr:tex display="block">             f&apos;(x_0) =  \lim _{h \to  0}  \frac {f(x_0+h) - f(x_0)}{h}         </fr:tex>
        if the limit exists. Now let&apos;s do some transformations:
        <fr:tex display="block">              \begin {align*}                 &amp; f&apos;(x_0) =  \lim _{h \to  0}  \frac {f(x_0+h) - f(x_0)}{h}  \\                   \implies  &amp;  \lim _{h \to  0}  \frac {f(x_0+h) - f(x_0) -f&apos;(x_0)h }{h} = 0  \\                   \implies  &amp;  \lim _{x \to  x_0}  \frac {f(x) - f(x_0) - f&apos;(x_0)(x-x_0)}{x-x_0} = 0  \\                    \implies  &amp;  \lim _{x \to  x_0}  \frac {|f(x) - f(x_0) - f&apos;(x_0)(x-x_0)|}{|x-x_0|} = 0              \end {align*}         </fr:tex></fr:p><fr:p>
        Since <fr:tex>f&apos;(x_0)</fr:tex> represents the slope of the line tangent to the graph of <fr:tex>f</fr:tex> at <fr:tex>(x_0, f(x_0))</fr:tex>,
        differentiability of <fr:tex>f</fr:tex> at <fr:tex>x_0</fr:tex> means that there exists a number <fr:tex>m</fr:tex> st
        <fr:tex display="block">              \lim _{x \to  x_0}  \frac {|f(x) - f(x_0) - m(x-x_0)|}{|x-x_0|} = 0         </fr:tex>
        Now consider the function <fr:tex>T: \mathbb {R} \to \mathbb {R}</fr:tex> where <fr:tex>T(s) = ms</fr:tex>
        <fr:tex display="block">             T(s+t) = m(s+t) = ms + mt = T(s) + T(t)              \\               T(cs) = mcs = c(ms) = cT(s)         </fr:tex>
        then <fr:tex>T</fr:tex> is a linear transformation. In fact <fr:tex>T</fr:tex> is the linear function that most closely approximates the 
        function <fr:tex>f</fr:tex> at the point <fr:tex>(x_0, f(x_0))</fr:tex>. So for <fr:tex>x</fr:tex> values that are very close to <fr:tex>x_0</fr:tex> we have
        <fr:tex display="block">             f(x)  \approx  m(x-x_0) + f(x_0)         </fr:tex></fr:p><fr:p>
        Now let&apos;s generalize the concept of derivatives to functions of the form <fr:tex>f: \mathbb {R} ^n \to \mathbb {R} ^m</fr:tex>.
        We assume the function <fr:tex>f</fr:tex> has the form
        <fr:tex display="block">              \begin {align*}                 &amp;f(x_1, x_2,  \cdots , x_n) =                   \\                   &amp;(f_1(x_1, x_2,  \cdots , x_n), f_2(x_1, x_2,  \cdots , x_n),  \cdots , f_m(x_1, x_2,  \cdots , x_n))              \end {align*}         </fr:tex>
        We want to search for this linear transformation which we will denoted by <fr:tex>Df</fr:tex>,
        that most closely approximates this function <fr:tex>f: \mathbb {R} ^n \to \mathbb {R} ^m</fr:tex> at some specific point <fr:tex>x_0=(x_{1_0}, x_{2_0},  \cdots , x_{n_0})  \in   \mathbb {R} ^n</fr:tex>.
        If <fr:tex>f</fr:tex> is differentiable at <fr:tex>x_0</fr:tex> then there exists a linear transformation <fr:tex>Df(x_0): \mathbb {R} ^n \to \mathbb {R} ^m</fr:tex> such that
        <fr:tex display="block">              \lim _{x \to  x_0}  \frac {                  \lVert                      f(x) - f(x_0) - Df(x_0)(x-x_0)                  \rVert              }{ \lVert x-x_0 \rVert } = 0         </fr:tex>
        The <fr:tex>\lVert \cdot \rVert</fr:tex> represents the <fr:strong>Euclidean norm</fr:strong> of the vector (Multi-dimensional version of the absolute value)
        <fr:tex display="block">              \lVert \vec {x} \rVert  =  \sqrt {x_1^2 + x_2^2 +  \cdots  + x_n^2} =  \sqrt { \sum _{i=1}^n x_i^2}         </fr:tex>
        which is just the length of the vector. This allows us to perform dividing.
    </fr:p><fr:p>
        As before we have
        <fr:tex display="block">             f(x)  \approx  Df(x_0)(x-x_0) + f(x_0)         </fr:tex>
        Now we want to write <fr:tex>Df(x)</fr:tex> as a matrix. Denote the basis of <fr:tex>\mathbb {R} ^n</fr:tex> as <fr:tex>e_j</fr:tex> and the basis of <fr:tex>\mathbb {R} ^m</fr:tex> as <fr:tex>f_i</fr:tex>.
        Then we want to find <fr:tex>a_{ij}</fr:tex> st
        <fr:tex display="block">             Df(x)(e_j) =  \sum _{i=1}^{m} a_{ij} f_j =  \begin {bmatrix}                 a_{1j}  \\  a_{2j}  \\   \vdots   \\  a_{mj}              \end {bmatrix}         </fr:tex>
        In other words, the <fr:tex>i</fr:tex>-th component of the <fr:tex>j</fr:tex>-th column of <fr:tex>Df(x)</fr:tex> is just the <fr:tex>i</fr:tex>-th component of the <fr:tex>Df(x)(e_j)</fr:tex></fr:p><fr:p>
        Recall from vector calculus that given a function <fr:tex>f: \mathbb {R} ^n \to \mathbb {R}</fr:tex> we defined the <fr:strong>partial derivative</fr:strong> of <fr:tex>f</fr:tex> with respect to the <fr:tex>x_j</fr:tex> as 
        <fr:tex display="block">              \frac { \partial  f}{ \partial  x_j} =  \lim _{h \to0 }              \frac {f(x_1, \cdots ,x_j+h, \cdots ,x_n) - f(x_1, \cdots ,x_n)}{h}         </fr:tex>
        Hence we can define the partial derivatives for each <fr:tex>f_i (1 \leq  i \leq  m)</fr:tex> with respect to each <fr:tex>x_j (1 \leq  j \leq  n)</fr:tex>.
        <fr:tex display="block">              \frac { \partial  f_i}{ \partial  x_j} =  \lim _{h \to0 }              \frac {f_i(x_1, \cdots ,x_j+h, \cdots ,x_n) - f_i(x_1, \cdots ,x_n)}{h}         </fr:tex>
        Thus we have
        <fr:tex display="block">              \frac { \partial  f_i}{ \partial  x_j} = a_{ij}         </fr:tex>
        To find <fr:tex>a_{ij}</fr:tex> of <fr:tex>Df(x_0)</fr:tex> we need to find the <fr:tex>i</fr:tex>-th element of <fr:tex>Df(x_0)(e_j)</fr:tex>. Let
        <fr:tex display="block">             x =  \begin {bmatrix}                 x_{1_0}  \\  x_{2_0}  \\   \vdots   \\  x_{n_0}              \end {bmatrix} +  \begin {bmatrix}                 0  \\   \vdots   \\  1  \\   \vdots   \\  0              \end {bmatrix} = x_0 + he_j         </fr:tex>
        We have 
        <fr:tex display="block">              \lim _{x \to  x_0} \frac { \lVert f(x)-f(x_0)-Df(x_0)(he_j) \rVert }{ \lVert he_j \rVert }              \\   \implies                \lim _{h \to0 } \frac { \lVert                  f(x_0+he_j) - f(x_0) -hDf(x_0)(e_j)              \rVert }{ \lVert h \rVert } = 0         </fr:tex>
        The component is given by 
        <fr:tex display="block">              \lim _{h \to0 } \frac { \lVert f_i(x_0+he_j)-f_i(x_0)-ha_{ij} \rVert }{ \lVert h \rVert } = 0              \\   \implies               a_{ij} =  \lim _{h \to0 } \frac {f_i(x_0+he_j) - f_i(x_0)}{h}         </fr:tex>
        which is exactly <fr:tex>\frac { \partial  f_i}{ \partial  x_j}</fr:tex>. Thus the matrix representation of <fr:tex>Df(x)</fr:tex> is given by 
        a matrix called the <fr:strong>Jacobin matrix</fr:strong> of <fr:tex>f</fr:tex>.
        <fr:tex display="block">             Df(x) =  \begin {bmatrix}                  \frac { \partial  f_1}{ \partial  x_1} &amp;  \frac { \partial  f_1}{ \partial  x_2} &amp;  \cdots  &amp;  \frac { \partial  f_1}{ \partial  x_n}  \\                   \frac { \partial  f_2}{ \partial  x_1} &amp;  \frac { \partial  f_2}{ \partial  x_2} &amp;  \cdots  &amp;  \frac { \partial  f_2}{ \partial  x_n}  \\                   \vdots  &amp;  \vdots  &amp;  \ddots  &amp;  \vdots   \\                   \frac { \partial  f_m}{ \partial  x_1} &amp;  \frac { \partial  f_m}{ \partial  x_2} &amp;  \cdots  &amp;  \frac { \partial  f_m}{ \partial  x_n}              \end {bmatrix} =  \left [                  \frac { \partial  f_i}{ \partial  x_j}              \right ]         </fr:tex>
        where <fr:tex>i</fr:tex> ranges row and <fr:tex>j</fr:tex> ranges column.
    </fr:p></fr:mainmatter></fr:tree></fr:mainmatter></fr:tree></fr:backlinks><fr:references></fr:references></fr:backmatter></fr:tree>