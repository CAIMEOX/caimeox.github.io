<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="true" expanded="true" root="false"><fr:frontmatter><fr:anchor>1039</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-000L</fr:addr><fr:route>def-000L.xml</fr:route><fr:title>Linear Combination</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Let <fr:tex>V</fr:tex> be a <fr:link href="def-000H.xml" type="local" addr="def-000H">vector space</fr:link> over a field <fr:tex>F</fr:tex>.
    Let <fr:tex>v_1,  \dots , v_n</fr:tex> be vectors in <fr:tex>V</fr:tex>.
    A <fr:strong>linear combination</fr:strong> of <fr:tex>v_1,  \dots , v_n</fr:tex> is an expression of the form
    <fr:tex display="block">         a_1 v_1 +  \dots  + a_n v_n     </fr:tex>
    where <fr:tex>a_1,  \dots , a_n  \in  F</fr:tex>.
</fr:p></fr:mainmatter><fr:backmatter><fr:contributions></fr:contributions><fr:context><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="false" show-heading="true" show-metadata="true" expanded="true" root="false"><fr:frontmatter><fr:anchor>1037</fr:anchor><fr:taxon>Linear Algebra</fr:taxon><fr:addr>math-0008</fr:addr><fr:route>math-0008.xml</fr:route><fr:title>Matrix Computation</fr:title><fr:date><fr:year>2024</fr:year><fr:month>6</fr:month><fr:day>10</fr:day></fr:date></fr:frontmatter><fr:mainmatter><fr:p>
    This post shows operations and applications over matrix, refers to Wikipedia.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>555</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-0043</fr:addr><fr:route>def-0043.xml</fr:route><fr:title>Transpose</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    The <fr:strong>transpose</fr:strong> of a matrix <fr:tex>A</fr:tex>, denoted by <fr:tex>A^T</fr:tex> is
    the matrix obtained by swapping the rows and columns of <fr:tex>A</fr:tex>.
    It satisfies the following properties:
    <fr:ul><fr:li><fr:tex>(A^T)^T = A</fr:tex></fr:li>
        <fr:li><fr:tex>(A + B)^T = A^T + B^T</fr:tex></fr:li>
        <fr:li><fr:tex>(cA)^T = cA^T</fr:tex></fr:li>
        <fr:li><fr:tex>(AB)^T = B^TA^T</fr:tex></fr:li></fr:ul></fr:p></fr:mainmatter></fr:tree><fr:p>
    If the following condition satisfies:
    <fr:tex display="block">         a_{ij} = a_{ji}  \quad   \forall  i,j     </fr:tex>
    Then the matrix is called symmetric.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>556</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-001N</fr:addr><fr:route>def-001N.xml</fr:route><fr:title>Symmetric Matrix</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    A square matrix <fr:tex>A</fr:tex> is symmetric if it is equal to its transpose:
    <fr:tex display="block">         A = A^T     </fr:tex>
    This also implies <fr:tex>A^{-1} A^T = I</fr:tex></fr:p></fr:mainmatter></fr:tree><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>557</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-0044</fr:addr><fr:route>def-0044.xml</fr:route><fr:title>Determinant</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    The determinant of a <fr:tex>n \times  n</fr:tex> square matrix <fr:tex>A</fr:tex> is commonly denoted <fr:tex>\det  A</fr:tex> or <fr:tex>|A|</fr:tex>.
    It satisfies the following properties:
    <fr:ul><fr:li><fr:tex>\det  A^T =  \det  A</fr:tex></fr:li>
        <fr:li><fr:tex>\det  AB =  \det  A  \det  B</fr:tex></fr:li>
        <fr:li><fr:tex>\det   \lambda  A =  \lambda ^n  \det  A</fr:tex></fr:li></fr:ul></fr:p></fr:mainmatter></fr:tree><fr:p>
    To compute the inverse of a matrix, we need <fr:strong>Adjugate matrix</fr:strong>.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>558</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-0045</fr:addr><fr:route>def-0045.xml</fr:route><fr:title>First Minor and Cofactor</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    If <fr:tex>A</fr:tex> is a square matrix, then the <fr:strong>minor</fr:strong> of the entry in the i-th row and j-th 
    column (also called the <fr:tex>(i, j)</fr:tex> minor, or a first minor) is the <fr:strong>determinant</fr:strong> of 
    the sub-matrix formed by deleting the i-th row and j-th column.
    The <fr:tex>(i, j)</fr:tex> minor is denoted as <fr:tex>M_{ij}</fr:tex>.
    The <fr:strong>Cofactor</fr:strong> is obtained by multiplying the minor by <fr:tex>(-1)^{i+j}</fr:tex>.
</fr:p></fr:mainmatter></fr:tree><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>559</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-0046</fr:addr><fr:route>def-0046.xml</fr:route><fr:title>Cofactor Matrix</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    The matrix formed by all of the <fr:link href="def-0045.xml" type="local" addr="def-0045">cofactors</fr:link> of a square matrix <fr:tex>A</fr:tex> is called the cofactor matrix,
    or <fr:strong>comatrix</fr:strong>:
    <fr:tex display="block">         C =  \left [               \begin {array}{cccc}                 C_{11} &amp; C_{12} &amp;  \cdots  &amp; C_{1n}  \\                  C_{21} &amp; C_{22} &amp;  \cdots  &amp; C_{2n}  \\                   \vdots  &amp;  \vdots  &amp;  \ddots  &amp;  \vdots   \\                  C_{n1} &amp; C_{n2} &amp;  \cdots  &amp; C_{nn}              \end {array}          \right ]     </fr:tex>
    The <fr:strong>Adjugate matrix</fr:strong> of <fr:tex>A</fr:tex> is the transpose of the cofactor matrix.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    Then the inverse of <fr:tex>A</fr:tex> is the transpose of the cofactor matrix times the reciprocal of the determinant of <fr:tex>A</fr:tex>:
    <fr:tex display="block">         A^{-1} =  \frac {1}{ \det  A}  \cdot   \text {adj} A =  \frac {1}{ \det  A}  \cdot  C^T     </fr:tex></fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>560</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-0047</fr:addr><fr:route>def-0047.xml</fr:route><fr:title>Singular Matrix</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    A square matrix that is not <fr:strong>invertible</fr:strong> is called <fr:strong>singular</fr:strong> or degenerate
</fr:p></fr:mainmatter></fr:tree><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>562</fr:anchor><fr:title>An important property of the inverse of a matrix</fr:title><fr:date><fr:year>2024</fr:year><fr:month>6</fr:month><fr:day>10</fr:day></fr:date><fr:parent>math-0008</fr:parent></fr:frontmatter><fr:mainmatter><fr:p><fr:tex display="block">             A  \cdot   \text {adj} A =  \text {adj} A  \cdot  A =  \det  A  \cdot  I         </fr:tex></fr:p>
 
   
   <fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="false" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>561</fr:anchor><fr:taxon>Proof</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>6</fr:month><fr:day>10</fr:day></fr:date></fr:frontmatter><fr:mainmatter>
        Let <fr:tex>A  \cdot   \text {adj} A = (b_{ij})</fr:tex> and we have
        <fr:tex display="block">             b_{ij} = a_{i1}A_{j1} + a_{i2}A_{j2} +  \cdots  + a_{in}A_{jn} =  \delta _{ij}  \cdot   \det  A         </fr:tex>
        Hence we have <fr:tex>A  \cdot   \text {adj} A =  \det  A  \cdot  I</fr:tex> 
    </fr:mainmatter></fr:tree>
 
</fr:mainmatter></fr:tree><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>564</fr:anchor><fr:title>Matrix Polynomial and Computation</fr:title><fr:date><fr:year>2024</fr:year><fr:month>6</fr:month><fr:day>10</fr:day></fr:date><fr:parent>math-0008</fr:parent></fr:frontmatter><fr:mainmatter><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>563</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-0048</fr:addr><fr:route>def-0048.xml</fr:route><fr:title>Matrix Polynomial</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    A <fr:strong>matrix polynomial</fr:strong> is a polynomial with square matrices as variables.
    The general form of a matrix polynomial is:
    <fr:tex display="block">         P(A) =  \sum _{i=0}^{n} a_i A^i     </fr:tex>
    where <fr:tex>A^0 = I</fr:tex> is the identity matrix.
</fr:p></fr:mainmatter></fr:tree><fr:p>
        If <fr:tex>A</fr:tex> is a diagonal matrix, then the polynomial of <fr:tex>A</fr:tex> is the diagonal matrix of the polynomial of the diagonal elements of <fr:tex>A</fr:tex>.
        <fr:tex display="block">             p(A) =  \begin {bmatrix}                 p(a_{11}) &amp; 0 &amp;  \cdots  &amp; 0  \\                  0 &amp; p(a_{22}) &amp;  \cdots  &amp; 0  \\                   \vdots  &amp;  \vdots  &amp;  \ddots  &amp;  \vdots   \\                  0 &amp; 0 &amp;  \cdots  &amp; p(a_{nn})              \end {bmatrix}         </fr:tex></fr:p><fr:p>
        If <fr:tex>A = P \Lambda  P^{-1}</fr:tex>, then <fr:tex>A^k = P  \Lambda  ^k P^{-1}</fr:tex> and hence
        <fr:tex display="block">             p(A) = a_0 I + a_1 A + a_2 A^2 +  \cdots  + a_n A^n = P  \Lambda  P^{-1}         </fr:tex></fr:p></fr:mainmatter></fr:tree><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>567</fr:anchor><fr:title>Solving a Linear System</fr:title><fr:date><fr:year>2024</fr:year><fr:month>6</fr:month><fr:day>10</fr:day></fr:date><fr:parent>math-0008</fr:parent></fr:frontmatter><fr:mainmatter><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>565</fr:anchor><fr:taxon>Theorem</fr:taxon><fr:addr>thm-0011</fr:addr><fr:route>thm-0011.xml</fr:route><fr:title>Cramer&apos;s rule</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Consider a system of <fr:tex>n</fr:tex> linear equations for <fr:tex>n</fr:tex> unknowns, represented in matrix multiplication form as follows:
    <fr:tex display="block">         A  \cdot  X = B     </fr:tex>
    where <fr:tex>A</fr:tex> is a square matrix of order <fr:tex>n</fr:tex>, <fr:tex>X</fr:tex> is a column matrix of order <fr:tex>n</fr:tex> and <fr:tex>B</fr:tex> is a column matrix of order <fr:tex>n</fr:tex>.
    <fr:tex display="block">         X =  \begin {bmatrix} x_1  \\  x_2  \\   \vdots   \\  x_n  \end {bmatrix}     </fr:tex>
    The Cramer&apos;s rule states that the solution to the system of equations is given by:
    <fr:tex display="block">         x_i =  \frac { \text {det}(A_i)}{ \text {det}(A)}     </fr:tex>
    where <fr:tex>A_i</fr:tex> is the matrix obtained by replacing the <fr:tex>i</fr:tex>th column of <fr:tex>A</fr:tex> by <fr:tex>B</fr:tex>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
        Matrix partitioning is the process of dividing a matrix into smaller submatrices. 
        This is often done to simplify the computation of matrix operations, such as matrix multiplication.
    </fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>566</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-0049</fr:addr><fr:route>def-0049.xml</fr:route><fr:title>Matrix Partitioning</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Let <fr:tex>A  \in   \mathbb {C} ^{m \times  n} </fr:tex>. A <fr:strong>partitioning</fr:strong> of <fr:tex>A</fr:tex> is a representation of <fr:tex>A</fr:tex> in the form
    <fr:tex display="block">         A =  \begin {bmatrix}             A_{11} &amp; A_{12} &amp;  \cdots  &amp; A_{1q}  \\              A_{21} &amp; A_{22} &amp;  \cdots  &amp; A_{2q}  \\               \vdots  &amp;  \vdots  &amp;  \ddots  &amp;  \vdots   \\              A_{p1} &amp; A_{p2} &amp;  \cdots  &amp; A_{pq}          \end {bmatrix}     </fr:tex>
    where <fr:tex>A_{ij}  \in   \mathbb {C} ^{m_i  \times  n_j} </fr:tex> for <fr:tex>1  \leq  i  \leq  p</fr:tex> and <fr:tex>1  \leq  j  \leq  q</fr:tex> such that
    <fr:tex display="block">          \sum _{i=1}^p m_i = m  \quad   \text {and}  \quad   \sum _{j=1}^q n_j = n.     </fr:tex>
    The partitioned matrix operations are similar to the operations on the normal matrix. 
</fr:p></fr:mainmatter></fr:tree><fr:p>
        If the partitioned matrix is formed as diagonal blocks, then we can compute the determinant of the matrix by the following formula:
        <fr:tex display="block">              \det  A =  \det  A_1  \cdot   \det  A_2  \cdots   \det  A_n         </fr:tex>
        And the inverse of the matrix is
        <fr:tex display="block">             A^{-1} =  \begin {bmatrix}                 A_1^{-1} &amp; O &amp;  \cdots  &amp; O  \\                  O &amp; A_2^{-1} &amp;  \cdots  &amp; O  \\                   \vdots  &amp;  \vdots  &amp;  \ddots  &amp;  \vdots   \\                  O &amp; O &amp;  \cdots  &amp; A_n^{-1}              \end {bmatrix}         </fr:tex></fr:p><fr:p>
        The column partitioning of matrix is useful. 
        If we have <fr:tex>m \times  s</fr:tex> matrix <fr:tex>A = (a_{ij})</fr:tex> and <fr:tex>s \times  n</fr:tex> matrix <fr:tex>B=(b_{ij})</fr:tex>,
        their product can be written:
        <fr:tex display="block">             AB =  \begin {bmatrix} A_1  \\  A_2  \\   \vdots  A_m  \end {bmatrix}              \begin {bmatrix}                 B_1 &amp; B_2 &amp;  \cdots  &amp; B_n              \end {bmatrix} =               \begin {bmatrix}                 A_1B_1 &amp; A_1B_2 &amp;  \cdots  &amp; A_1B_n  \\                  A_2B_1 &amp; A_2B_2 &amp;  \cdots  &amp; A_2B_n  \\                   \vdots  &amp;  \vdots  &amp;  \ddots  &amp;  \vdots   \\                  A_mB_1 &amp; A_mB_2 &amp;  \cdots  &amp; A_mB_n              \end {bmatrix}         </fr:tex>
        We can show that <fr:tex>A=O \iff  A^TA=O</fr:tex>.
    </fr:p></fr:mainmatter></fr:tree><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>570</fr:anchor><fr:taxon>Section</fr:taxon><fr:title>Matrix Transformation</fr:title><fr:date><fr:year>2024</fr:year><fr:month>6</fr:month><fr:day>10</fr:day></fr:date><fr:parent>math-0008</fr:parent></fr:frontmatter><fr:mainmatter><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>568</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-004A</fr:addr><fr:route>def-004A.xml</fr:route><fr:title>Elementary Operations</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    There are three types of elementary matrices, which correspond to three types of row operations
    (respectively, column operations, row operations are equivalent to multiplying on the left by the
    corresponding elementary matrix, and column operations are equivalent to multiplying on the right
    by the corresponding elementary matrix):
    <fr:ul><fr:li><fr:strong>Row switching</fr:strong>: A row within the matrix can be switched with another row.
            <fr:tex display="block">                 P_{i,j} =  \begin {bmatrix}                     1  \\                      &amp;  \ddots   \\                      &amp; &amp; 0 &amp; &amp;  1  \\                       &amp; &amp; &amp;  \ddots   \\                       &amp; &amp; 1 &amp; &amp; 0  \\                       &amp; &amp; &amp; &amp; &amp;  \ddots   \\                      &amp; &amp; &amp; &amp; &amp; &amp; 1                  \end {bmatrix}             </fr:tex></fr:li>
        <fr:li><fr:strong>Row multiplication</fr:strong>: Each element in a row can be multiplied by a non-zero constant.
            <fr:tex display="block">                 D_i(k) =  \text {diag} (1,  \cdots , k,  \cdots , 1)             </fr:tex></fr:li>
        <fr:li><fr:strong>Row additio</fr:strong>: A row can be replaced by the sum of that row and a multiple of another row.
            <fr:tex display="block">                 T_{i,j} =  \begin {bmatrix}                     1  \\                      &amp;  \ddots   \\                      &amp; &amp; 1 &amp; &amp; k  \\                       &amp; &amp; &amp;  \ddots   \\                       &amp; &amp; &amp; &amp; 1  \\                       &amp; &amp; &amp; &amp; &amp;  \ddots   \\                      &amp; &amp; &amp; &amp; &amp; &amp; 1                  \end {bmatrix}             </fr:tex></fr:li></fr:ul></fr:p></fr:mainmatter></fr:tree><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>569</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-004B</fr:addr><fr:route>def-004B.xml</fr:route><fr:title>Column / Row Equivalence</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Two matrices <fr:tex>A,B</fr:tex> are column / row equivalent if one can 
    be obtained from the other by a finite sequence of <fr:link href="def-004A.xml" type="local" addr="def-004A">elementary operations</fr:link>,
    denoted <fr:tex>A  \sim  B</fr:tex>.
    The column / row equivalence is an <fr:link href="def-000X.xml" type="local" addr="def-000X">equivalence relation</fr:link>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
        We can show that a matrix <fr:tex>A</fr:tex> is invertible iff there are finite elementary matrices
        <fr:tex>E_1, E_2,  \cdots , E_n</fr:tex> such that
        <fr:tex display="block">             A = E_1E_2 \cdots  E_n         </fr:tex></fr:p><fr:p>
        From above we can deduce that a square matrix <fr:tex>A</fr:tex> is invertible iff <fr:tex>A \sim  E</fr:tex>.
        This trick can be used for solving a linear system and computing the inverse of a matrix.
        For instance, given <fr:tex>AX=B</fr:tex> we can solve <fr:tex>X</fr:tex> by the following steps:
        Let <fr:tex>P</fr:tex> be a matrix such that <fr:tex>PA=I</fr:tex> where <fr:tex>I</fr:tex> is the identity matrix.
        Hence <fr:tex>P = A^{-1}</fr:tex> and we have <fr:tex>X = PB</fr:tex>, we can do elementary operations over matrix <fr:tex>(A, B)</fr:tex>
        to get the solution of <fr:tex>X</fr:tex>.
    </fr:p></fr:mainmatter></fr:tree><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>573</fr:anchor><fr:title>Rank</fr:title><fr:date><fr:year>2024</fr:year><fr:month>6</fr:month><fr:day>10</fr:day></fr:date><fr:parent>math-0008</fr:parent></fr:frontmatter><fr:mainmatter><fr:p>
        Now let&apos;s talk about the concept of <fr:strong>rank</fr:strong>.
        In linear algebra, the (column)<fr:strong>rank</fr:strong> of a matrix <fr:tex>A</fr:tex> is the dimension of the vector space 
        generated (or spanned) by its columns.
    </fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>571</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-004J</fr:addr><fr:route>def-004J.xml</fr:route><fr:title>Rank</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    The rank of a linear map or operator <fr:tex>\Phi</fr:tex> is defined as the dimension of its range:
    <fr:tex display="block">          \text {rank} (M) \equiv \dim ( \text {range } (M))     </fr:tex>
    Note that the range (<fr:strong>column space</fr:strong>) of a matrix <fr:tex>M</fr:tex> is the <fr:link href="def-000M.xml" type="local" addr="def-000M">span</fr:link> of its column vectors.
</fr:p></fr:mainmatter></fr:tree><fr:p>
        Similarly we can define the row rank of a matrix. A fundamental result in linear algebra is 
        that the column rank and the row rank are always equal. Hence we can simply call it the rank of a matrix.
        If the column rank equals to the column size, we say that the matrix is full column rank.
    </fr:p><fr:p>
        A common approach to finding the rank of a matrix is to reduce it to a simpler form, 
        generally <fr:strong>row echelon form</fr:strong>, by elementary row operations.
        Row operations do not change the row space (hence do not change the row rank), and, being 
        invertible, map the column space to an isomorphic space (hence do not change the column rank).
        <fr:tex display="block">             A  \sim  B  \implies   \text {rank} (A) =  \text {rank} (B)              \\                \text {rank} (A) =  \text {rank} (A^T)         </fr:tex>
        Once in row echelon form, the rank is clearly the same for both row rank and column rank, 
        and equals to the number of <fr:strong>pivots</fr:strong> (or basic columns) and also the number of non-zero rows.
    </fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>572</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-004I</fr:addr><fr:route>def-004I.xml</fr:route><fr:title>Row Echelon Form</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    A matrix is in <fr:strong>row echelon form</fr:strong> if
    <fr:ul><fr:li>All rows having only zero entries are at the bottom</fr:li>
        <fr:li>
            The leading entry (that is, the <fr:strong>left-most</fr:strong> nonzero entry) of every nonzero row, called
            the <fr:strong>pivot</fr:strong>, is on the right of the leading entry of every row above.
        </fr:li></fr:ul>
    Some texts add the condition that the leading coefficient must be <fr:tex>1</fr:tex>
    while others require this only in <fr:strong>reduced row echelon form</fr:strong>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
        For instance, matrix <fr:tex>A =  \begin {bmatrix}1&amp;2&amp;1 \\ -2&amp;-3&amp;1 \\ 3&amp;5&amp;0 \end {bmatrix}</fr:tex> can be transformed into reduced row-echelon form:
        <fr:tex display="block">\begin {aligned}{ \begin {bmatrix}1&amp;2&amp;1 \\ -2&amp;-3&amp;1 \\ 3&amp;5&amp;0 \end {bmatrix}}&amp; \xrightarrow  {2R_{1}+R_{2} \to  R_{2}} { \begin {bmatrix}1&amp;2&amp;1 \\ 0&amp;1&amp;3 \\ 3&amp;5&amp;0 \end {bmatrix}} \xrightarrow  {-3R_{1}+R_{3} \to  R_{3}} { \begin {bmatrix}1&amp;2&amp;1 \\ 0&amp;1&amp;3 \\ 0&amp;-1&amp;-3 \end {bmatrix}} \\ &amp; \xrightarrow  {R_{2}+R_{3} \to  R_{3}}  \, \, { \begin {bmatrix}1&amp;2&amp;1 \\ 0&amp;1&amp;3 \\ 0&amp;0&amp;0 \end {bmatrix}} \xrightarrow  {-2R_{2}+R_{1} \to  R_{1}} { \begin {bmatrix}1&amp;0&amp;-5 \\ 0&amp;1&amp;3 \\ 0&amp;0&amp;0 \end {bmatrix}}~. \end {aligned}</fr:tex>
        The final matrix has two non-zero rows and thus the rank of matrix <fr:tex>A</fr:tex> is <fr:tex>2</fr:tex>.
    </fr:p><fr:p>
        The determinantal rank of a matrix is the order of the largest non-zero minor of the matrix.
        It is also the number of non-zero eigenvalues of the matrix. 
        This does not give an efficient way of computing the rank, but it is useful theoretically: 
        a single non-zero minor witnesses a lower bound for the rank of the matrix,
        which can be useful to prove that certain operations do not lower the rank of a matrix.
    </fr:p><fr:p>
        We can conclude the properties of rank:
        <fr:tex display="block">              \begin {align*}                 &amp;0  \leq   \text {rank} (A_{m \times  n})  \leq   \min (m,n)  \\                  &amp; \text {rank} (A^T) =  \text {rank} (A)  \\                   &amp; A  \sim  B  \implies   \text {rank} (A)= \text {rank} (B)  \\                   &amp; P, Q  \text { is invertible}  \implies   \text {rank} (PAQ) =  \text {rank} (A)  \\                   &amp;  \max ( \text {rank} (A),  \text {rank} (B))  \leq   \text {rank} (A, B)  \leq   \text {rank} (A) +  \text {rank} (B)  \\                   &amp;  \text {rank} (A+B)  \leq   \text {rank} (A) +  \text {rank} (B)  \\                  &amp;  \text {rank} (AB)  \leq   \min ( \text {rank} (A),  \text {rank} (B))  \\                   &amp; A_{m \times  n}B_{n \times  l} = O \implies   \text {rank} (A) +  \text {rank} (B)  \leq  n              \end {align*}         </fr:tex></fr:p><fr:p>
        With such properties we can prove an important theorem:
        <fr:tex display="block">             AB = O  \land  A  \text { is full rank}  \implies  B = O         </fr:tex>
        which is known as the cancellation law of matrix multiplication.
    </fr:p><fr:p>
        The rank of a matrix is also related to the solution of a linear system.
        If the rank of the coefficient matrix is less than the rank of the augmented matrix, 
        then the system is inconsistent. Hence there does not exist a solution.
        Similarly we have the following discussion: For a <fr:tex>n</fr:tex>-variable linear system <fr:tex>Ax = b</fr:tex>
        <fr:ul><fr:li>
                If <fr:tex>\text {rank} (A) &lt;  \text {rank} (A,b)</fr:tex>, the solution does not exist.
            </fr:li>
            <fr:li>
                If <fr:tex>\text {rank} (A)= \text {rank} (A,b)=n</fr:tex>, there is a unique solution.
            </fr:li>
            <fr:li>
                Ig <fr:tex>\text {rank} (A)= \text {rank} (A,b)&lt;n</fr:tex>, there are infinite solutions.
            </fr:li></fr:ul>
        This leads two fundamental theorems of linear systems:
        <fr:ul><fr:li><fr:tex>n</fr:tex>-variable regular linear system <fr:tex>Ax = 0</fr:tex> has a non-trivial solution iff <fr:tex>\text {rank} (A) &lt; n</fr:tex>.
            </fr:li>
            <fr:li>
                Matrix equation <fr:tex>AX = B</fr:tex> has solutions iff <fr:tex>\text {rank} (A) =  \text {rank} (A,B)</fr:tex>.
            </fr:li></fr:ul></fr:p></fr:mainmatter></fr:tree><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>576</fr:anchor><fr:title>Linear Combinations</fr:title><fr:date><fr:year>2024</fr:year><fr:month>6</fr:month><fr:day>10</fr:day></fr:date><fr:parent>math-0008</fr:parent></fr:frontmatter><fr:mainmatter><fr:p></fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>574</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-000L</fr:addr><fr:route>def-000L.xml</fr:route><fr:title>Linear Combination</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Let <fr:tex>V</fr:tex> be a <fr:link href="def-000H.xml" type="local" addr="def-000H">vector space</fr:link> over a field <fr:tex>F</fr:tex>.
    Let <fr:tex>v_1,  \dots , v_n</fr:tex> be vectors in <fr:tex>V</fr:tex>.
    A <fr:strong>linear combination</fr:strong> of <fr:tex>v_1,  \dots , v_n</fr:tex> is an expression of the form
    <fr:tex display="block">         a_1 v_1 +  \dots  + a_n v_n     </fr:tex>
    where <fr:tex>a_1,  \dots , a_n  \in  F</fr:tex>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
        Given two sets of vectors, we say that they are eqaul iff their elements can write as linear combinations of each other.
        This condition can be written using rank:
        <fr:tex display="block">              \text {rank} (A) =  \text {rank} (B) =  \text {rank} (A, B)         </fr:tex></fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>575</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-000P</fr:addr><fr:route>def-000P.xml</fr:route><fr:title>Linearly independent</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    A set of vectors <fr:tex>\{ v_1,  \dots , v_n \}</fr:tex> is called <fr:strong>linearly independent</fr:strong> if
    <fr:tex display="block">a_1 v_1 +  \dots  + a_n v_n = 0</fr:tex>
    implies that <fr:tex>a_1 =  \dots  = a_n = 0</fr:tex>.
    The trivial case of <fr:tex>\{ 0 \}</fr:tex> is also considered linearly independent.
</fr:p></fr:mainmatter></fr:tree></fr:mainmatter></fr:tree></fr:mainmatter></fr:tree><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="false" show-heading="true" show-metadata="true" expanded="true" root="false"><fr:frontmatter><fr:anchor>1038</fr:anchor><fr:taxon>Linear Algebra</fr:taxon><fr:addr>math-0002</fr:addr><fr:route>math-0002.xml</fr:route><fr:title>Finite Dimensional Vector Space</fr:title><fr:date><fr:year>2024</fr:year><fr:month>1</fr:month><fr:day>26</fr:day></fr:date></fr:frontmatter><fr:mainmatter><fr:p>
    This note introduces the concept of finite-dimensional vector space.
    Refer to <fr:link href="linear-algebra-2015.xml" type="local" addr="linear-algebra-2015">Linear Algebra Done Right</fr:link>.
</fr:p><fr:p>
    Adding up scalar mulitples of vectors in a list gives a linear combination.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>391</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-000L</fr:addr><fr:route>def-000L.xml</fr:route><fr:title>Linear Combination</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Let <fr:tex>V</fr:tex> be a <fr:link href="def-000H.xml" type="local" addr="def-000H">vector space</fr:link> over a field <fr:tex>F</fr:tex>.
    Let <fr:tex>v_1,  \dots , v_n</fr:tex> be vectors in <fr:tex>V</fr:tex>.
    A <fr:strong>linear combination</fr:strong> of <fr:tex>v_1,  \dots , v_n</fr:tex> is an expression of the form
    <fr:tex display="block">         a_1 v_1 +  \dots  + a_n v_n     </fr:tex>
    where <fr:tex>a_1,  \dots , a_n  \in  F</fr:tex>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    To talk about a structure, we usually define a collection of this structure.
    Hence we have span for linear combinations.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>392</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-000M</fr:addr><fr:route>def-000M.xml</fr:route><fr:title>Linear Span</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Let <fr:tex>V</fr:tex> be a vector space over a field <fr:tex>F</fr:tex>.
    Let <fr:tex>v_1,  \dots , v_n</fr:tex> be vectors in <fr:tex>V</fr:tex>.
    The <fr:strong>span</fr:strong> of <fr:tex>v_1,  \dots , v_n</fr:tex> is defined as
    <fr:tex display="block">          \text {span} (v_1,  \dots , v_n) =  \{ a_1 v_1 +  \dots  + a_n v_n  \mid  a_i  \in  F \}      </fr:tex>
    The span of empty set is defined to be <fr:tex>\{ 0 \}</fr:tex>.    
</fr:p><fr:p>
    If <fr:tex>\text {span} (v_1,  \dots , v_n) = V</fr:tex>, we say that <fr:tex>v_1,  \dots , v_n</fr:tex> <fr:strong>spans</fr:strong> <fr:tex>V</fr:tex>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    Suppose we have span <fr:tex>S= \text {span} (v_1,  \dots , v_n)</fr:tex>. (Span is trivially a subspace.)
    Obviously for all <fr:tex>v_j (1  \leq  j  \leq  n)</fr:tex>, <fr:tex>v_j  \in  S</fr:tex>.
    Because subspaces are closed under scalar multiplication and addition, every
    subspace of <fr:tex>V</fr:tex> containing <fr:tex>v_1,  \dots , v_n</fr:tex> must contain <fr:tex>S</fr:tex>.
    Thus we conclude that <fr:tex>S</fr:tex> is the smallest subspace containing <fr:tex>v_1,  \dots , v_n</fr:tex>.
</fr:p><fr:p>
    The discussion about <fr:strong>spans</fr:strong> leads to a key definition in linear algebra.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>393</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-000N</fr:addr><fr:route>def-000N.xml</fr:route><fr:title>Finite-Dimensional Vector Space</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    A <fr:link href="def-000H.xml" type="local" addr="def-000H">vector space</fr:link> <fr:tex>V</fr:tex> is called <fr:strong>finite-dimensional</fr:strong> if some <fr:link href="def-000G.xml" type="local" addr="def-000G">list</fr:link> of vectors <fr:tex>v_1,  \dots , v_n</fr:tex> <fr:link href="def-000M.xml" type="local" addr="def-000M">spans</fr:link> <fr:tex>V</fr:tex>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    The opposite of finite-dimensional is infinite-dimensional.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>394</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-000O</fr:addr><fr:route>def-000O.xml</fr:route><fr:title>Infinite-dimensional vector space</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    A vector space <fr:tex>V</fr:tex> is called <fr:strong>infinite-dimensional</fr:strong> if it is not <fr:link href="def-000N.xml" type="local" addr="def-000N">finite-dimensional</fr:link>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    Consider the situation that there is only one way to
    express a vector <fr:tex>v</fr:tex> as a linear combination of vectors in a list <fr:tex>v_1,  \dots , v_n</fr:tex>.
    What property of the list <fr:tex>v_1,  \dots , v_n</fr:tex> does this situation imply? The answer is
    linear independence.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>395</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-000P</fr:addr><fr:route>def-000P.xml</fr:route><fr:title>Linearly independent</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    A set of vectors <fr:tex>\{ v_1,  \dots , v_n \}</fr:tex> is called <fr:strong>linearly independent</fr:strong> if
    <fr:tex display="block">a_1 v_1 +  \dots  + a_n v_n = 0</fr:tex>
    implies that <fr:tex>a_1 =  \dots  = a_n = 0</fr:tex>.
    The trivial case of <fr:tex>\{ 0 \}</fr:tex> is also considered linearly independent.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    If some vectors are not linearly independent, then there are more than one way to
    express a vector as a linear combination of vectors in the list. This leads to 
    the following definition.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>396</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-000Q</fr:addr><fr:route>def-000Q.xml</fr:route><fr:title>Linearly dependent</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    A set of vectors <fr:tex>\{ v_1,  \dots , v_n \}</fr:tex> is called <fr:strong>linearly dependent</fr:strong> if
    <fr:tex display="block">a_1 v_1 +  \dots  + a_n v_n = 0</fr:tex>
    for some <fr:tex>a_1,  \dots , a_n  \in   \mathbb {F}</fr:tex> with at least one <fr:tex>a_i  \neq  0</fr:tex> (not all <fr:tex>0</fr:tex>).
</fr:p></fr:mainmatter></fr:tree><fr:p>
    The following lemma is a direct consequence of the definition of linear independence.
    It states that for a given linearly dependent list, we can always remove a vector
    without changing the span.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>397</fr:anchor><fr:taxon>Lemma</fr:taxon><fr:addr>thm-0001</fr:addr><fr:route>thm-0001.xml</fr:route><fr:title>Linear Dependence Lemma</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Let <fr:tex>v_1,  \dots , v_n</fr:tex> be vectors in a vector space <fr:tex>V</fr:tex> over a field <fr:tex>\mathbb {F}</fr:tex>.
    If <fr:tex>v_1,  \dots , v_n</fr:tex> are linearly dependent, then there exists <fr:tex>1  \leq  i  \leq  n</fr:tex> such that
    <fr:ul><fr:li><fr:tex>v_i  \in   \text {span} (v_1,  \dots , v_{i-1})</fr:tex></fr:li>
        <fr:li>Remove <fr:tex>v_i</fr:tex> from the list <fr:tex>v_1,  \dots , v_n</fr:tex> and the span does not change</fr:li></fr:ul></fr:p></fr:mainmatter></fr:tree><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>398</fr:anchor><fr:taxon>Lemma</fr:taxon><fr:addr>thm-0002</fr:addr><fr:route>thm-0002.xml</fr:route><fr:title>Length of linearly independent list <fr:tex>\leq</fr:tex> length of spanning list</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    In a finite dimensional vector space, the length of a linearly independent list is less than or equal to the length of a spanning list.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    We have discussed linear independent lists and spanning lists.
    Now we are ready to define a basis.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>399</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-000R</fr:addr><fr:route>def-000R.xml</fr:route><fr:title>Basis</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    A basis of <fr:tex>V</fr:tex> is a list of vectors in <fr:tex>V</fr:tex>
    that is linearly independent and spans <fr:tex>V</fr:tex>. 
</fr:p><fr:p><fr:strong>Criterion for basis</fr:strong>
    A list of vectors <fr:tex>\{ v_1,  \dots , v_n \}</fr:tex> is a basis of <fr:tex>V</fr:tex> if and only if
    every <fr:tex>v  \in  V</fr:tex> can be written <fr:strong>uniquely</fr:strong> as a linear combination of <fr:tex>v_1,  \dots , v_n</fr:tex>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    For instance, we have standard basis <fr:tex>\{ e_1,  \dots , e_n \}</fr:tex> for <fr:tex>\mathbb {F}^n</fr:tex>,
    where <fr:tex>e_i</fr:tex> is the vector with <fr:tex>1</fr:tex> at <fr:tex>i</fr:tex>-th position and <fr:tex>0</fr:tex> elsewhere.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>400</fr:anchor><fr:taxon>Theorem</fr:taxon><fr:addr>thm-0005</fr:addr><fr:route>thm-0005.xml</fr:route><fr:title>Spanning List contains a basis</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Every spanning list in a vector space can be reduced to a basis.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    From the <fr:link href="thm-0005.xml" type="local" addr="thm-0005">theorem</fr:link> we can infer a corollary.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>401</fr:anchor><fr:taxon>Corollary</fr:taxon><fr:addr>thm-0006</fr:addr><fr:route>thm-0006.xml</fr:route><fr:title>Basis of finite-dimensional vector space</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Every finite-dimensional vector space has a basis.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    The next result states for a spanning list can be reduced to a basis.
    We can adjoin one or more vectors to a linearly independent list to form a basis.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>402</fr:anchor><fr:taxon>Theorem</fr:taxon><fr:addr>thm-0007</fr:addr><fr:route>thm-0007.xml</fr:route><fr:title>Linearly dependent list extends to a basis</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Every linearly independent list of vectors in  a finite-dimensional vector space can be extended to a basis.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    Remind the definition of <fr:link href="der-000K" type="external">direct sum</fr:link>, we can now show that
    every subspace of a finite-dimensional vecrtor space can be paired
    with another subspace to form a direct sum of the whole space.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>403</fr:anchor><fr:taxon>Theorem</fr:taxon><fr:addr>thm-0008</fr:addr><fr:route>thm-0008.xml</fr:route><fr:title>Direct Sum of Subspaces of <fr:tex>V</fr:tex></fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Suppose <fr:tex>V</fr:tex> is a finite dimensional vector space,
    and <fr:tex>U</fr:tex> is a subspace of <fr:tex>V</fr:tex>.
    Then there exists a subspace <fr:tex>W</fr:tex> of <fr:tex>V</fr:tex> such that
    <fr:tex>V = U  \oplus  W</fr:tex>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    This post discusses about <fr:em>finite-dimensional vector space</fr:em>.
    But we have not yet defined what is dimension.
    We tempted to define the dimension as the length of basis intuitively.
    With this definition we should prove its well-definedness.
    That is, every basis has the same length.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>404</fr:anchor><fr:taxon>Theorem</fr:taxon><fr:addr>thm-0009</fr:addr><fr:route>thm-0009.xml</fr:route><fr:title>Basis length is invariant</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Let <fr:tex>V</fr:tex> be a finite-dimensional vector space.
    Then every basis of <fr:tex>V</fr:tex> has the same length.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    This can be proved by <fr:link href="thm-0002.xml" type="local" addr="thm-0002">Lemma 8</fr:link>.
    Now we can formally define the dimension of such spaces.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>405</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-001V</fr:addr><fr:route>def-001V.xml</fr:route><fr:title>Dimension</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    The <fr:strong>dimension</fr:strong> of a finite-dimensional vector space <fr:tex>V</fr:tex> is the length of any basis of the vector space.
    Denoted by <fr:tex>\dim  V</fr:tex>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    Every subspace of a finite-dimensional vector space is also finite-dimensional.
    Hence we can talk about the dimension of a subspace.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>406</fr:anchor><fr:taxon>Theorem</fr:taxon><fr:addr>thm-000A</fr:addr><fr:route>thm-000A.xml</fr:route><fr:title>Dimension of a subspace</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Let <fr:tex>V</fr:tex> be a finite-dimensional vector space,
    and <fr:tex>U</fr:tex> be a subspace of <fr:tex>V</fr:tex>.
    Then <fr:tex>\dim  U  \leq   \dim  V</fr:tex>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    According to the definition of <fr:link href="def-000P.xml" type="local" addr="def-000P">linearly independent</fr:link>,
    to show a list of vectors is a basis, we only need to show it is linearly independent,
    and it spans the whole space.
    The next theorems simplifies the task:
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>407</fr:anchor><fr:taxon>Theorem</fr:taxon><fr:addr>thm-000B</fr:addr><fr:route>thm-000B.xml</fr:route><fr:title>Linearly independent list of the right length is a basis</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Let <fr:tex>V</fr:tex> be a finite-dimensional vector space.
    Then every <fr:link href="def-000P.xml" type="local" addr="def-000P">linearly independent</fr:link> list of vectors in <fr:tex>V</fr:tex> with length equal to <fr:tex>\dim  V</fr:tex> is a basis of <fr:tex>V</fr:tex>.
</fr:p></fr:mainmatter></fr:tree><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>408</fr:anchor><fr:taxon>Theorem</fr:taxon><fr:addr>thm-000C</fr:addr><fr:route>thm-000C.xml</fr:route><fr:title>Spanning list of the right length is a basis</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Let <fr:tex>V</fr:tex> be a finite-dimensional vector space.
    Then every <fr:link href="def-000M.xml" type="local" addr="def-000M">spanning</fr:link> list of vectors in <fr:tex>V</fr:tex> with length equal to <fr:tex>\dim  V</fr:tex> is a basis of <fr:tex>V</fr:tex>.
</fr:p></fr:mainmatter></fr:tree><fr:p>
    Now we move to the discussion of the dimension of the sum of two subspaces.
    This is analogous to the <fr:link href="thm-000E.xml" type="local" addr="thm-000E">inclusion-exclusion principle</fr:link>.
</fr:p><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="true" show-heading="true" show-metadata="false" expanded="true" root="false"><fr:frontmatter><fr:anchor>409</fr:anchor><fr:taxon>Theorem</fr:taxon><fr:addr>thm-000D</fr:addr><fr:route>thm-000D.xml</fr:route><fr:title>Dimension of a sum</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    Let <fr:tex>V</fr:tex> be a finite-dimensional vector space,
    and <fr:tex>U</fr:tex> and <fr:tex>W</fr:tex> be subspaces of <fr:tex>V</fr:tex>.
    Then
    <fr:tex display="block">          \dim (U + W) =  \dim  U +  \dim  W -  \dim (U  \cap  W).     </fr:tex></fr:p></fr:mainmatter></fr:tree></fr:mainmatter></fr:tree></fr:context><fr:related><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="false" show-heading="true" show-metadata="true" expanded="true" root="false"><fr:frontmatter><fr:anchor>1036</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-000H</fr:addr><fr:route>def-000H.xml</fr:route><fr:title>Vector Space</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    A vector space over a <fr:link href="def-0006.xml" type="local" addr="def-0006">field</fr:link> <fr:tex>F</fr:tex> is a non-empty set <fr:tex>V</fr:tex> together with a binary operation and a binary function that satisfy the axioms listed below. 
    In this context, the elements of <fr:tex>V</fr:tex> are commonly called <fr:strong>vectors</fr:strong>, and the elements of <fr:tex>F</fr:tex> are called <fr:strong>scalars</fr:strong>.
    <fr:ul><fr:li>Commutativity: <fr:tex>              \forall  x, y  \in  V, x + y = y + x         </fr:tex></fr:li>
        <fr:li>Associativity: <fr:tex>              \forall  x, y, z  \in  V, (x + y) + z = x + (y + z)         </fr:tex></fr:li>
        <fr:li>Additive Identity: <fr:tex>              \exists  0  \in  V  \text { such that }  \forall  x  \in  V, x + 0 = x         </fr:tex></fr:li>
        <fr:li>Multiplicative Identity: <fr:tex>              \forall  x  \in  V, 1x = x         </fr:tex></fr:li>
        <fr:li>Additive Inverse: <fr:tex>              \forall  x  \in  V,  \exists  y  \in  V  \text { such that } x + y = 0         </fr:tex></fr:li>
        <fr:li>Distributivity: <fr:tex>              \forall  x, y  \in  V,  \forall  c, d  \in  F, c(x + y) = cx + cy, (c + d)x = cx + dx         </fr:tex></fr:li></fr:ul></fr:p><fr:p>
    Elements of a vector space are called <fr:strong>vectors</fr:strong> or <fr:strong>points</fr:strong>.
</fr:p></fr:mainmatter></fr:tree></fr:related><fr:backlinks><fr:tree xmlns:fr="http://www.jonmsterling.com/jms-005P.xml" toc="true" numbered="false" show-heading="true" show-metadata="true" expanded="true" root="false"><fr:frontmatter><fr:anchor>1035</fr:anchor><fr:taxon>Definition</fr:taxon><fr:addr>def-001R</fr:addr><fr:route>def-001R.xml</fr:route><fr:title>Light-cone Coordinates</fr:title></fr:frontmatter><fr:mainmatter><fr:p>
    The <fr:strong>light-cone coordinates</fr:strong> can be defined as
    two independent <fr:link href="def-000L.xml" type="local" addr="def-000L">linear combinations</fr:link> of the time 
    and a chosen spatial coordinate (conventionally <fr:tex>x^1</fr:tex>):
    <fr:tex display="block">          \begin {align*}             x^+  \equiv   \frac {1}{ \sqrt {2}} (X^0 + X^1)  \\              x^-  \equiv   \frac {1}{ \sqrt {2}} (X^0 - X^1)          \end {align*}     </fr:tex>
    while other spatial coordinates remain unchanged. Thus the complete set of 
    light-cone coordinates is <fr:tex>(x^+,x^-,x^2,x^3)</fr:tex>.
</fr:p></fr:mainmatter></fr:tree></fr:backlinks><fr:references></fr:references></fr:backmatter></fr:tree>